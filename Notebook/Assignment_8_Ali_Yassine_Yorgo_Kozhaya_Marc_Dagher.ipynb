{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "093da4d2",
      "metadata": {},
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "bde0ff11",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bde0ff11",
        "outputId": "c471431c-b72d-47ae-cd24-1e49d7fea3c8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import sklearn.model_selection\n",
        "import sklearn.preprocessing as preproc\n",
        "from sklearn.feature_extraction import text\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddbc3837",
      "metadata": {
        "id": "ddbc3837"
      },
      "source": [
        "# Inspecting Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f6db1c50",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6db1c50",
        "outputId": "80495486-84c1-46d5-f159-eee6f370022e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(53043, 2)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load the dataset\n",
        "df = pd.read_csv('/content/Combined Data.csv', index_col=0)\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1dbc724d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dbc724d",
        "outputId": "ed2f9d60-80fc-48bb-cdee-78653d9c5bf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 53043 entries, 0 to 53042\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   statement  52681 non-null  object\n",
            " 1   status     53043 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 1.2+ MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0e6bff53",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "0e6bff53",
        "outputId": "7298583d-b477-4278-8fe5-3915435f5a64"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"statement\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          51073,\n          \"22\",\n          \"52681\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"status\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          7,\n          \"16351\",\n          \"53043\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-e8ec7b1e-4be1-4ff3-aa77-2b1dc2bd48b1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>statement</th>\n",
              "      <th>status</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>52681</td>\n",
              "      <td>53043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>51073</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>what do you mean?</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>22</td>\n",
              "      <td>16351</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e8ec7b1e-4be1-4ff3-aa77-2b1dc2bd48b1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e8ec7b1e-4be1-4ff3-aa77-2b1dc2bd48b1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e8ec7b1e-4be1-4ff3-aa77-2b1dc2bd48b1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-267c557a-ff8a-40d7-ae8f-558858e8c843\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-267c557a-ff8a-40d7-ae8f-558858e8c843')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-267c557a-ff8a-40d7-ae8f-558858e8c843 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                statement  status\n",
              "count               52681   53043\n",
              "unique              51073       7\n",
              "top     what do you mean?  Normal\n",
              "freq                   22   16351"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9f04e1a0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f04e1a0",
        "outputId": "1bc4d9f6-aca4-4d56-991b-e59c8c8bcbea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(52681, 2)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# drop the rows containing null values\n",
        "df = df.dropna()\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b0c7c471",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "b0c7c471",
        "outputId": "ef94ac5f-01d1-4c5d-9707-f1c8e33e4138"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 52681,\n  \"fields\": [\n    {\n      \"column\": \"statement\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 51073,\n        \"samples\": [\n          \"he's been a chain smoker for 30 years.\",\n          \"Dependence on therapist I attend IOP groups and individual therapy sessions at the same place, my therapist who I have worked with on and off for a year and a couple months just told me today that she is leaving soon and I am heartbroken. I love my therapist and I don't know how I am going to keep progressing without her. There will be a replacement for her but idk what to do, I don't want a different therapist. :(\",\n          \"These feelings constantly come back. Someone from my past that hurt me came back a month ago and once again disrespected me and i just feel like shit. Idk why these feelings keep resurfacing but it just hurts. I do not want to be over dramatic but Its hurts when you were nothing but loving/kind to someone and they disrespect you. I just hate feeling like this, feeling like i cannot trust anyone or that no one would ever truly love me unless i have something to offer. I am always worried about my looks and its just making me depressed. I really do not feel like i fit in with the world I am just here. Idk what my next step should be to get help but I am really going through it. (Yes I am in therapy) but how do i help myself ? I have been depressed/anxious for years and most day i do not even leave my house. But nobody around me seems to care and honestly I am tired of feeling this way. But at the same time i do not want to give up on myself bc i feel like I am here to be somebody great. I am just trying to find my way right now. It keeps coming back\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"status\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"Anxiety\",\n          \"Normal\",\n          \"Bipolar\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-b4d565ec-91bb-4510-8a58-c6cc8ca73277\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>statement</th>\n",
              "      <th>status</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>oh my gosh</td>\n",
              "      <td>Anxiety</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>trouble sleeping, confused mind, restless hear...</td>\n",
              "      <td>Anxiety</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b4d565ec-91bb-4510-8a58-c6cc8ca73277')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b4d565ec-91bb-4510-8a58-c6cc8ca73277 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b4d565ec-91bb-4510-8a58-c6cc8ca73277');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e506dd20-47c1-4594-ac27-0ac55e18e949\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e506dd20-47c1-4594-ac27-0ac55e18e949')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e506dd20-47c1-4594-ac27-0ac55e18e949 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                           statement   status\n",
              "0                                         oh my gosh  Anxiety\n",
              "1  trouble sleeping, confused mind, restless hear...  Anxiety"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# output the first tow rows of the dataset\n",
        "df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ea1ad19",
      "metadata": {
        "id": "5ea1ad19"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43781aec",
      "metadata": {
        "id": "43781aec"
      },
      "source": [
        "- __Deduplication__ Removing the dublicate rows, which share the same UserId, ProfileName, Time, and Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "e6f82c4f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "e6f82c4f",
        "outputId": "d1b682e1-22b5-4fc1-fd62-db146ead2055"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>False</th>\n",
              "      <td>51073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>True</th>\n",
              "      <td>1608</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ],
            "text/plain": [
              "False    51073\n",
              "True      1608\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.duplicated(subset=['statement']).value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "259866e8",
      "metadata": {
        "id": "259866e8"
      },
      "outputs": [],
      "source": [
        "df.drop_duplicates(subset=['statement'],inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "f97cc8b6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        },
        "id": "f97cc8b6",
        "outputId": "cb9e2898-4d39-4e86-bea6-17fef673557a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>False</th>\n",
              "      <td>51073</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ],
            "text/plain": [
              "False    51073\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.duplicated(subset=['statement']).value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "bd01742a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd01742a",
        "outputId": "19938e86-cfdf-412c-890e-190e27175c8d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(51073, 2)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "a14fdc0d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688
        },
        "id": "a14fdc0d",
        "outputId": "0e5a9c97-0de6-4d68-e3b5-0ef21c37a318"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# create a histogram showing the review distribution over scores from 1 to 5\n",
        "# value counts is pandas built in function that counts the number of occurences of different values in a series (numerical column)\n",
        "# plot is a pandas built in function to plot the values of a series (it uses matplotlib by default)\n",
        "ax = df['status'].value_counts().plot(kind='bar', figsize=(6,6))\n",
        "\n",
        "# get figure returns a matplotlib object (figure)\n",
        "fig = ax.get_figure()\n",
        "\n",
        "# built in matplotlib functions\n",
        "ax.set_title(\"Disorder Frequencies\")\n",
        "ax.set_xlabel('Disorder Type')\n",
        "ax.set_ylabel('Value Counts');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "3108a8ef",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "3108a8ef",
        "outputId": "731d6352-2665-4f02-d0a8-59f5dce0c6c7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 51073,\n  \"fields\": [\n    {\n      \"column\": \"statement\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 51073,\n        \"samples\": [\n          \"he's been a chain smoker for 30 years.\",\n          \"Dependence on therapist I attend IOP groups and individual therapy sessions at the same place, my therapist who I have worked with on and off for a year and a couple months just told me today that she is leaving soon and I am heartbroken. I love my therapist and I don't know how I am going to keep progressing without her. There will be a replacement for her but idk what to do, I don't want a different therapist. :(\",\n          \"These feelings constantly come back. Someone from my past that hurt me came back a month ago and once again disrespected me and i just feel like shit. Idk why these feelings keep resurfacing but it just hurts. I do not want to be over dramatic but Its hurts when you were nothing but loving/kind to someone and they disrespect you. I just hate feeling like this, feeling like i cannot trust anyone or that no one would ever truly love me unless i have something to offer. I am always worried about my looks and its just making me depressed. I really do not feel like i fit in with the world I am just here. Idk what my next step should be to get help but I am really going through it. (Yes I am in therapy) but how do i help myself ? I have been depressed/anxious for years and most day i do not even leave my house. But nobody around me seems to care and honestly I am tired of feeling this way. But at the same time i do not want to give up on myself bc i feel like I am here to be somebody great. I am just trying to find my way right now. It keeps coming back\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"status\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"Anxiety\",\n          \"Normal\",\n          \"Bipolar\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-101475aa-e65e-4b96-94dd-4cc50c1def2d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>statement</th>\n",
              "      <th>status</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>oh my gosh</td>\n",
              "      <td>Anxiety</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-101475aa-e65e-4b96-94dd-4cc50c1def2d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-101475aa-e65e-4b96-94dd-4cc50c1def2d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-101475aa-e65e-4b96-94dd-4cc50c1def2d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "    statement   status  Label\n",
              "0  oh my gosh  Anxiety      0"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create a label column in the dataframe and assing all its values to 0\n",
        "df['Label'] = 0\n",
        "df.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "af83a1b3",
      "metadata": {
        "id": "af83a1b3"
      },
      "outputs": [],
      "source": [
        "def cluster_status(status):\n",
        "    if status in ['Anxiety','Bipolar','Stress','Personality disorder']:\n",
        "        return 3\n",
        "    elif status == 'Normal':\n",
        "        return 0\n",
        "    elif status == 'Depression':\n",
        "        return 1\n",
        "    elif status == 'Suicidal':\n",
        "        return 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "d28a688b",
      "metadata": {
        "id": "d28a688b"
      },
      "outputs": [],
      "source": [
        "df['Label']=df['status'].apply(cluster_status)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "d736160f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "d736160f",
        "outputId": "97d27108-629e-40dc-a75b-3358cd7d6f33"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"statement\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"wkwkwk what a joke\",\n          \"wedding teaser concept using the song day6 - only, sounds good ga siiih\",\n          \"Leaves are also standby in front of the PC ... because the office is no longer on leave\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"status\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Normal\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-a90d91a8-b2e7-45d3-9dab-782eeb49f8d8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>statement</th>\n",
              "      <th>status</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>733</th>\n",
              "      <td>Gr gr dreaming of ex crush to be my game, God</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>734</th>\n",
              "      <td>wkwkwk what a joke</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>735</th>\n",
              "      <td>Leaves are also standby in front of the PC ......</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>736</th>\n",
              "      <td>Thank God even though it's just a ride through</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>737</th>\n",
              "      <td>wedding teaser concept using the song day6 - o...</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a90d91a8-b2e7-45d3-9dab-782eeb49f8d8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a90d91a8-b2e7-45d3-9dab-782eeb49f8d8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a90d91a8-b2e7-45d3-9dab-782eeb49f8d8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-a5020ccb-2bfc-4fa9-8020-12b76747bf42\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a5020ccb-2bfc-4fa9-8020-12b76747bf42')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-a5020ccb-2bfc-4fa9-8020-12b76747bf42 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                             statement  status  Label\n",
              "733      Gr gr dreaming of ex crush to be my game, God  Normal      0\n",
              "734                                 wkwkwk what a joke  Normal      0\n",
              "735  Leaves are also standby in front of the PC ......  Normal      0\n",
              "736     Thank God even though it's just a ride through  Normal      0\n",
              "737  wedding teaser concept using the song day6 - o...  Normal      0"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# df.loc[df['status']=='Anxiety'].head()  # note that we checked differnt status to be sure\n",
        "df.loc[df['status']=='Normal'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "289e5bd0",
      "metadata": {
        "id": "289e5bd0"
      },
      "source": [
        "# Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "136a21bb",
      "metadata": {
        "id": "136a21bb"
      },
      "outputs": [],
      "source": [
        "# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
        "contractions = {\n",
        "\"ain't\": \"am not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"needn't\": \"need not\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there had\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who's\": \"who is\",\n",
        "\"won't\": \"will not\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"you'd\": \"you would\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you're\": \"you are\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "bc05c559",
      "metadata": {
        "id": "bc05c559"
      },
      "outputs": [],
      "source": [
        "# function to replace the contracted words with their longer counterparts\n",
        "def decontract_words(text):\n",
        "    text = text.split()\n",
        "    new_text = []\n",
        "    for word in text:\n",
        "        if word in contractions:\n",
        "            new_text.append(contractions[word])\n",
        "        else:\n",
        "            new_text.append(word)\n",
        "    return \" \".join(new_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "8bd8d18a",
      "metadata": {
        "id": "8bd8d18a"
      },
      "outputs": [],
      "source": [
        "# function to format words and remove unwanted characters using regex\n",
        "def format_text_regex(text):\n",
        "    # ^https?:\\/\\/(?:www\\.)?[-a-zA-Z0-9@:%.\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b(?:[-a-zA-Z0-9()@:%\\+.~#?&\\/=]*)$\n",
        "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE) #clean all URLs\n",
        "    text = re.sub(r'\\<a href', ' ', text) #clean html style URL\n",
        "    text = re.sub(r'&amp;', '', text) #remove &amp; chars\n",
        "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text) #remove special characters\n",
        "    text = re.sub(r'<br />', ' ', text) #remove html style <br>\n",
        "    text = re.sub(r'\\'', ' ', text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "0dad6169",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dad6169",
        "outputId": "4fdb5ad8-01be-438f-9cae-f1b2bd617e0c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# function to remove stop words usng the nltk\n",
        "nltk.download('stopwords')\n",
        "def remove_stopwords(text):\n",
        "    text = text.split()\n",
        "    stops = set(stopwords.words(\"english\"))\n",
        "    text = [w for w in text if not w in stops]\n",
        "    return \" \".join(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "e02eb735",
      "metadata": {
        "id": "e02eb735"
      },
      "outputs": [],
      "source": [
        "# function that groups logic from other preprocessing functions to clean text\n",
        "def clean_text(text):\n",
        "\n",
        "    # Convert words to lower case\n",
        "    text = text.lower()\n",
        "\n",
        "    # Use other preprocessing functions\n",
        "    text = decontract_words(text)\n",
        "    text = format_text_regex(text)\n",
        "    text = remove_stopwords(text)\n",
        "\n",
        "    # Tokenize each word\n",
        "    text =  nltk.WordPunctTokenizer().tokenize(text)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "52563ba7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52563ba7",
        "outputId": "b66f5239-1208-4b39-c950-3e68e85a5166"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 51073 entries, 0 to 52840\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   statement  51073 non-null  object\n",
            " 1   status     51073 non-null  object\n",
            " 2   Label      51073 non-null  int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 1.6+ MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "fb292bfa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "fb292bfa",
        "outputId": "98c50301-6a13-4bec-d687-54a544267e7a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 51073,\n  \"fields\": [\n    {\n      \"column\": \"statement\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 51073,\n        \"samples\": [\n          \"he's been a chain smoker for 30 years.\",\n          \"Dependence on therapist I attend IOP groups and individual therapy sessions at the same place, my therapist who I have worked with on and off for a year and a couple months just told me today that she is leaving soon and I am heartbroken. I love my therapist and I don't know how I am going to keep progressing without her. There will be a replacement for her but idk what to do, I don't want a different therapist. :(\",\n          \"These feelings constantly come back. Someone from my past that hurt me came back a month ago and once again disrespected me and i just feel like shit. Idk why these feelings keep resurfacing but it just hurts. I do not want to be over dramatic but Its hurts when you were nothing but loving/kind to someone and they disrespect you. I just hate feeling like this, feeling like i cannot trust anyone or that no one would ever truly love me unless i have something to offer. I am always worried about my looks and its just making me depressed. I really do not feel like i fit in with the world I am just here. Idk what my next step should be to get help but I am really going through it. (Yes I am in therapy) but how do i help myself ? I have been depressed/anxious for years and most day i do not even leave my house. But nobody around me seems to care and honestly I am tired of feeling this way. But at the same time i do not want to give up on myself bc i feel like I am here to be somebody great. I am just trying to find my way right now. It keeps coming back\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"status\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"Anxiety\",\n          \"Normal\",\n          \"Bipolar\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0,\n          2,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Text_Cleaned\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-4f5b8119-33d9-4aba-889f-275873115907\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>statement</th>\n",
              "      <th>status</th>\n",
              "      <th>Label</th>\n",
              "      <th>Text_Cleaned</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>oh my gosh</td>\n",
              "      <td>Anxiety</td>\n",
              "      <td>3</td>\n",
              "      <td>[oh, gosh]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>trouble sleeping, confused mind, restless hear...</td>\n",
              "      <td>Anxiety</td>\n",
              "      <td>3</td>\n",
              "      <td>[trouble, sleeping, confused, mind, restless, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>All wrong, back off dear, forward doubt. Stay ...</td>\n",
              "      <td>Anxiety</td>\n",
              "      <td>3</td>\n",
              "      <td>[wrong, back, dear, forward, doubt, stay, rest...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4f5b8119-33d9-4aba-889f-275873115907')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4f5b8119-33d9-4aba-889f-275873115907 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4f5b8119-33d9-4aba-889f-275873115907');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-76b38e22-3d22-44f8-b85b-75ec02180b2e\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-76b38e22-3d22-44f8-b85b-75ec02180b2e')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-76b38e22-3d22-44f8-b85b-75ec02180b2e button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                           statement   status  Label  \\\n",
              "0                                         oh my gosh  Anxiety      3   \n",
              "1  trouble sleeping, confused mind, restless hear...  Anxiety      3   \n",
              "2  All wrong, back off dear, forward doubt. Stay ...  Anxiety      3   \n",
              "\n",
              "                                        Text_Cleaned  \n",
              "0                                         [oh, gosh]  \n",
              "1  [trouble, sleeping, confused, mind, restless, ...  \n",
              "2  [wrong, back, dear, forward, doubt, stay, rest...  "
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Use the map to apply the clean_text on the df.statement column and store the result in a new column called text_cleaned\n",
        "df['Text_Cleaned'] = list(map(clean_text, df.statement))\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "458e5002",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "id": "458e5002",
        "outputId": "00901a14-bb33-4939-e842-a19786248a3d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 51073,\n  \"fields\": [\n    {\n      \"column\": \"statement\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 51073,\n        \"samples\": [\n          \"he's been a chain smoker for 30 years.\",\n          \"Dependence on therapist I attend IOP groups and individual therapy sessions at the same place, my therapist who I have worked with on and off for a year and a couple months just told me today that she is leaving soon and I am heartbroken. I love my therapist and I don't know how I am going to keep progressing without her. There will be a replacement for her but idk what to do, I don't want a different therapist. :(\",\n          \"These feelings constantly come back. Someone from my past that hurt me came back a month ago and once again disrespected me and i just feel like shit. Idk why these feelings keep resurfacing but it just hurts. I do not want to be over dramatic but Its hurts when you were nothing but loving/kind to someone and they disrespect you. I just hate feeling like this, feeling like i cannot trust anyone or that no one would ever truly love me unless i have something to offer. I am always worried about my looks and its just making me depressed. I really do not feel like i fit in with the world I am just here. Idk what my next step should be to get help but I am really going through it. (Yes I am in therapy) but how do i help myself ? I have been depressed/anxious for years and most day i do not even leave my house. But nobody around me seems to care and honestly I am tired of feeling this way. But at the same time i do not want to give up on myself bc i feel like I am here to be somebody great. I am just trying to find my way right now. It keeps coming back\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"status\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"Anxiety\",\n          \"Normal\",\n          \"Bipolar\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0,\n          2,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Text_Cleaned\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lemmatized_text\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-b5dc8360-23f6-4c59-8cb6-febbc5d38bd3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>statement</th>\n",
              "      <th>status</th>\n",
              "      <th>Label</th>\n",
              "      <th>Text_Cleaned</th>\n",
              "      <th>lemmatized_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>oh my gosh</td>\n",
              "      <td>Anxiety</td>\n",
              "      <td>3</td>\n",
              "      <td>[oh, gosh]</td>\n",
              "      <td>[oh, gosh]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>trouble sleeping, confused mind, restless hear...</td>\n",
              "      <td>Anxiety</td>\n",
              "      <td>3</td>\n",
              "      <td>[trouble, sleeping, confused, mind, restless, ...</td>\n",
              "      <td>[trouble, sleeping, confused, mind, restless, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>All wrong, back off dear, forward doubt. Stay ...</td>\n",
              "      <td>Anxiety</td>\n",
              "      <td>3</td>\n",
              "      <td>[wrong, back, dear, forward, doubt, stay, rest...</td>\n",
              "      <td>[wrong, back, dear, forward, doubt, stay, rest...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b5dc8360-23f6-4c59-8cb6-febbc5d38bd3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b5dc8360-23f6-4c59-8cb6-febbc5d38bd3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b5dc8360-23f6-4c59-8cb6-febbc5d38bd3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-c4ac2cee-8054-4ab1-b1e2-c98cbf723a9b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c4ac2cee-8054-4ab1-b1e2-c98cbf723a9b')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-c4ac2cee-8054-4ab1-b1e2-c98cbf723a9b button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                           statement   status  Label  \\\n",
              "0                                         oh my gosh  Anxiety      3   \n",
              "1  trouble sleeping, confused mind, restless hear...  Anxiety      3   \n",
              "2  All wrong, back off dear, forward doubt. Stay ...  Anxiety      3   \n",
              "\n",
              "                                        Text_Cleaned  \\\n",
              "0                                         [oh, gosh]   \n",
              "1  [trouble, sleeping, confused, mind, restless, ...   \n",
              "2  [wrong, back, dear, forward, doubt, stay, rest...   \n",
              "\n",
              "                                     lemmatized_text  \n",
              "0                                         [oh, gosh]  \n",
              "1  [trouble, sleeping, confused, mind, restless, ...  \n",
              "2  [wrong, back, dear, forward, doubt, stay, rest...  "
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# function to lemmatize words in text cleaned and create a new column lemmatized text and store them there\n",
        "def lemmatized_words(text):\n",
        "    lemm = nltk.stem.WordNetLemmatizer()\n",
        "    df['lemmatized_text'] = list(map(lambda word:\n",
        "                                     list(map(lemm.lemmatize, word)),\n",
        "                                     df.Text_Cleaned))\n",
        "\n",
        "\n",
        "lemmatized_words(df.Text_Cleaned)\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "97c0a5f9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        },
        "id": "97c0a5f9",
        "outputId": "d31a73b8-b8c5-4bc9-dc67-2a663b2edc9d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df[[ 'statement', 'Text_Cleaned', 'lemmatized_text','status']]\",\n  \"rows\": 600,\n  \"fields\": [\n    {\n      \"column\": \"statement\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 600,\n        \"samples\": [\n          \"Why does it seem like tonight I'm restless, sad like I want to cry but I don't know what to be sad about\",\n          \"Help! Lately, whenever I meet a lot of people I don't know, I always feel really dizzy, my stomach suddenly feels nauseous, anxious, panicked. How do you solve it? And why is that?\",\n          \"every night overthinking, anxious, nervous..every time I do activities that I don't like, I always feel nauseous and have stomachaches. sometimes if you think too much it gives you a headache :(\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Text_Cleaned\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lemmatized_text\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"status\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Anxiety\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-e204c8e4-a74d-41ba-b4ba-ca6fc8c04edc\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>statement</th>\n",
              "      <th>Text_Cleaned</th>\n",
              "      <th>lemmatized_text</th>\n",
              "      <th>status</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>oh my gosh</td>\n",
              "      <td>[oh, gosh]</td>\n",
              "      <td>[oh, gosh]</td>\n",
              "      <td>Anxiety</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>trouble sleeping, confused mind, restless heart. All out of tune</td>\n",
              "      <td>[trouble, sleeping, confused, mind, restless, heart, tune]</td>\n",
              "      <td>[trouble, sleeping, confused, mind, restless, heart, tune]</td>\n",
              "      <td>Anxiety</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>All wrong, back off dear, forward doubt. Stay in a restless and restless place</td>\n",
              "      <td>[wrong, back, dear, forward, doubt, stay, restless, restless, place]</td>\n",
              "      <td>[wrong, back, dear, forward, doubt, stay, restless, restless, place]</td>\n",
              "      <td>Anxiety</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I've shifted my focus to something else but I'm still worried</td>\n",
              "      <td>[shifted, focus, something, else, still, worried]</td>\n",
              "      <td>[shifted, focus, something, else, still, worried]</td>\n",
              "      <td>Anxiety</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I'm restless and restless, it's been a month now, boy. What do you mean?</td>\n",
              "      <td>[restless, restless, month, boy, mean]</td>\n",
              "      <td>[restless, restless, month, boy, mean]</td>\n",
              "      <td>Anxiety</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>612</th>\n",
              "      <td>Yes, the point is that I feel tired, sad, annoyed, restless. It's like the feeling is mixed in my heart and mind, in my brain I'm traveling various things from problems to happiness that I've felt until this moment.</td>\n",
              "      <td>[yes, point, feel, tired, sad, annoyed, restless, like, feeling, mixed, heart, mind, brain, traveling, various, things, problems, happiness, felt, moment]</td>\n",
              "      <td>[yes, point, feel, tired, sad, annoyed, restless, like, feeling, mixed, heart, mind, brain, traveling, various, thing, problem, happiness, felt, moment]</td>\n",
              "      <td>Anxiety</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>613</th>\n",
              "      <td>Hmm why are you so nervous</td>\n",
              "      <td>[hmm, nervous]</td>\n",
              "      <td>[hmm, nervous]</td>\n",
              "      <td>Anxiety</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>614</th>\n",
              "      <td>restless and worrying too much</td>\n",
              "      <td>[restless, worrying, much]</td>\n",
              "      <td>[restless, worrying, much]</td>\n",
              "      <td>Anxiety</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>615</th>\n",
              "      <td>When the effects of the depressant have started to wear off and the depression phase has started, it's really tiring</td>\n",
              "      <td>[effects, depressant, started, wear, depression, phase, started, really, tiring]</td>\n",
              "      <td>[effect, depressant, started, wear, depression, phase, started, really, tiring]</td>\n",
              "      <td>Anxiety</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>616</th>\n",
              "      <td>Going through a phase where anxiety continues to haunt</td>\n",
              "      <td>[going, phase, anxiety, continues, haunt]</td>\n",
              "      <td>[going, phase, anxiety, continues, haunt]</td>\n",
              "      <td>Anxiety</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>600 rows × 4 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e204c8e4-a74d-41ba-b4ba-ca6fc8c04edc')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e204c8e4-a74d-41ba-b4ba-ca6fc8c04edc button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e204c8e4-a74d-41ba-b4ba-ca6fc8c04edc');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-5ef08fdc-cab9-48d2-a56e-3e4852700b0c\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5ef08fdc-cab9-48d2-a56e-3e4852700b0c')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-5ef08fdc-cab9-48d2-a56e-3e4852700b0c button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                   statement  \\\n",
              "0                                                                                                                                                                                                                 oh my gosh   \n",
              "1                                                                                                                                                           trouble sleeping, confused mind, restless heart. All out of tune   \n",
              "2                                                                                                                                             All wrong, back off dear, forward doubt. Stay in a restless and restless place   \n",
              "3                                                                                                                                                              I've shifted my focus to something else but I'm still worried   \n",
              "4                                                                                                                                                   I'm restless and restless, it's been a month now, boy. What do you mean?   \n",
              "..                                                                                                                                                                                                                       ...   \n",
              "612  Yes, the point is that I feel tired, sad, annoyed, restless. It's like the feeling is mixed in my heart and mind, in my brain I'm traveling various things from problems to happiness that I've felt until this moment.   \n",
              "613                                                                                                                                                                                               Hmm why are you so nervous   \n",
              "614                                                                                                                                                                                           restless and worrying too much   \n",
              "615                                                                                                     When the effects of the depressant have started to wear off and the depression phase has started, it's really tiring   \n",
              "616                                                                                                                                                                   Going through a phase where anxiety continues to haunt   \n",
              "\n",
              "                                                                                                                                                   Text_Cleaned  \\\n",
              "0                                                                                                                                                    [oh, gosh]   \n",
              "1                                                                                                    [trouble, sleeping, confused, mind, restless, heart, tune]   \n",
              "2                                                                                          [wrong, back, dear, forward, doubt, stay, restless, restless, place]   \n",
              "3                                                                                                             [shifted, focus, something, else, still, worried]   \n",
              "4                                                                                                                        [restless, restless, month, boy, mean]   \n",
              "..                                                                                                                                                          ...   \n",
              "612  [yes, point, feel, tired, sad, annoyed, restless, like, feeling, mixed, heart, mind, brain, traveling, various, things, problems, happiness, felt, moment]   \n",
              "613                                                                                                                                              [hmm, nervous]   \n",
              "614                                                                                                                                  [restless, worrying, much]   \n",
              "615                                                                            [effects, depressant, started, wear, depression, phase, started, really, tiring]   \n",
              "616                                                                                                                   [going, phase, anxiety, continues, haunt]   \n",
              "\n",
              "                                                                                                                                              lemmatized_text  \\\n",
              "0                                                                                                                                                  [oh, gosh]   \n",
              "1                                                                                                  [trouble, sleeping, confused, mind, restless, heart, tune]   \n",
              "2                                                                                        [wrong, back, dear, forward, doubt, stay, restless, restless, place]   \n",
              "3                                                                                                           [shifted, focus, something, else, still, worried]   \n",
              "4                                                                                                                      [restless, restless, month, boy, mean]   \n",
              "..                                                                                                                                                        ...   \n",
              "612  [yes, point, feel, tired, sad, annoyed, restless, like, feeling, mixed, heart, mind, brain, traveling, various, thing, problem, happiness, felt, moment]   \n",
              "613                                                                                                                                            [hmm, nervous]   \n",
              "614                                                                                                                                [restless, worrying, much]   \n",
              "615                                                                           [effect, depressant, started, wear, depression, phase, started, really, tiring]   \n",
              "616                                                                                                                 [going, phase, anxiety, continues, haunt]   \n",
              "\n",
              "      status  \n",
              "0    Anxiety  \n",
              "1    Anxiety  \n",
              "2    Anxiety  \n",
              "3    Anxiety  \n",
              "4    Anxiety  \n",
              "..       ...  \n",
              "612  Anxiety  \n",
              "613  Anxiety  \n",
              "614  Anxiety  \n",
              "615  Anxiety  \n",
              "616  Anxiety  \n",
              "\n",
              "[600 rows x 4 columns]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# optional line to adjust table width\n",
        "pd.set_option('max_colwidth', 500)\n",
        "\n",
        "# sample the data after preprocessing\n",
        "df[[ 'statement', 'Text_Cleaned', 'lemmatized_text','status']].head(600)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92e284cb",
      "metadata": {
        "id": "92e284cb"
      },
      "source": [
        "### Vizualizing the Length Distribution for our Lemmatized Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "9808c578",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "9808c578",
        "outputId": "017e02c9-5b9d-48b2-996f-1d42d6bbdcd1"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "lengths = df['lemmatized_text'].apply(len)\n",
        "\n",
        "fig,ax=plt.subplots(figsize=(5,5))\n",
        "ax.hist(lengths,bins=1000,edgecolor='black')\n",
        "ax.set_title('Lemmatized Text Lengths Distribution')\n",
        "ax.set_xlabel('Number of Tokens')\n",
        "ax.set_ylabel('Frequency')\n",
        "ax.set_xlim(0,250)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "98619c6e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "98619c6e",
        "outputId": "3d82ed74-1999-42be-ddff-aa46e69ba924"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lemmatized_text</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>False</th>\n",
              "      <td>43350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>True</th>\n",
              "      <td>7723</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ],
            "text/plain": [
              "lemmatized_text\n",
              "False    43350\n",
              "True      7723\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(df['lemmatized_text'].apply(len) > 100).value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "ab8c3eea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "ab8c3eea",
        "outputId": "fb29c913-afed-4edb-e361-17138940b2cf"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lemmatized_text</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>False</th>\n",
              "      <td>45758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>True</th>\n",
              "      <td>5315</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ],
            "text/plain": [
              "lemmatized_text\n",
              "False    45758\n",
              "True      5315\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(df['lemmatized_text'].apply(len) > 125).value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "72676dbc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "72676dbc",
        "outputId": "ce0a9bcc-fc34-41dc-ad98-10d7e99c94c3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lemmatized_text</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>False</th>\n",
              "      <td>47233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>True</th>\n",
              "      <td>3840</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ],
            "text/plain": [
              "lemmatized_text\n",
              "False    47233\n",
              "True      3840\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(df['lemmatized_text'].apply(len) > 150).value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3415b6e",
      "metadata": {
        "id": "b3415b6e"
      },
      "source": [
        "Based on the above figure, we found it logical to set the sequence length to 125 as a middle way between short and long sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "c18991b9",
      "metadata": {
        "id": "c18991b9"
      },
      "outputs": [],
      "source": [
        "sequence_length=125"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d70a2589",
      "metadata": {
        "id": "d70a2589"
      },
      "source": [
        "## W2Vector embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "7b6dc965",
      "metadata": {
        "id": "7b6dc965"
      },
      "outputs": [],
      "source": [
        "## getting the data ready\n",
        "data=df['lemmatized_text'].to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "7b291792",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b291792",
        "outputId": "83c84a2f-2d21-4d1a-d2d1-9dfc0216d7fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['oh', 'gosh'],\n",
              " ['trouble', 'sleeping', 'confused', 'mind', 'restless', 'heart', 'tune'],\n",
              " ['wrong',\n",
              "  'back',\n",
              "  'dear',\n",
              "  'forward',\n",
              "  'doubt',\n",
              "  'stay',\n",
              "  'restless',\n",
              "  'restless',\n",
              "  'place'],\n",
              " ['shifted', 'focus', 'something', 'else', 'still', 'worried'],\n",
              " ['restless', 'restless', 'month', 'boy', 'mean'],\n",
              " ['every', 'break', 'must', 'nervous', 'like', 'something', 'wrong', 'heck'],\n",
              " ['feel', 'scared', 'anxious', 'may', 'family', 'u', 'protected'],\n",
              " ['ever', 'felt', 'nervous', 'know'],\n",
              " ['slept', 'well', '2', 'day', 'like', 'restless', 'huh'],\n",
              " ['really', 'worried', 'want', 'cry']]"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "bf0d95e0",
      "metadata": {
        "id": "bf0d95e0"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(action='ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "fe128dc4",
      "metadata": {
        "id": "fe128dc4"
      },
      "outputs": [],
      "source": [
        "embedding_size=300\n",
        "# Create CBOW model\n",
        "model_cbow = gensim.models.Word2Vec(data, min_count=1,\n",
        "                                vector_size=embedding_size, window=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "e5969fb5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5969fb5",
        "outputId": "aa9288dc-bdd5-4063-8c92-57262b5d3375"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 0.00878975, -0.01023117, -0.05999797,  0.12608849,  0.16910453,\n",
              "       -0.03233052,  0.02926807,  0.07676207,  0.0739871 , -0.0837993 ,\n",
              "        0.0180738 , -0.11628714,  0.06831345, -0.0086078 , -0.08064554,\n",
              "        0.02345169, -0.01102237, -0.04727242,  0.0596103 ,  0.1216834 ,\n",
              "       -0.12616757,  0.07789709,  0.01629597,  0.02264495,  0.08901156,\n",
              "        0.08475657,  0.02350325, -0.0393285 , -0.03513982, -0.0591691 ,\n",
              "       -0.21624455,  0.08256544,  0.01781911, -0.15591872,  0.06545634,\n",
              "        0.2988336 ,  0.02124329, -0.17501028, -0.16622123,  0.01729871,\n",
              "       -0.17566164,  0.06853662,  0.27462968, -0.06723837, -0.12884578,\n",
              "       -0.01583683, -0.173758  ,  0.16634092, -0.01682585,  0.11784674,\n",
              "        0.12350617,  0.03294566, -0.08796453, -0.06996462, -0.02605063,\n",
              "       -0.0331637 ,  0.04666992, -0.03582903,  0.04679418, -0.15442662,\n",
              "       -0.17527209, -0.0732862 , -0.1267984 ,  0.14700222, -0.02914515,\n",
              "       -0.02622194,  0.08477233, -0.11976615, -0.01996195,  0.16221975,\n",
              "        0.0096135 ,  0.31354904,  0.03677008,  0.04160938, -0.01783015,\n",
              "       -0.06869867, -0.18114346,  0.18461749, -0.05276092,  0.09978765,\n",
              "       -0.0483623 , -0.2945217 ,  0.11998843, -0.06775248, -0.00942189,\n",
              "       -0.11263987, -0.04563543,  0.08693819,  0.09077426,  0.13101767,\n",
              "       -0.01600307, -0.08859662, -0.09348053,  0.18070684,  0.15694639,\n",
              "       -0.03484444,  0.01032258, -0.14822298, -0.05209324, -0.06749861,\n",
              "        0.19849406,  0.09727377,  0.1175536 , -0.06117622, -0.10629149,\n",
              "       -0.07719506, -0.266685  , -0.08021808,  0.180719  ,  0.00815102,\n",
              "        0.00967958, -0.17964627,  0.12797068,  0.1281757 ,  0.09346588,\n",
              "       -0.0682468 ,  0.18033105, -0.15892415,  0.1751056 ,  0.0744321 ,\n",
              "        0.07766772,  0.09919808,  0.05132906,  0.01451815, -0.07384341,\n",
              "       -0.13774411, -0.00560649, -0.15391317, -0.01131572,  0.11520474,\n",
              "        0.2550244 ,  0.09502438, -0.16330487,  0.06003226,  0.01055944,\n",
              "        0.15142405,  0.13146457, -0.25825775, -0.20858353,  0.06067707,\n",
              "       -0.01619343, -0.1217979 , -0.07173444, -0.13984714,  0.19430038,\n",
              "       -0.23806165,  0.07164419, -0.06357434,  0.14508563,  0.05598941,\n",
              "       -0.04624222, -0.03148401, -0.2708576 , -0.16629004, -0.02439733,\n",
              "       -0.10435729,  0.07646702,  0.04574162,  0.07181917,  0.089306  ,\n",
              "       -0.03802798,  0.09586892, -0.07254035,  0.01214902, -0.1717168 ,\n",
              "        0.02085738, -0.03911108, -0.00895899, -0.04008931,  0.13865606,\n",
              "        0.03719346,  0.11945883,  0.1420554 , -0.13572767, -0.1056365 ,\n",
              "        0.00912784, -0.05608998, -0.06305463, -0.08081964, -0.15561438,\n",
              "        0.03735629,  0.19889477, -0.22567078, -0.09928375,  0.02807834,\n",
              "        0.01252261,  0.1567806 ,  0.06979452,  0.01807269, -0.00558002,\n",
              "       -0.09549665,  0.14130187, -0.13659963, -0.09962026,  0.09159061,\n",
              "        0.21895118, -0.06518445, -0.12752064,  0.02375554, -0.09786975,\n",
              "       -0.11766616,  0.09187231,  0.0549568 , -0.0777279 , -0.24064496,\n",
              "       -0.01553642,  0.01025353, -0.01321702,  0.00572098, -0.06679545,\n",
              "       -0.0186842 ,  0.1550735 ,  0.08195563,  0.06157633,  0.03842571,\n",
              "       -0.01382841,  0.11236699,  0.01772459, -0.05544212, -0.09172788,\n",
              "        0.12926133,  0.01813228,  0.04963012, -0.17694107, -0.04984789,\n",
              "       -0.0868056 ,  0.09795745,  0.02495938, -0.02959341,  0.16637181,\n",
              "       -0.03142437, -0.04844716,  0.09233779,  0.11119046,  0.00832118,\n",
              "       -0.1126007 ,  0.02010728,  0.03762989, -0.26431924, -0.09170859,\n",
              "        0.07457964,  0.11773603, -0.01053937, -0.14919636,  0.00106102,\n",
              "       -0.20912234, -0.22878483,  0.01675634,  0.00978636,  0.03166952,\n",
              "        0.07677676,  0.04422414,  0.0028779 , -0.07549895, -0.12318947,\n",
              "       -0.02164292,  0.09642796, -0.01850503, -0.10880314, -0.07304884,\n",
              "        0.0776286 , -0.05095898, -0.03408723, -0.0764377 , -0.13741887,\n",
              "       -0.08867639,  0.01397037, -0.05635495, -0.07735875, -0.01525501,\n",
              "        0.01648523,  0.00649169, -0.06314375, -0.09720119,  0.16446728,\n",
              "        0.00838094,  0.14804101, -0.18553844, -0.116144  , -0.17433591,\n",
              "       -0.01718061,  0.00454088, -0.09665701,  0.02983702, -0.20354299,\n",
              "       -0.06562927, -0.13028637,  0.10168378, -0.08140083,  0.10038068,\n",
              "        0.06656654, -0.09275918, -0.03153896, -0.05042892, -0.17040935,\n",
              "        0.06435465,  0.08450129,  0.0586226 ,  0.10693431,  0.09285497],\n",
              "      dtype=float32)"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_cbow.wv['gosh']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c07c05f",
      "metadata": {
        "id": "5c07c05f"
      },
      "source": [
        "### Splitting the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "c38fb6dc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "c38fb6dc",
        "outputId": "c0892dd8-1847-4f11-fa4d-7474e0df0acc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 51073,\n  \"fields\": [\n    {\n      \"column\": \"statement\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 51073,\n        \"samples\": [\n          \"he's been a chain smoker for 30 years.\",\n          \"Dependence on therapist I attend IOP groups and individual therapy sessions at the same place, my therapist who I have worked with on and off for a year and a couple months just told me today that she is leaving soon and I am heartbroken. I love my therapist and I don't know how I am going to keep progressing without her. There will be a replacement for her but idk what to do, I don't want a different therapist. :(\",\n          \"These feelings constantly come back. Someone from my past that hurt me came back a month ago and once again disrespected me and i just feel like shit. Idk why these feelings keep resurfacing but it just hurts. I do not want to be over dramatic but Its hurts when you were nothing but loving/kind to someone and they disrespect you. I just hate feeling like this, feeling like i cannot trust anyone or that no one would ever truly love me unless i have something to offer. I am always worried about my looks and its just making me depressed. I really do not feel like i fit in with the world I am just here. Idk what my next step should be to get help but I am really going through it. (Yes I am in therapy) but how do i help myself ? I have been depressed/anxious for years and most day i do not even leave my house. But nobody around me seems to care and honestly I am tired of feeling this way. But at the same time i do not want to give up on myself bc i feel like I am here to be somebody great. I am just trying to find my way right now. It keeps coming back\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"status\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"Anxiety\",\n          \"Normal\",\n          \"Bipolar\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0,\n          2,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Text_Cleaned\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lemmatized_text\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-1633823f-fa9e-4704-9b0c-9ecc8c0a68f9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>statement</th>\n",
              "      <th>status</th>\n",
              "      <th>Label</th>\n",
              "      <th>Text_Cleaned</th>\n",
              "      <th>lemmatized_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>oh my gosh</td>\n",
              "      <td>Anxiety</td>\n",
              "      <td>3</td>\n",
              "      <td>[oh, gosh]</td>\n",
              "      <td>[oh, gosh]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>trouble sleeping, confused mind, restless heart. All out of tune</td>\n",
              "      <td>Anxiety</td>\n",
              "      <td>3</td>\n",
              "      <td>[trouble, sleeping, confused, mind, restless, heart, tune]</td>\n",
              "      <td>[trouble, sleeping, confused, mind, restless, heart, tune]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>All wrong, back off dear, forward doubt. Stay in a restless and restless place</td>\n",
              "      <td>Anxiety</td>\n",
              "      <td>3</td>\n",
              "      <td>[wrong, back, dear, forward, doubt, stay, restless, restless, place]</td>\n",
              "      <td>[wrong, back, dear, forward, doubt, stay, restless, restless, place]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I've shifted my focus to something else but I'm still worried</td>\n",
              "      <td>Anxiety</td>\n",
              "      <td>3</td>\n",
              "      <td>[shifted, focus, something, else, still, worried]</td>\n",
              "      <td>[shifted, focus, something, else, still, worried]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I'm restless and restless, it's been a month now, boy. What do you mean?</td>\n",
              "      <td>Anxiety</td>\n",
              "      <td>3</td>\n",
              "      <td>[restless, restless, month, boy, mean]</td>\n",
              "      <td>[restless, restless, month, boy, mean]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1633823f-fa9e-4704-9b0c-9ecc8c0a68f9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1633823f-fa9e-4704-9b0c-9ecc8c0a68f9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1633823f-fa9e-4704-9b0c-9ecc8c0a68f9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-ab2b04bc-c939-4110-90ab-900758e50eb8\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ab2b04bc-c939-4110-90ab-900758e50eb8')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-ab2b04bc-c939-4110-90ab-900758e50eb8 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                                        statement  \\\n",
              "0                                                                      oh my gosh   \n",
              "1                trouble sleeping, confused mind, restless heart. All out of tune   \n",
              "2  All wrong, back off dear, forward doubt. Stay in a restless and restless place   \n",
              "3                   I've shifted my focus to something else but I'm still worried   \n",
              "4        I'm restless and restless, it's been a month now, boy. What do you mean?   \n",
              "\n",
              "    status  Label  \\\n",
              "0  Anxiety      3   \n",
              "1  Anxiety      3   \n",
              "2  Anxiety      3   \n",
              "3  Anxiety      3   \n",
              "4  Anxiety      3   \n",
              "\n",
              "                                                           Text_Cleaned  \\\n",
              "0                                                            [oh, gosh]   \n",
              "1            [trouble, sleeping, confused, mind, restless, heart, tune]   \n",
              "2  [wrong, back, dear, forward, doubt, stay, restless, restless, place]   \n",
              "3                     [shifted, focus, something, else, still, worried]   \n",
              "4                                [restless, restless, month, boy, mean]   \n",
              "\n",
              "                                                        lemmatized_text  \n",
              "0                                                            [oh, gosh]  \n",
              "1            [trouble, sleeping, confused, mind, restless, heart, tune]  \n",
              "2  [wrong, back, dear, forward, doubt, stay, restless, restless, place]  \n",
              "3                     [shifted, focus, something, else, still, worried]  \n",
              "4                                [restless, restless, month, boy, mean]  "
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "9c67097c",
      "metadata": {
        "id": "9c67097c"
      },
      "outputs": [],
      "source": [
        "X=df['lemmatized_text']\n",
        "y=df['Label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "34628a3d",
      "metadata": {
        "id": "34628a3d"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "9e47a74a",
      "metadata": {
        "id": "9e47a74a"
      },
      "outputs": [],
      "source": [
        "X_train,X_test,y_train,y_test= train_test_split(X,y,test_size=0.15,random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "229cd456",
      "metadata": {
        "id": "229cd456"
      },
      "source": [
        "### Converting embeddings to tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "18f70934",
      "metadata": {
        "id": "18f70934"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3b62798",
      "metadata": {
        "id": "d3b62798"
      },
      "outputs": [],
      "source": [
        "def convert_sequences_to_tensor(sequences, num_tokens_in_sequence, embedding_size):\n",
        "    '''\n",
        "    We want a torch.FloatTensor() of size (num_sequences, num_tokens_in_sequence, embedding_size)\n",
        "    '''\n",
        "    num_sequences = len(sequences)\n",
        "    print((num_sequences, num_tokens_in_sequence, embedding_size))\n",
        "\n",
        "    data_tensor = torch.zeros((num_sequences, num_tokens_in_sequence, embedding_size))\n",
        "\n",
        "    for index, review in enumerate(list(sequences)):\n",
        "        # Create a word embedding for each word in the review (where a review is a sequence)\n",
        "        truncated_clean_review = review[:num_tokens_in_sequence]  # truncate to sequence length limit\n",
        "\n",
        "        if len(truncated_clean_review) == 0: # accounting for the case where some words might not be recognized by the word2vector model\n",
        "            continue\n",
        "\n",
        "        list_of_word_embeddings = [\n",
        "            model_cbow.wv[word] if word in model_cbow.wv else [0.0] * embedding_size for word in truncated_clean_review\n",
        "        ]\n",
        "\n",
        "        # convert the review to a tensor\n",
        "        sequence_tensor = torch.FloatTensor(list_of_word_embeddings)\n",
        "\n",
        "        # add the review to our tensor of data\n",
        "        review_length = sequence_tensor.shape[0]  # (review_length, embedding_size)\n",
        "        data_tensor[index, :review_length, :] = sequence_tensor\n",
        "\n",
        "    return data_tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "102c2d05",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "102c2d05",
        "outputId": "26aa4b1e-5b2a-4916-da60-90e577175d2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(43412, 125, 300)\n",
            "(7661, 125, 300)\n"
          ]
        }
      ],
      "source": [
        "train_data_X = convert_sequences_to_tensor(X_train.to_numpy(), sequence_length, embedding_size)\n",
        "train_data_y = torch.FloatTensor([int(d) for d in y_train.to_numpy()])\n",
        "\n",
        "test_data_X = convert_sequences_to_tensor(X_test.to_numpy(), sequence_length, embedding_size)\n",
        "test_data_y = torch.FloatTensor([int(d) for d in y_test.to_numpy()])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0100e4f",
      "metadata": {
        "id": "a0100e4f"
      },
      "source": [
        "#### Double checking for consistency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c96add56",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c96add56",
        "outputId": "f234e7a9-666f-4aca-9ea5-3c5df657e12b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(43412, 43412)"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_data_X),len(train_data_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d3e341a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d3e341a",
        "outputId": "de49eb76-427b-49f8-b1c8-f156c356a4f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(7661, 7661)"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(test_data_X),len(test_data_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f66a837",
      "metadata": {
        "id": "7f66a837"
      },
      "source": [
        "### Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "414c4799",
      "metadata": {
        "id": "414c4799"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4c34b36",
      "metadata": {
        "id": "a4c34b36"
      },
      "outputs": [],
      "source": [
        "train_data = TensorDataset(train_data_X, train_data_y)\n",
        "test_data = TensorDataset(test_data_X, test_data_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4638d7ac",
      "metadata": {
        "id": "4638d7ac"
      },
      "outputs": [],
      "source": [
        "batch_size=25\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4d538fb",
      "metadata": {},
      "source": [
        "# LSTM Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54b17c49",
      "metadata": {
        "id": "54b17c49"
      },
      "source": [
        "### Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa7723e4",
      "metadata": {
        "id": "fa7723e4"
      },
      "source": [
        "#### Monitoring our Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1983297",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1983297",
        "outputId": "91a699e0-6cf5-4317-f998-ba2749256695"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[ 0.0232, -0.2021,  0.2335,  ..., -0.0040,  0.1520,  0.0167],\n",
              "         [-1.0337, -0.6775, -1.2836,  ..., -0.8939,  0.6153, -0.2091],\n",
              "         [ 0.5585, -0.8154,  0.3793,  ...,  0.0377,  0.1625, -0.2452],\n",
              "         ...,\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]),\n",
              " tensor(1.))"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c164199",
      "metadata": {
        "id": "2c164199"
      },
      "source": [
        "#### implementing our class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "6900bb65",
      "metadata": {
        "id": "6900bb65"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "6874c3dd",
      "metadata": {
        "id": "6874c3dd"
      },
      "outputs": [],
      "source": [
        "device ='cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8501c9a",
      "metadata": {
        "id": "d8501c9a"
      },
      "source": [
        "The following code adopt a similar structure to the rnn and gru implementations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "575e2add",
      "metadata": {
        "id": "575e2add"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self,input_size, hidden_size,num_layers, num_classes):\n",
        "        super(LSTM,self).__init__()\n",
        "        self.num_layers=num_layers\n",
        "        self.hidden_size=hidden_size\n",
        "        self.lstm= nn.LSTM(input_size,hidden_size,num_layers,batch_first=True) # batch must be in first dimension\n",
        "        # if batch first was set to true, input shape: batch size,sequence number, features size\n",
        "        self.fc=nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0=torch.zeros(self.num_layers, x.size(0),self.hidden_size).to(device)\n",
        "        c0=torch.zeros(self.num_layers, x.size(0),self.hidden_size).to(device)\n",
        "        out, _= self.lstm(x,(h0,c0)) # we do not need the second returned output (h_n, c_n) - final states at each layer\n",
        "        out=out[:,-1,:]\n",
        "        out = self.fc(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b7f7ea0",
      "metadata": {
        "id": "3b7f7ea0"
      },
      "source": [
        "### Model 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "295de17c",
      "metadata": {
        "id": "295de17c"
      },
      "outputs": [],
      "source": [
        "hidden_size=256\n",
        "num_layers=3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "095e6a0a",
      "metadata": {
        "id": "095e6a0a"
      },
      "outputs": [],
      "source": [
        "lstm_classifier_1=LSTM(input_size=embedding_size,hidden_size=hidden_size,num_layers=num_layers,num_classes=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ffbb81a",
      "metadata": {
        "id": "7ffbb81a"
      },
      "outputs": [],
      "source": [
        "learning_rate=0.001 # setting the lea\n",
        "# for the optimizer, we may use either the adam or the sgd optimizers\n",
        "optimizer = Adam(lstm_classifier_1.parameters(), lr=learning_rate)\n",
        "# we are dealing with multi class classification problem, thus will use cross entropy loss\n",
        "loss_criterion=nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68e946f6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68e946f6",
        "outputId": "5070faba-203f-4107-ab27-eebb03c10992"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1625092"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's see how many Parameters our Model has!\n",
        "num_model_params = 0\n",
        "for param in lstm_classifier_1.parameters():\n",
        "    num_model_params += param.flatten().shape[0]\n",
        "\n",
        "num_model_params"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2d2f3bc",
      "metadata": {
        "id": "c2d2f3bc"
      },
      "source": [
        "#### Training phase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uaB8aZsAJre1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaB8aZsAJre1",
        "outputId": "be6f2aa0-58f1-40f0-ac82-935392e724aa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LSTM(\n",
              "  (lstm): LSTM(300, 256, num_layers=3, batch_first=True)\n",
              "  (fc): Linear(in_features=256, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lstm_classifier_1.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca78ff84",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca78ff84",
        "outputId": "f2b561b1-2930-4822-853d-623cd7a61009"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Loss: 0.869624037936836\n",
            "Epoch [2/10], Loss: 0.6256229815962813\n",
            "Epoch [3/10], Loss: 0.5680555929448595\n",
            "Epoch [4/10], Loss: 0.521328924833939\n",
            "Epoch [5/10], Loss: 0.47743216730015126\n",
            "Epoch [6/10], Loss: 0.43954661914893245\n",
            "Epoch [7/10], Loss: 0.39236406348409775\n",
            "Epoch [8/10], Loss: 0.3482256204353095\n",
            "Epoch [9/10], Loss: 0.3052753558662271\n",
            "Epoch [10/10], Loss: 0.26389656927507366\n"
          ]
        }
      ],
      "source": [
        "epochs=10\n",
        "\n",
        "# Initialize training and test accuracy\n",
        "train_acc = 0\n",
        "test_acc = 0\n",
        "\n",
        "# Loop through each epoch\n",
        "for epoch in range(epochs):\n",
        "    # Update progress bar description with current accuracy\n",
        "\n",
        "    # Set model to training mode\n",
        "    lstm_classifier_1.train()\n",
        "    running_loss=0.0\n",
        "    # Iterate through training data loader\n",
        "    for text,labels in train_loader:\n",
        "        # bs = labels.shape[0]\n",
        "\n",
        "        text=text.to(device)\n",
        "        labels = labels.long().to(device)\n",
        "\n",
        "        # print('text tensor:',text)\n",
        "        # print('text shape:',text.shape)\n",
        "        # print('label tensor:',label)\n",
        "\n",
        "        # Initialize hidden and memory states\n",
        "        # hidden = torch.zeros(num_layers, bs, hidden_size, device=device)\n",
        "        # memory = torch.zeros(num_layers, bs, hidden_size, device=device)\n",
        "\n",
        "        # Forward pass through the model\n",
        "        outputs = lstm_classifier_1(text)\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = loss_criterion(outputs, labels)\n",
        "        running_loss+=loss.item()\n",
        "\n",
        "        # Backpropagation and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    average_loss=running_loss/ len(train_loader)\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {average_loss}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8PQFoosO6dw",
      "metadata": {
        "id": "d8PQFoosO6dw"
      },
      "source": [
        "Let us try to increase the number of epochs to 15 to allow our model to train more"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7v5Gw4UGL-zN",
      "metadata": {
        "id": "7v5Gw4UGL-zN"
      },
      "outputs": [],
      "source": [
        "lstm_classifier_2=LSTM(input_size=embedding_size,hidden_size=hidden_size,num_layers=num_layers,num_classes=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aUPygGaSPDph",
      "metadata": {
        "id": "aUPygGaSPDph"
      },
      "outputs": [],
      "source": [
        "learning_rate=0.001\n",
        "optimizer = Adam(lstm_classifier_2.parameters(), lr=learning_rate)\n",
        "loss_criterion=nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "v7EJYPDCPIFY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7EJYPDCPIFY",
        "outputId": "6f7340c1-f640-4878-a339-973e25010ea1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LSTM(\n",
              "  (lstm): LSTM(300, 256, num_layers=3, batch_first=True)\n",
              "  (fc): Linear(in_features=256, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lstm_classifier_2.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GCsi029JPIB1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCsi029JPIB1",
        "outputId": "32f77609-f2dc-4051-d66e-500530d7835a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 0.9710846660668369\n",
            "Epoch [2/15], Loss: 0.6601907367258835\n",
            "Epoch [3/15], Loss: 0.5744890228179795\n",
            "Epoch [4/15], Loss: 0.5212915767848526\n",
            "Epoch [5/15], Loss: 0.577023297522558\n",
            "Epoch [6/15], Loss: 0.5067762620125136\n",
            "Epoch [7/15], Loss: 0.4611450318090519\n",
            "Epoch [8/15], Loss: 0.4214520062790153\n",
            "Epoch [9/15], Loss: 0.3780832311366918\n",
            "Epoch [10/15], Loss: 0.33816815543411516\n",
            "Epoch [11/15], Loss: 0.29416516745783644\n",
            "Epoch [12/15], Loss: 0.2549882162583238\n",
            "Epoch [13/15], Loss: 0.21537091436532632\n",
            "Epoch [14/15], Loss: 0.18418409042092546\n",
            "Epoch [15/15], Loss: 0.1616770026102988\n"
          ]
        }
      ],
      "source": [
        "epochs=15\n",
        "\n",
        "# Initialize training and test accuracy\n",
        "train_acc = 0\n",
        "test_acc = 0\n",
        "\n",
        "# Loop through each epoch\n",
        "for epoch in range(epochs):\n",
        "    # Update progress bar description with current accuracy\n",
        "\n",
        "    # Set model to training mode\n",
        "    lstm_classifier_2.train()\n",
        "    running_loss=0.0\n",
        "    # Iterate through training data loader\n",
        "    for text,labels in train_loader:\n",
        "        # bs = labels.shape[0]\n",
        "\n",
        "        text=text.to(device)\n",
        "        labels = labels.long().to(device)\n",
        "\n",
        "        # print('text tensor:',text)\n",
        "        # print('text shape:',text.shape)\n",
        "        # print('label tensor:',label)\n",
        "\n",
        "        # Initialize hidden and memory states\n",
        "        # hidden = torch.zeros(num_layers, bs, hidden_size, device=device)\n",
        "        # memory = torch.zeros(num_layers, bs, hidden_size, device=device)\n",
        "\n",
        "        # Forward pass through the model\n",
        "        outputs = lstm_classifier_2(text)\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = loss_criterion(outputs, labels)\n",
        "        running_loss+=loss.item()\n",
        "\n",
        "        # Backpropagation and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    average_loss=running_loss/ len(train_loader)\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {average_loss}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8jg756ttRFGX",
      "metadata": {
        "id": "8jg756ttRFGX"
      },
      "source": [
        "Let us now evaluate the performance of our model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WagKVepXTsBa",
      "metadata": {
        "id": "WagKVepXTsBa"
      },
      "source": [
        "#### Creating Our evaluation method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "FBMtsc3XPSA7",
      "metadata": {
        "id": "FBMtsc3XPSA7"
      },
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "KQmSnUycRSDA",
      "metadata": {
        "id": "KQmSnUycRSDA"
      },
      "outputs": [],
      "source": [
        "# i used insights from pattrick lobber github cnn code and tensorboard video to develop the following method, refrence provided in report\n",
        "import torch.nn.functional as F\n",
        "def evaluate_model(model, test_dataloader, loss_criterion, device, writer=None):\n",
        "    model.eval()\n",
        "    total_loss=0.0\n",
        "    total_samples=0\n",
        "    correct_classifications=0\n",
        "    # for the pr curve\n",
        "    all_probs=[]\n",
        "    all_labels=[]\n",
        "    size=len(test_dataloader) # dataset size\n",
        "    with torch.no_grad():\n",
        "        for text,labels in test_dataloader:\n",
        "            text=text.to(device)\n",
        "            labels=labels.long().to(device)\n",
        "\n",
        "            outputs=model(text)\n",
        "            loss=loss_criterion(outputs,labels)\n",
        "            total_loss+=loss.item()\n",
        "\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "            predicted_classes = torch.argmax(outputs, dim=1)\n",
        "            correct_classifications += (predicted_classes == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "            all_probs.extend(probs.cpu().tolist())\n",
        "            all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "    avg_class_loss=total_loss/size\n",
        "    classification_accuracy=100.0*(correct_classifications/total_samples)\n",
        "\n",
        "    print(f\"Average Classification Loss: {avg_class_loss}\")\n",
        "    print(f\"Classification Accuracy: {classification_accuracy}%\")\n",
        "\n",
        "\n",
        "    if writer is not None: # i want to use a writer for every model so that i may see the results and compare them\n",
        "        writer.add_scalar('Loss/Classification', avg_class_loss, 0)\n",
        "        writer.add_scalar('Accuracy/Classification', classification_accuracy, 0)\n",
        "        # log per class the pr curves\n",
        "        for class_index in range(len(all_probs[0])):\n",
        "            writer.add_pr_curve(\n",
        "                f'PR_Curve/Class_{class_index}',\n",
        "                torch.tensor([label == class_index for label in all_labels], dtype=torch.uint8),\n",
        "                torch.tensor([prob[class_index] for prob in all_probs]),\n",
        "                global_step=0\n",
        "            )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1UHROCXrTwPy",
      "metadata": {
        "id": "1UHROCXrTwPy"
      },
      "source": [
        "#### evaluating our first model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6sa4YpvOTzh6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sa4YpvOTzh6",
        "outputId": "915d17ac-d6cc-4f8b-d87c-616458f5d3a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Classification Loss: 0.8348020230714195\n",
            "Classification Accuracy: 76.34773528260018%\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(lstm_classifier_2, test_loader, loss_criterion, device, writer=SummaryWriter('runs/model_1'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hu_lKk-60X_4",
      "metadata": {
        "id": "hu_lKk-60X_4"
      },
      "source": [
        "### Finetuning our Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Uyq8bWXE0ekt",
      "metadata": {
        "id": "Uyq8bWXE0ekt"
      },
      "source": [
        "first, we will start by changing the learning rate to half the past value. Moreover, I will try setting the hidden size to 512 and train the model for 15 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "s3Ybe-7l2bvj",
      "metadata": {
        "id": "s3Ybe-7l2bvj"
      },
      "outputs": [],
      "source": [
        "hidden_size=512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BOcJMEaK2LZs",
      "metadata": {
        "id": "BOcJMEaK2LZs"
      },
      "outputs": [],
      "source": [
        "lstm_classifier_3=LSTM(input_size=embedding_size,hidden_size=hidden_size,num_layers=num_layers,num_classes=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bVrFI1O_2LZt",
      "metadata": {
        "id": "bVrFI1O_2LZt"
      },
      "outputs": [],
      "source": [
        "learning_rate=learning_rate/2\n",
        "optimizer = Adam(lstm_classifier_3.parameters(), lr=learning_rate)\n",
        "loss_criterion=nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rSahfYkU2LZt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSahfYkU2LZt",
        "outputId": "5e3c6f76-b29f-45dc-a822-304de1a3f911"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LSTM(\n",
              "  (lstm): LSTM(300, 512, num_layers=3, batch_first=True)\n",
              "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lstm_classifier_3.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "n5XMWynO2LZt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5XMWynO2LZt",
        "outputId": "c32ef895-3d91-4ee0-f771-bf4d875397c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.1543518596499833\n",
            "Epoch [2/15], Loss: 0.8748918678407773\n",
            "Epoch [3/15], Loss: 0.6540343460847265\n",
            "Epoch [4/15], Loss: 0.5758533428370026\n",
            "Epoch [5/15], Loss: 0.5250701154520543\n",
            "Epoch [6/15], Loss: 0.47981499908004055\n",
            "Epoch [7/15], Loss: 0.42529954976162554\n",
            "Epoch [8/15], Loss: 0.37064207368409696\n",
            "Epoch [9/15], Loss: 0.3145915299726926\n",
            "Epoch [10/15], Loss: 0.2588949281417764\n",
            "Epoch [11/15], Loss: 0.21059165045696218\n",
            "Epoch [12/15], Loss: 0.16848075825551598\n",
            "Epoch [13/15], Loss: 0.1360465545203509\n",
            "Epoch [14/15], Loss: 0.11407547248001963\n",
            "Epoch [15/15], Loss: 0.09319619160286532\n"
          ]
        }
      ],
      "source": [
        "epochs=15\n",
        "\n",
        "# Initialize training and test accuracy\n",
        "train_acc = 0\n",
        "test_acc = 0\n",
        "\n",
        "# Loop through each epoch\n",
        "for epoch in range(epochs):\n",
        "    # Update progress bar description with current accuracy\n",
        "\n",
        "    # Set model to training mode\n",
        "    lstm_classifier_3.train()\n",
        "    running_loss=0.0\n",
        "    # Iterate through training data loader\n",
        "    for text,labels in train_loader:\n",
        "        # bs = labels.shape[0]\n",
        "\n",
        "        text=text.to(device)\n",
        "        labels = labels.long().to(device)\n",
        "\n",
        "        # print('text tensor:',text)\n",
        "        # print('text shape:',text.shape)\n",
        "        # print('label tensor:',label)\n",
        "\n",
        "        # Initialize hidden and memory states\n",
        "        # hidden = torch.zeros(num_layers, bs, hidden_size, device=device)\n",
        "        # memory = torch.zeros(num_layers, bs, hidden_size, device=device)\n",
        "\n",
        "        # Forward pass through the model\n",
        "        outputs = lstm_classifier_3(text)\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = loss_criterion(outputs, labels)\n",
        "        running_loss+=loss.item()\n",
        "\n",
        "        # Backpropagation and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    average_loss=running_loss/ len(train_loader)\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {average_loss}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gwY2LMe25Ug2",
      "metadata": {
        "id": "gwY2LMe25Ug2"
      },
      "source": [
        "#### evaluating our finetuned model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "t41lzEHG5T2k",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t41lzEHG5T2k",
        "outputId": "52fc7204-6313-4e3b-b2fc-a3016bc11dfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Classification Loss: 1.0187871278254528\n",
            "Classification Accuracy: 76.0997258843493%\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(lstm_classifier_3, test_loader, loss_criterion, device, writer=SummaryWriter('runs/model_2'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Q7OgfEpd5ily",
      "metadata": {
        "id": "Q7OgfEpd5ily"
      },
      "source": [
        "Athough the model loss in the training phase decreased in comparison with the our first model, a similar (slightly less) accuracy was obtained on the test data. Thus, this means that our model is overfitting, thats why we will keep our hidden layers number as is and introduce dropouts. Note that from now on, we will adopt our new learning rate."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7dDXjA_E5kTq",
      "metadata": {
        "id": "7dDXjA_E5kTq"
      },
      "source": [
        "### Adding dropouts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0c1ad52",
      "metadata": {},
      "source": [
        "We will add dropouts so that our model becomes prone to overfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dGw22-4d6WdT",
      "metadata": {
        "id": "dGw22-4d6WdT"
      },
      "outputs": [],
      "source": [
        "class LSTM_V1(nn.Module):\n",
        "    def __init__(self,input_size, hidden_size,num_layers, num_classes):\n",
        "        super(LSTM_V1,self).__init__()\n",
        "        self.num_layers=num_layers\n",
        "        self.hidden_size=hidden_size\n",
        "        self.lstm= nn.LSTM(input_size,hidden_size,num_layers,batch_first=True,dropout=0.3)\n",
        "        self.fc=nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0=torch.zeros(self.num_layers, x.size(0),self.hidden_size).to(device)\n",
        "        c0=torch.zeros(self.num_layers, x.size(0),self.hidden_size).to(device)\n",
        "        out, _= self.lstm(x,(h0,c0)) # we do not need the second returned output (h_n)\n",
        "        out=out[:,-1,:]\n",
        "        out = self.fc(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0xF8lig-6oFx",
      "metadata": {
        "id": "0xF8lig-6oFx"
      },
      "outputs": [],
      "source": [
        "lstm_classifier_4=LSTM_V1(input_size=embedding_size,hidden_size=hidden_size,num_layers=num_layers,num_classes=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84_P4U9T6oFx",
      "metadata": {
        "id": "84_P4U9T6oFx"
      },
      "outputs": [],
      "source": [
        "optimizer = Adam(lstm_classifier_4.parameters(), lr=learning_rate)\n",
        "loss_criterion=nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y-F9RZ9Q6oFx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-F9RZ9Q6oFx",
        "outputId": "3b138346-6bb2-44ba-e4e5-c22966738424"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LSTM_V1(\n",
              "  (lstm): LSTM(300, 512, num_layers=3, batch_first=True, dropout=0.3)\n",
              "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lstm_classifier_4.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "X2PPP8Ex6oFx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2PPP8Ex6oFx",
        "outputId": "9a7fc767-e8ee-4178-ac22-f048e18326d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 0.9597894063645145\n",
            "Epoch [2/15], Loss: 0.7186096153282612\n",
            "Epoch [3/15], Loss: 0.6458067005659975\n",
            "Epoch [4/15], Loss: 0.6084104004987849\n",
            "Epoch [5/15], Loss: 0.5695538812230632\n",
            "Epoch [6/15], Loss: 0.5348890919279078\n",
            "Epoch [7/15], Loss: 0.5068962416757985\n",
            "Epoch [8/15], Loss: 0.4740452340444272\n",
            "Epoch [9/15], Loss: 0.4441381752722622\n",
            "Epoch [10/15], Loss: 0.41191451802686774\n",
            "Epoch [11/15], Loss: 0.37555808124438605\n",
            "Epoch [12/15], Loss: 0.3385550942471402\n",
            "Epoch [13/15], Loss: 0.2978308242378423\n",
            "Epoch [14/15], Loss: 0.25740316094670856\n",
            "Epoch [15/15], Loss: 0.22228090015867866\n"
          ]
        }
      ],
      "source": [
        "epochs=15\n",
        "\n",
        "# Initialize training and test accuracy\n",
        "train_acc = 0\n",
        "test_acc = 0\n",
        "\n",
        "# Loop through each epoch\n",
        "for epoch in range(epochs):\n",
        "    # Update progress bar description with current accuracy\n",
        "\n",
        "    # Set model to training mode\n",
        "    lstm_classifier_4.train()\n",
        "    running_loss=0.0\n",
        "    # Iterate through training data loader\n",
        "    for text,labels in train_loader:\n",
        "        # bs = labels.shape[0]\n",
        "\n",
        "        text=text.to(device)\n",
        "        labels = labels.long().to(device)\n",
        "\n",
        "        # print('text tensor:',text)\n",
        "        # print('text shape:',text.shape)\n",
        "        # print('label tensor:',label)\n",
        "\n",
        "        # Initialize hidden and memory states\n",
        "        # hidden = torch.zeros(num_layers, bs, hidden_size, device=device)\n",
        "        # memory = torch.zeros(num_layers, bs, hidden_size, device=device)\n",
        "\n",
        "        # Forward pass through the model\n",
        "        outputs = lstm_classifier_4(text)\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = loss_criterion(outputs, labels)\n",
        "        running_loss+=loss.item()\n",
        "\n",
        "        # Backpropagation and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    average_loss=running_loss/ len(train_loader)\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {average_loss}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KdQWwPPK6oFy",
      "metadata": {
        "id": "KdQWwPPK6oFy"
      },
      "source": [
        "#### evaluating our finetuned model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ao0-j9NW6oFy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ao0-j9NW6oFy",
        "outputId": "c60f465f-75b9-4ecd-dfd0-ce6b817420c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Classification Loss: 0.7751262835939855\n",
            "Classification Accuracy: 77.0395509724579%\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(lstm_classifier_4, test_loader, loss_criterion, device, writer=SummaryWriter('runs/model_3'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "okhmpIjL6_aH",
      "metadata": {
        "id": "okhmpIjL6_aH"
      },
      "source": [
        "There is an increase in the accuracy in comparison with the previous models with respect to the test data. Better results will be achieved in the model is trained for 5 more epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kD4J5Go07AGZ",
      "metadata": {
        "id": "kD4J5Go07AGZ"
      },
      "source": [
        "### Trying the bidirectional LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "8p-ClRcS7ExJ",
      "metadata": {
        "id": "8p-ClRcS7ExJ"
      },
      "outputs": [],
      "source": [
        "class LSTM_V2(nn.Module):\n",
        "    def __init__(self,input_size, hidden_size,num_layers, num_classes):\n",
        "        super(LSTM_V2,self).__init__()\n",
        "        self.num_layers=num_layers\n",
        "        self.hidden_size=hidden_size\n",
        "        self.lstm= nn.LSTM(input_size,hidden_size,num_layers,batch_first=True,dropout=0.3,bidirectional=True) # batch must be in first dimension\n",
        "        # if batch first was set to true, input shape: batch size,sequence number, features size\n",
        "        self.fc=nn.Linear(hidden_size*2, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        num_directions = 2  # we need to accomodate the hidden and cell states to both directions\n",
        "        h0=torch.zeros(self.num_layers*num_directions, x.size(0),self.hidden_size).to(device)\n",
        "        c0=torch.zeros(self.num_layers*num_directions, x.size(0),self.hidden_size).to(device)\n",
        "        out, _= self.lstm(x,(h0,c0)) # we do not need the second returned output (h_n)\n",
        "        out=out[:,-1,:]\n",
        "        out = self.fc(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oJhD1XHEDicO",
      "metadata": {
        "id": "oJhD1XHEDicO"
      },
      "source": [
        "we will be now processing in two directions, one from the start to the end and the other vice versa. For each directional step we need to use one hidden size. Thats why we will need to double the hidden size for the last layer to concatenate the forward and backward outputs. Thus, it will be reasonable to drop the hidden size to 256 ( if it remains 512, we will recieve an input of size 1024 at the last layer, resulting in increasing overfitting risk, extra memory usage, and slower training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QAd5TJECBZIo",
      "metadata": {
        "id": "QAd5TJECBZIo"
      },
      "outputs": [],
      "source": [
        "hidden_size=256\n",
        "lstm_classifier_5=LSTM_V2(input_size=embedding_size,hidden_size=hidden_size,num_layers=num_layers,num_classes=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mCKYaBdWBZIp",
      "metadata": {
        "id": "mCKYaBdWBZIp"
      },
      "outputs": [],
      "source": [
        "optimizer = Adam(lstm_classifier_5.parameters(), lr=learning_rate)\n",
        "loss_criterion=nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nhm7usKIBZIp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhm7usKIBZIp",
        "outputId": "ff3cf630-6116-475d-fe00-f0b9899fe951"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LSTM_V2(\n",
              "  (lstm): LSTM(300, 256, num_layers=3, batch_first=True, dropout=0.3, bidirectional=True)\n",
              "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lstm_classifier_5.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cU2-OGnBZIp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cU2-OGnBZIp",
        "outputId": "17554dea-da75-42d4-b7d9-34d4b083e25c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 0.8691808864791708\n",
            "Epoch [2/15], Loss: 0.6515010031874179\n",
            "Epoch [3/15], Loss: 0.5873475208681208\n",
            "Epoch [4/15], Loss: 0.53680898607738\n",
            "Epoch [5/15], Loss: 0.4944985058452456\n",
            "Epoch [6/15], Loss: 0.4554186855830471\n",
            "Epoch [7/15], Loss: 0.4168819786404904\n",
            "Epoch [8/15], Loss: 0.3780861123238203\n",
            "Epoch [9/15], Loss: 0.334465088336699\n",
            "Epoch [10/15], Loss: 0.29048477168364917\n",
            "Epoch [11/15], Loss: 0.2520993145297469\n",
            "Epoch [12/15], Loss: 0.2199809644521398\n",
            "Epoch [13/15], Loss: 0.18602034305938117\n",
            "Epoch [14/15], Loss: 0.16189876927312724\n",
            "Epoch [15/15], Loss: 0.13929175232871124\n"
          ]
        }
      ],
      "source": [
        "epochs=15\n",
        "\n",
        "# Initialize training and test accuracy\n",
        "train_acc = 0\n",
        "test_acc = 0\n",
        "\n",
        "# Loop through each epoch\n",
        "for epoch in range(epochs):\n",
        "    # Update progress bar description with current accuracy\n",
        "\n",
        "    # Set model to training mode\n",
        "    lstm_classifier_5.train()\n",
        "    running_loss=0.0\n",
        "    # Iterate through training data loader\n",
        "    for text,labels in train_loader:\n",
        "        # bs = labels.shape[0]\n",
        "\n",
        "        text=text.to(device)\n",
        "        labels = labels.long().to(device)\n",
        "\n",
        "        # print('text tensor:',text)\n",
        "        # print('text shape:',text.shape)\n",
        "        # print('label tensor:',label)\n",
        "\n",
        "        # Initialize hidden and memory states\n",
        "        # hidden = torch.zeros(num_layers, bs, hidden_size, device=device)\n",
        "        # memory = torch.zeros(num_layers, bs, hidden_size, device=device)\n",
        "\n",
        "        # Forward pass through the model\n",
        "        outputs = lstm_classifier_5(text)\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = loss_criterion(outputs, labels)\n",
        "        running_loss+=loss.item()\n",
        "\n",
        "        # Backpropagation and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    average_loss=running_loss/ len(train_loader)\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {average_loss}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "okGe010yBZIp",
      "metadata": {
        "id": "okGe010yBZIp"
      },
      "source": [
        "#### evaluating our finetuned model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "To08kqE7BZIp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "To08kqE7BZIp",
        "outputId": "b8b2e377-3956-4822-c1e2-23f5d3a2cb97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Classification Loss: 0.8172653004745513\n",
            "Classification Accuracy: 77.97937606056651%\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(lstm_classifier_5, test_loader, loss_criterion, device, writer=SummaryWriter('runs/model_4'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5h8_-fVIJ9To",
      "metadata": {
        "id": "5h8_-fVIJ9To"
      },
      "source": [
        "There is a slight increase in the classification accuaracy, but this is not enough. This being said, I will try now to increase the number of training epochs to 20, apply L2 regularization to our adam optimizer, and set the learning rate to half its past value."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3mp-IzDuKe2D",
      "metadata": {
        "id": "3mp-IzDuKe2D"
      },
      "source": [
        "#### Bidirectional LSTM V2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ToQB3YRyKom1",
      "metadata": {
        "id": "ToQB3YRyKom1"
      },
      "outputs": [],
      "source": [
        "hidden_size=256\n",
        "lstm_classifier_6=LSTM_V2(input_size=embedding_size,hidden_size=hidden_size,num_layers=num_layers,num_classes=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b_NFpEsrKoje",
      "metadata": {
        "id": "b_NFpEsrKoje"
      },
      "outputs": [],
      "source": [
        "learning_rate=learning_rate/2\n",
        "optimizer = Adam(lstm_classifier_6.parameters(), lr=learning_rate,weight_decay=0.00001)\n",
        "loss_criterion=nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ZytpUJoLWNW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZytpUJoLWNW",
        "outputId": "5ea6ff85-2b8b-4052-8d7b-581121a11d36"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LSTM_V3(\n",
              "  (lstm): LSTM(300, 256, num_layers=3, batch_first=True, dropout=0.3, bidirectional=True)\n",
              "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lstm_classifier_6.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CxoZMSigK6Ur",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxoZMSigK6Ur",
        "outputId": "eceb3ff8-8f9d-4041-f410-3a3d0bde572d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20], Loss: 0.9437159943079633\n",
            "Epoch [2/20], Loss: 0.6818764236255425\n",
            "Epoch [3/20], Loss: 0.6097592472875674\n",
            "Epoch [4/20], Loss: 0.5635290157798384\n",
            "Epoch [5/20], Loss: 0.5292897775091518\n",
            "Epoch [6/20], Loss: 0.5006692680687347\n",
            "Epoch [7/20], Loss: 0.47277386296941554\n",
            "Epoch [8/20], Loss: 0.44738339644759706\n",
            "Epoch [9/20], Loss: 0.41762979287454705\n",
            "Epoch [10/20], Loss: 0.38928236217421913\n",
            "Epoch [11/20], Loss: 0.3567889538852559\n",
            "Epoch [12/20], Loss: 0.3261091331911108\n",
            "Epoch [13/20], Loss: 0.2973258737077916\n",
            "Epoch [14/20], Loss: 0.26597250512038145\n",
            "Epoch [15/20], Loss: 0.23799118341070588\n",
            "Epoch [16/20], Loss: 0.20884197608717364\n",
            "Epoch [17/20], Loss: 0.19084492343369064\n",
            "Epoch [18/20], Loss: 0.16921060990379178\n",
            "Epoch [19/20], Loss: 0.15235633333521922\n",
            "Epoch [20/20], Loss: 0.13959115778528172\n"
          ]
        }
      ],
      "source": [
        "epochs=20\n",
        "\n",
        "# Initialize training and test accuracy\n",
        "train_acc = 0\n",
        "test_acc = 0\n",
        "\n",
        "# Loop through each epoch\n",
        "for epoch in range(epochs):\n",
        "    # Update progress bar description with current accuracy\n",
        "\n",
        "    # Set model to training mode\n",
        "    lstm_classifier_6.train()\n",
        "    running_loss=0.0\n",
        "    # Iterate through training data loader\n",
        "    for text,labels in train_loader:\n",
        "        # bs = labels.shape[0]\n",
        "\n",
        "        text=text.to(device)\n",
        "        labels = labels.long().to(device)\n",
        "\n",
        "        # print('text tensor:',text)\n",
        "        # print('text shape:',text.shape)\n",
        "        # print('label tensor:',label)\n",
        "\n",
        "        # Initialize hidden and memory states\n",
        "        # hidden = torch.zeros(num_layers, bs, hidden_size, device=device)\n",
        "        # memory = torch.zeros(num_layers, bs, hidden_size, device=device)\n",
        "\n",
        "        # Forward pass through the model\n",
        "        outputs = lstm_classifier_6(text)\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = loss_criterion(outputs, labels)\n",
        "        running_loss+=loss.item()\n",
        "\n",
        "        # Backpropagation and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    average_loss=running_loss/ len(train_loader)\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {average_loss}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gZ7WxgVULZnw",
      "metadata": {
        "id": "gZ7WxgVULZnw"
      },
      "source": [
        "#### evaluating our bi-lstm v2 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q0W2IXAzK6HV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0W2IXAzK6HV",
        "outputId": "c75a9085-5d8b-4b79-ddcc-d1fb09703c32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Classification Loss: 0.8385523604259428\n",
            "Classification Accuracy: 76.28246965148153%\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(lstm_classifier_6, test_loader, loss_criterion, device, writer=SummaryWriter('runs/model_5'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dUmDx8oOLgZ",
      "metadata": {
        "id": "3dUmDx8oOLgZ"
      },
      "source": [
        "Based on the current results, there is a need to increase the dropout parameter to 0.5 and decrease the number of layers, it is clear that our model is overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EdJTlPVlS09D",
      "metadata": {
        "id": "EdJTlPVlS09D"
      },
      "source": [
        "### Bidirectional LSTM V3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "coDwdrpbS6n0",
      "metadata": {
        "id": "coDwdrpbS6n0"
      },
      "outputs": [],
      "source": [
        "class LSTM_V3(nn.Module):\n",
        "    def __init__(self,input_size, hidden_size,num_layers, num_classes):\n",
        "        super(LSTM_V3,self).__init__()\n",
        "        self.num_layers=num_layers\n",
        "        self.hidden_size=hidden_size\n",
        "        self.lstm= nn.LSTM(input_size,hidden_size,num_layers,batch_first=True,dropout=0.5,bidirectional=True) # batch must be in first dimension\n",
        "        # if batch first was set to true, input shape: batch size,sequence number, features size\n",
        "        self.fc=nn.Linear(hidden_size*2, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        num_directions = 2  # we need to accomodate the hidden and cell states to both directions\n",
        "        h0=torch.zeros(self.num_layers*num_directions, x.size(0),self.hidden_size).to(device)\n",
        "        c0=torch.zeros(self.num_layers*num_directions, x.size(0),self.hidden_size).to(device)\n",
        "        out, _= self.lstm(x,(h0,c0)) # we do not need the second returned output (h_n)\n",
        "        out=out[:,-1,:]\n",
        "        out = self.fc(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bPVtnclPTOYZ",
      "metadata": {
        "id": "bPVtnclPTOYZ"
      },
      "outputs": [],
      "source": [
        "hidden_size=256\n",
        "num_layers=2\n",
        "lstm_classifier_7=LSTM_V3(input_size=embedding_size,hidden_size=hidden_size,num_layers=num_layers,num_classes=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0XCPRV4TJFW",
      "metadata": {
        "id": "a0XCPRV4TJFW"
      },
      "outputs": [],
      "source": [
        "optimizer = Adam(lstm_classifier_7.parameters(), lr=learning_rate,weight_decay=0.00001)\n",
        "loss_criterion=nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GQLMDWrFUZM8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQLMDWrFUZM8",
        "outputId": "e74c314f-ce80-4a49-c813-cc15dae18820"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LSTM_V3(\n",
              "  (lstm): LSTM(300, 256, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
              "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 114,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lstm_classifier_7.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "r4PQbVB-UPVR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4PQbVB-UPVR",
        "outputId": "b8d4205c-2cbb-4b02-8f37-f1165cd7207f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20], Loss: 0.9509117581855311\n",
            "Epoch [2/20], Loss: 0.6667493803788\n",
            "Epoch [3/20], Loss: 0.5966118009150817\n",
            "Epoch [4/20], Loss: 0.5521225274878806\n",
            "Epoch [5/20], Loss: 0.5203906426449652\n",
            "Epoch [6/20], Loss: 0.49191367407433934\n",
            "Epoch [7/20], Loss: 0.4632759995256681\n",
            "Epoch [8/20], Loss: 0.43855150174789204\n",
            "Epoch [9/20], Loss: 0.4100617792802634\n",
            "Epoch [10/20], Loss: 0.38300564493951816\n",
            "Epoch [11/20], Loss: 0.34818406005014135\n",
            "Epoch [12/20], Loss: 0.3194196551660731\n",
            "Epoch [13/20], Loss: 0.287973557836411\n",
            "Epoch [14/20], Loss: 0.2640744125589548\n",
            "Epoch [15/20], Loss: 0.2399335364796646\n",
            "Epoch [16/20], Loss: 0.21640545725297006\n",
            "Epoch [17/20], Loss: 0.19677877183210063\n",
            "Epoch [18/20], Loss: 0.18552087058051006\n",
            "Epoch [19/20], Loss: 0.16961714592223442\n",
            "Epoch [20/20], Loss: 0.1561716237426057\n"
          ]
        }
      ],
      "source": [
        "epochs=20\n",
        "\n",
        "# Initialize training and test accuracy\n",
        "train_acc = 0\n",
        "test_acc = 0\n",
        "\n",
        "# Loop through each epoch\n",
        "for epoch in range(epochs):\n",
        "    # Update progress bar description with current accuracy\n",
        "\n",
        "    # Set model to training mode\n",
        "    lstm_classifier_7.train()\n",
        "    running_loss=0.0\n",
        "    # Iterate through training data loader\n",
        "    for text,labels in train_loader:\n",
        "        # bs = labels.shape[0]\n",
        "\n",
        "        text=text.to(device)\n",
        "        labels = labels.long().to(device)\n",
        "\n",
        "        # print('text tensor:',text)\n",
        "        # print('text shape:',text.shape)\n",
        "        # print('label tensor:',label)\n",
        "\n",
        "        # Initialize hidden and memory states\n",
        "        # hidden = torch.zeros(num_layers, bs, hidden_size, device=device)\n",
        "        # memory = torch.zeros(num_layers, bs, hidden_size, device=device)\n",
        "\n",
        "        # Forward pass through the model\n",
        "        outputs = lstm_classifier_7(text)\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = loss_criterion(outputs, labels)\n",
        "        running_loss+=loss.item()\n",
        "\n",
        "        # Backpropagation and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    average_loss=running_loss/ len(train_loader)\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {average_loss}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DP2_EvpDUkiy",
      "metadata": {
        "id": "DP2_EvpDUkiy"
      },
      "source": [
        "#### evaluating our bi-lstm v3 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "taEWBsQiUkiz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taEWBsQiUkiz",
        "outputId": "df81c963-116e-48f1-a8bb-ae6365decf34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Classification Loss: 0.9052649723663198\n",
            "Classification Accuracy: 76.54353217595614%\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(lstm_classifier_7, test_loader, loss_criterion, device, writer=SummaryWriter('runs/model_6'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xkRz3k7Oc7QH",
      "metadata": {
        "id": "xkRz3k7Oc7QH"
      },
      "source": [
        "Based on the following results, it is noticed that even though we decreased the number of layers to 2 and increased the dropout to 0.5, our model is still learning well our data, but failing to generalize to our test data. Thus, we need to introduce some changes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5qkCucEkeVz_",
      "metadata": {
        "id": "5qkCucEkeVz_"
      },
      "source": [
        "### Trying a the RMSprop optimizer instead of Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7aKaBY40g_hJ",
      "metadata": {
        "id": "7aKaBY40g_hJ"
      },
      "outputs": [],
      "source": [
        "hidden_size=256\n",
        "num_layers=2\n",
        "lstm_classifier_8=LSTM_V3(input_size=embedding_size,hidden_size=hidden_size,num_layers=num_layers,num_classes=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1Apv0WO-hGSH",
      "metadata": {
        "id": "1Apv0WO-hGSH"
      },
      "outputs": [],
      "source": [
        "from torch.optim import RMSprop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XgASpbOCg_hJ",
      "metadata": {
        "id": "XgASpbOCg_hJ"
      },
      "outputs": [],
      "source": [
        "optimizer = RMSprop(lstm_classifier_8.parameters(), lr=0.01)\n",
        "loss_criterion=nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RdnssrDIg_hK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdnssrDIg_hK",
        "outputId": "70c0715b-2c1b-4f71-8077-c2c46de87f36"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LSTM_V3(\n",
              "  (lstm): LSTM(300, 256, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
              "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 139,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lstm_classifier_8.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "l3f-QqHHg_hK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3f-QqHHg_hK",
        "outputId": "18d1b278-dc29-4b49-dce6-138e28e22903"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20], Loss: 1.2255146356753117\n",
            "Epoch [2/20], Loss: 0.8042100380396253\n",
            "Epoch [3/20], Loss: 0.7447080633629308\n",
            "Epoch [4/20], Loss: 0.7040958929984955\n",
            "Epoch [5/20], Loss: 0.7012549106527075\n",
            "Epoch [6/20], Loss: 0.6913517235714468\n",
            "Epoch [7/20], Loss: 0.6912292467926725\n",
            "Epoch [8/20], Loss: 0.696408604127481\n",
            "Epoch [9/20], Loss: 0.6887913230993176\n",
            "Epoch [10/20], Loss: 0.6961764930889534\n",
            "Epoch [11/20], Loss: 0.6899187595304155\n",
            "Epoch [12/20], Loss: 0.6867549249545281\n",
            "Epoch [13/20], Loss: 0.6849136420567967\n",
            "Epoch [14/20], Loss: 0.6946567266956444\n",
            "Epoch [15/20], Loss: 0.6959051844043572\n",
            "Epoch [16/20], Loss: 0.6957776357747387\n",
            "Epoch [17/20], Loss: 0.6989444673884924\n",
            "Epoch [18/20], Loss: 0.7226040366743238\n",
            "Epoch [19/20], Loss: 0.7087922535223321\n",
            "Epoch [20/20], Loss: 0.7127663444858484\n"
          ]
        }
      ],
      "source": [
        "epochs=20\n",
        "\n",
        "# Initialize training and test accuracy\n",
        "train_acc = 0\n",
        "test_acc = 0\n",
        "\n",
        "# Loop through each epoch\n",
        "for epoch in range(epochs):\n",
        "    # Update progress bar description with current accuracy\n",
        "\n",
        "    # Set model to training mode\n",
        "    lstm_classifier_8.train()\n",
        "    running_loss=0.0\n",
        "    # Iterate through training data loader\n",
        "    for text,labels in train_loader:\n",
        "        # bs = labels.shape[0]\n",
        "\n",
        "        text=text.to(device)\n",
        "        labels = labels.long().to(device)\n",
        "\n",
        "        # print('text tensor:',text)\n",
        "        # print('text shape:',text.shape)\n",
        "        # print('label tensor:',label)\n",
        "\n",
        "        # Initialize hidden and memory states\n",
        "        # hidden = torch.zeros(num_layers, bs, hidden_size, device=device)\n",
        "        # memory = torch.zeros(num_layers, bs, hidden_size, device=device)\n",
        "\n",
        "        # Forward pass through the model\n",
        "        outputs = lstm_classifier_8(text)\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = loss_criterion(outputs, labels)\n",
        "        running_loss+=loss.item()\n",
        "\n",
        "        # Backpropagation and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    average_loss=running_loss/ len(train_loader)\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {average_loss}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wHzOeJ5og_hK",
      "metadata": {
        "id": "wHzOeJ5og_hK"
      },
      "source": [
        "#### evaluating our bi-lstm v4 model with the SGD optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S5_f72Kfg_hK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5_f72Kfg_hK",
        "outputId": "def9afd2-3438-4c1f-9482-8b84ca30e8a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Classification Loss: 0.7015067949551325\n",
            "Classification Accuracy: 70.53909411304006%\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(lstm_classifier_8, test_loader, loss_criterion, device, writer=SummaryWriter('runs/model_6'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ek1cwaFVtAAi",
      "metadata": {
        "id": "ek1cwaFVtAAi"
      },
      "source": [
        "Although the accuracy is not better than before with respect to the test data, what is notable is that the loss is comparable in both the training and test data unlike in other models. Based on our results, it is clear that the RMSprop that is adopted in rnn training approaches does not fit our case."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qOTp84HAtzMM",
      "metadata": {
        "id": "qOTp84HAtzMM"
      },
      "source": [
        "### Why Not Decrease the sequence length"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "__pEzcsGt4YP",
      "metadata": {
        "id": "__pEzcsGt4YP"
      },
      "source": [
        "Since the results are not getting better despite the proper learning the model is gaining and the several fine tunings, it would be logical to try and adjust our data format then monitor the results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SgUJljvguepF",
      "metadata": {
        "id": "SgUJljvguepF"
      },
      "source": [
        "#### re Handling our data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "4Fi1ulxQutFr",
      "metadata": {
        "id": "4Fi1ulxQutFr"
      },
      "outputs": [],
      "source": [
        "sequence_length=100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "ojJRZw_vulhr",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojJRZw_vulhr",
        "outputId": "6a4b990f-d975-4d70-a863-01c7160c8363"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(43412, 100, 300)\n",
            "(7661, 100, 300)\n"
          ]
        }
      ],
      "source": [
        "train_data_X_v2 = convert_sequences_to_tensor(X_train.to_numpy(), sequence_length, embedding_size)\n",
        "train_data_y_v2 = torch.FloatTensor([int(d) for d in y_train.to_numpy()])\n",
        "\n",
        "test_data_X_v2 = convert_sequences_to_tensor(X_test.to_numpy(), sequence_length, embedding_size)\n",
        "test_data_y_v2 = torch.FloatTensor([int(d) for d in y_test.to_numpy()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "LqSwaLVPulhs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqSwaLVPulhs",
        "outputId": "1637c136-7388-4ec6-bf8f-6867acfe8c77"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(43412, 43412, 7661, 7661)"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_data_X_v2),len(train_data_y_v2),len(test_data_X_v2),len(test_data_y_v2) # double checking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "fIkRgVdkxmz7",
      "metadata": {
        "id": "fIkRgVdkxmz7"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.optim import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "OiGlOIWYulhs",
      "metadata": {
        "id": "OiGlOIWYulhs"
      },
      "outputs": [],
      "source": [
        "train_data_v2 = TensorDataset(train_data_X_v2, train_data_y_v2)\n",
        "test_data_v2 = TensorDataset(test_data_X_v2, test_data_y_v2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "94ybEZqbulhs",
      "metadata": {
        "id": "94ybEZqbulhs"
      },
      "outputs": [],
      "source": [
        "batch_size=32\n",
        "train_loader = DataLoader(train_data_v2, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data_v2, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_PJgHMoXxL7v",
      "metadata": {
        "id": "_PJgHMoXxL7v"
      },
      "source": [
        "### Bi LSTM latest version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "w_gOSbORvwlW",
      "metadata": {
        "id": "w_gOSbORvwlW"
      },
      "outputs": [],
      "source": [
        "hidden_size=256\n",
        "num_layers=2\n",
        "lstm_classifier_9=LSTM_V3(input_size=embedding_size,hidden_size=hidden_size,num_layers=num_layers,num_classes=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "_ddV4UgwvwlW",
      "metadata": {
        "id": "_ddV4UgwvwlW"
      },
      "outputs": [],
      "source": [
        "optimizer = Adam(lstm_classifier_9.parameters(), lr=0.0005,weight_decay=0.00001)\n",
        "loss_criterion=nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "IQUkySfYyDJe",
      "metadata": {
        "id": "IQUkySfYyDJe"
      },
      "outputs": [],
      "source": [
        "device='cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "Eduo5e3NvwlW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eduo5e3NvwlW",
        "outputId": "33f989d9-8451-49dc-cf8b-1218204f895a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LSTM_V3(\n",
              "  (lstm): LSTM(300, 256, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
              "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lstm_classifier_9.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "gZsk3LSGvwlW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZsk3LSGvwlW",
        "outputId": "c9cb60d1-fbea-4d4d-fa74-789998428f11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20], Loss: 0.8011154153589475\n",
            "Epoch [2/20], Loss: 0.6061295878166826\n",
            "Epoch [3/20], Loss: 0.5494741767798615\n",
            "Epoch [4/20], Loss: 0.5137851131515813\n",
            "Epoch [5/20], Loss: 0.48253668231962354\n",
            "Epoch [6/20], Loss: 0.4510374052742429\n",
            "Epoch [7/20], Loss: 0.4182763477525205\n",
            "Epoch [8/20], Loss: 0.38299799443125815\n",
            "Epoch [9/20], Loss: 0.3506036444883533\n",
            "Epoch [10/20], Loss: 0.3189748882583027\n",
            "Epoch [11/20], Loss: 0.29196173685630983\n",
            "Epoch [12/20], Loss: 0.2663956230688859\n",
            "Epoch [13/20], Loss: 0.23877682189567614\n",
            "Epoch [14/20], Loss: 0.2170374895114515\n",
            "Epoch [15/20], Loss: 0.19661009789210473\n",
            "Epoch [16/20], Loss: 0.1772907469558887\n",
            "Epoch [17/20], Loss: 0.1610795420367345\n",
            "Epoch [18/20], Loss: 0.1499093102768898\n",
            "Epoch [19/20], Loss: 0.1366480799109106\n",
            "Epoch [20/20], Loss: 0.12662620544529676\n"
          ]
        }
      ],
      "source": [
        "epochs=20\n",
        "\n",
        "# Initialize training and test accuracy\n",
        "train_acc = 0\n",
        "test_acc = 0\n",
        "\n",
        "# Loop through each epoch\n",
        "for epoch in range(epochs):\n",
        "    # Update progress bar description with current accuracy\n",
        "\n",
        "    # Set model to training mode\n",
        "    lstm_classifier_9.train()\n",
        "    running_loss=0.0\n",
        "    # Iterate through training data loader\n",
        "    for text,labels in train_loader:\n",
        "        # bs = labels.shape[0]\n",
        "\n",
        "        text=text.to(device)\n",
        "        labels = labels.long().to(device)\n",
        "\n",
        "        # print('text tensor:',text)\n",
        "        # print('text shape:',text.shape)\n",
        "        # print('label tensor:',label)\n",
        "\n",
        "        # Initialize hidden and memory states\n",
        "        # hidden = torch.zeros(num_layers, bs, hidden_size, device=device)\n",
        "        # memory = torch.zeros(num_layers, bs, hidden_size, device=device)\n",
        "\n",
        "        # Forward pass through the model\n",
        "        outputs = lstm_classifier_9(text)\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = loss_criterion(outputs, labels)\n",
        "        running_loss+=loss.item()\n",
        "\n",
        "        # Backpropagation and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    average_loss=running_loss/ len(train_loader)\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {average_loss}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tz_dHZVB0ByK",
      "metadata": {
        "id": "tz_dHZVB0ByK"
      },
      "source": [
        "It is worth noting that the training loss at the 20th epoch is less than that of the corresponfing previous models."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04FJUzp7vwlW",
      "metadata": {
        "id": "04FJUzp7vwlW"
      },
      "source": [
        "#### evaluating our bi-lstm v5 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "dh2Uc2bivwlX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dh2Uc2bivwlX",
        "outputId": "1b9d6161-fe88-45e0-9a9c-d77e444dd55e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Classification Loss: 0.900097847605745\n",
            "Classification Accuracy: 76.28246965148153%\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(lstm_classifier_9, test_loader, loss_criterion, device, writer=SummaryWriter('runs/model_8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yfDqnJ0fMECO",
      "metadata": {
        "id": "yfDqnJ0fMECO"
      },
      "source": [
        "### Trying out our best performining architecture on different sequence lengths"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ODLetVaUPeWq",
      "metadata": {
        "id": "ODLetVaUPeWq"
      },
      "source": [
        "For curiousity, we will tryout our best architecture while setting out the sequence length to 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "ZLmkzIIrL7BB",
      "metadata": {
        "id": "ZLmkzIIrL7BB"
      },
      "outputs": [],
      "source": [
        "hidden_size=256\n",
        "num_layers=3\n",
        "lstm_classifier_11=LSTM_V2(input_size=embedding_size,hidden_size=hidden_size,num_layers=num_layers,num_classes=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "9JCmQ8SxL7BB",
      "metadata": {
        "id": "9JCmQ8SxL7BB"
      },
      "outputs": [],
      "source": [
        "optimizer = Adam(lstm_classifier_11.parameters(), lr=0.0005)\n",
        "loss_criterion=nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "cxn8FMSUL7BB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxn8FMSUL7BB",
        "outputId": "cfcc5fb7-ce59-469f-cb9f-52fc713c231d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LSTM_V2(\n",
              "  (lstm): LSTM(300, 256, num_layers=3, batch_first=True, dropout=0.3, bidirectional=True)\n",
              "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lstm_classifier_11.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "bXQlUaQLL7BB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXQlUaQLL7BB",
        "outputId": "3a67e18f-b54c-4f61-a826-213cdb913e7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 0.8609684751195422\n",
            "Epoch [2/15], Loss: 0.6660252780095567\n",
            "Epoch [3/15], Loss: 0.6081540994812833\n",
            "Epoch [4/15], Loss: 0.5621677429779716\n",
            "Epoch [5/15], Loss: 0.5215696855753594\n",
            "Epoch [6/15], Loss: 0.48465966439326197\n",
            "Epoch [7/15], Loss: 0.44886574248509453\n",
            "Epoch [8/15], Loss: 0.4090283367077305\n",
            "Epoch [9/15], Loss: 0.3678597759073854\n",
            "Epoch [10/15], Loss: 0.3273166773089962\n",
            "Epoch [11/15], Loss: 0.2802795764078045\n",
            "Epoch [12/15], Loss: 0.23973052365349862\n",
            "Epoch [13/15], Loss: 0.2020409328042198\n",
            "Epoch [14/15], Loss: 0.1756333556731747\n",
            "Epoch [15/15], Loss: 0.148973096087273\n"
          ]
        }
      ],
      "source": [
        "epochs=15\n",
        "\n",
        "# Initialize training and test accuracy\n",
        "train_acc = 0\n",
        "test_acc = 0\n",
        "\n",
        "# Loop through each epoch\n",
        "for epoch in range(epochs):\n",
        "    # Update progress bar description with current accuracy\n",
        "\n",
        "    # Set model to training mode\n",
        "    lstm_classifier_11.train()\n",
        "    running_loss=0.0\n",
        "    # Iterate through training data loader\n",
        "    for text,labels in train_loader:\n",
        "        # bs = labels.shape[0]\n",
        "\n",
        "        text=text.to(device)\n",
        "        labels = labels.long().to(device)\n",
        "\n",
        "        # print('text tensor:',text)\n",
        "        # print('text shape:',text.shape)\n",
        "        # print('label tensor:',label)\n",
        "\n",
        "        # Initialize hidden and memory states\n",
        "        # hidden = torch.zeros(num_layers, bs, hidden_size, device=device)\n",
        "        # memory = torch.zeros(num_layers, bs, hidden_size, device=device)\n",
        "\n",
        "        # Forward pass through the model\n",
        "        outputs = lstm_classifier_11(text)\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = loss_criterion(outputs, labels)\n",
        "        running_loss+=loss.item()\n",
        "\n",
        "        # Backpropagation and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    average_loss=running_loss/ len(train_loader)\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {average_loss}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qBNi_GS5L7BC",
      "metadata": {
        "id": "qBNi_GS5L7BC"
      },
      "source": [
        "#### evaluating our finetuned model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "nBjl342aL7BC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBjl342aL7BC",
        "outputId": "a7e61d88-3c98-40cd-c1bf-0a4ab2baa147"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Classification Loss: 0.8944278956080477\n",
            "Classification Accuracy: 76.51742592350868%\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(lstm_classifier_11, test_loader, loss_criterion, device, writer=SummaryWriter('runs/model_10'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3qlSwA8zL7BC",
      "metadata": {
        "id": "3qlSwA8zL7BC"
      },
      "source": [
        "Based on the results, we notice that setting the sequence length to 125 is better than to 100"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6Fvj923L01uj",
      "metadata": {
        "id": "6Fvj923L01uj"
      },
      "source": [
        "### Decreasing more the sequence length !"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VKtKSVUF54Hd",
      "metadata": {
        "id": "VKtKSVUF54Hd"
      },
      "source": [
        "Note that we will set the batch size to 32 and sequence length to 75, this length works for 77 % of our data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "D5L8np32w9p8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "D5L8np32w9p8",
        "outputId": "1574c416-ca19-42f8-8ae1-ce233fa8025b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lemmatized_text</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>True</th>\n",
              "      <td>39528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>False</th>\n",
              "      <td>11545</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ],
            "text/plain": [
              "lemmatized_text\n",
              "True     39528\n",
              "False    11545\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "((df['lemmatized_text'].apply(len)< 75).value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "XFSs10nj0sI1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFSs10nj0sI1",
        "outputId": "04634cb7-4a30-4a29-bbd1-3ddb871d5eb0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7739510112975545"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "39528 / len(df['lemmatized_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "eINSq86H29FS",
      "metadata": {
        "id": "eINSq86H29FS"
      },
      "outputs": [],
      "source": [
        "sequence_length=75"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "wC6NboFT29FT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wC6NboFT29FT",
        "outputId": "e659bc9b-510c-4418-ea4f-abaaa1e05b5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(43412, 75, 300)\n",
            "(7661, 75, 300)\n"
          ]
        }
      ],
      "source": [
        "train_data_X_v3 = convert_sequences_to_tensor(X_train.to_numpy(), sequence_length, embedding_size)\n",
        "train_data_y_v3 = torch.FloatTensor([int(d) for d in y_train.to_numpy()])\n",
        "\n",
        "test_data_X_v3 = convert_sequences_to_tensor(X_test.to_numpy(), sequence_length, embedding_size)\n",
        "test_data_y_v3 = torch.FloatTensor([int(d) for d in y_test.to_numpy()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "NrYo65BS29FT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrYo65BS29FT",
        "outputId": "3026318f-b093-4767-b124-03ec13fb65f6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(43412, 43412, 7661, 7661)"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_data_X_v3),len(train_data_y_v3),len(test_data_X_v3),len(test_data_y_v3) # double checking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "LGqmJq-V29FT",
      "metadata": {
        "id": "LGqmJq-V29FT"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.optim import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "MpwYiSCS29FT",
      "metadata": {
        "id": "MpwYiSCS29FT"
      },
      "outputs": [],
      "source": [
        "train_data_v3 = TensorDataset(train_data_X_v3, train_data_y_v3)\n",
        "test_data_v3 = TensorDataset(test_data_X_v3, test_data_y_v3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "2HgvF7D029FT",
      "metadata": {
        "id": "2HgvF7D029FT"
      },
      "outputs": [],
      "source": [
        "batch_size=32\n",
        "train_loader = DataLoader(train_data_v3, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data_v3, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zvoZIyrH3N7k",
      "metadata": {
        "id": "zvoZIyrH3N7k"
      },
      "source": [
        "### Training our v6 Bi-LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "eAk0Fj3O3jxx",
      "metadata": {
        "id": "eAk0Fj3O3jxx"
      },
      "outputs": [],
      "source": [
        "hidden_size=256\n",
        "num_layers=2\n",
        "lstm_classifier_10=LSTM_V3(input_size=embedding_size,hidden_size=hidden_size,num_layers=num_layers,num_classes=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "WEYYsheV3jxy",
      "metadata": {
        "id": "WEYYsheV3jxy"
      },
      "outputs": [],
      "source": [
        "optimizer = Adam(lstm_classifier_10.parameters(), lr=0.0005,weight_decay=0.00001)\n",
        "loss_criterion=nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "vtP8bKMh3jxy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtP8bKMh3jxy",
        "outputId": "bb9822cf-411a-413f-a35a-0f34b17e70f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LSTM_V3(\n",
              "  (lstm): LSTM(300, 256, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
              "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lstm_classifier_10.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "VHpFu6AN3jxy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHpFu6AN3jxy",
        "outputId": "270fda3c-0857-4673-ac3b-77b6333ac002"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20], Loss: 0.8207078656248952\n",
            "Epoch [2/20], Loss: 0.6455993304357164\n",
            "Epoch [3/20], Loss: 0.5985809248997696\n",
            "Epoch [4/20], Loss: 0.5588504993792571\n",
            "Epoch [5/20], Loss: 0.5279641489751565\n",
            "Epoch [6/20], Loss: 0.49932595023400833\n",
            "Epoch [7/20], Loss: 0.475965042652764\n",
            "Epoch [8/20], Loss: 0.44521858257103597\n",
            "Epoch [9/20], Loss: 0.41664814659138333\n",
            "Epoch [10/20], Loss: 0.3832990528086397\n",
            "Epoch [11/20], Loss: 0.34995286050269775\n",
            "Epoch [12/20], Loss: 0.32082729799537074\n",
            "Epoch [13/20], Loss: 0.2832866636192043\n",
            "Epoch [14/20], Loss: 0.25307833374249383\n",
            "Epoch [15/20], Loss: 0.22075535278072803\n",
            "Epoch [16/20], Loss: 0.19960212567512306\n",
            "Epoch [17/20], Loss: 0.1726881849484116\n",
            "Epoch [18/20], Loss: 0.15346458771416346\n",
            "Epoch [19/20], Loss: 0.13725342310988395\n",
            "Epoch [20/20], Loss: 0.12479474925928163\n"
          ]
        }
      ],
      "source": [
        "epochs=20\n",
        "\n",
        "# Initialize training and test accuracy\n",
        "train_acc = 0\n",
        "test_acc = 0\n",
        "\n",
        "# Loop through each epoch\n",
        "for epoch in range(epochs):\n",
        "    # Update progress bar description with current accuracy\n",
        "\n",
        "    # Set model to training mode\n",
        "    lstm_classifier_10.train()\n",
        "    running_loss=0.0\n",
        "    # Iterate through training data loader\n",
        "    for text,labels in train_loader:\n",
        "        # bs = labels.shape[0]\n",
        "\n",
        "        text=text.to(device)\n",
        "        labels = labels.long().to(device)\n",
        "\n",
        "        # print('text tensor:',text)\n",
        "        # print('text shape:',text.shape)\n",
        "        # print('label tensor:',label)\n",
        "\n",
        "        # Initialize hidden and memory states\n",
        "        # hidden = torch.zeros(num_layers, bs, hidden_size, device=device)\n",
        "        # memory = torch.zeros(num_layers, bs, hidden_size, device=device)\n",
        "\n",
        "        # Forward pass through the model\n",
        "        outputs = lstm_classifier_10(text)\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = loss_criterion(outputs, labels)\n",
        "        running_loss+=loss.item()\n",
        "\n",
        "        # Backpropagation and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    average_loss=running_loss/ len(train_loader)\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {average_loss}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FMuSRUUZ3jxy",
      "metadata": {
        "id": "FMuSRUUZ3jxy"
      },
      "source": [
        "#### evaluating our bi-lstm v6 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "WHGTmC003jxy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHGTmC003jxy",
        "outputId": "1aa35472-0092-4b38-dc07-a0543019ad19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Classification Loss: 1.025893891726931\n",
            "Classification Accuracy: 75.18600704868817%\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(lstm_classifier_10, test_loader, loss_criterion, device, writer=SummaryWriter('runs/model_9'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd795498",
      "metadata": {},
      "source": [
        "There is no improvement is classification accuaracy, thus because lstm is capable of handling large sequences properly unlike rnns and grus given its complex architecture. It might be actually worth it to try increasing the sequence length!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IDJ6uXcTJF8L",
      "metadata": {
        "id": "IDJ6uXcTJF8L"
      },
      "source": [
        "### Trying with our best previous model settings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KTYp5BcVJmDv",
      "metadata": {
        "id": "KTYp5BcVJmDv"
      },
      "source": [
        "The following model architectur got the highest accuaracy among all the previous models of approx 78%, so we will try it while adjusting the sequence length to 75"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "MYsYK4uXJMRb",
      "metadata": {
        "id": "MYsYK4uXJMRb"
      },
      "outputs": [],
      "source": [
        "hidden_size=256\n",
        "lstm_classifier_12=LSTM_V2(input_size=embedding_size,hidden_size=hidden_size,num_layers=num_layers,num_classes=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "7QJCUNX-JMRb",
      "metadata": {
        "id": "7QJCUNX-JMRb"
      },
      "outputs": [],
      "source": [
        "optimizer = Adam(lstm_classifier_12.parameters(), lr=0.0005)\n",
        "loss_criterion=nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "gJZ9-uSaJMRb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJZ9-uSaJMRb",
        "outputId": "1e40f2c3-0464-44ef-a86b-96d8e0f8bac4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LSTM_V2(\n",
              "  (lstm): LSTM(300, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
              "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lstm_classifier_12.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "DbwEg8hIJMRc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbwEg8hIJMRc",
        "outputId": "2ca16e61-5688-46d6-b2ff-d693f7116367"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 0.8370928430741731\n",
            "Epoch [2/15], Loss: 0.6698631128473324\n",
            "Epoch [3/15], Loss: 0.6099777989468332\n",
            "Epoch [4/15], Loss: 0.5627397903534271\n",
            "Epoch [5/15], Loss: 0.5210001140876824\n",
            "Epoch [6/15], Loss: 0.48688282767638796\n",
            "Epoch [7/15], Loss: 0.4484723518021245\n",
            "Epoch [8/15], Loss: 0.4076051027592966\n",
            "Epoch [9/15], Loss: 0.3666841540054618\n",
            "Epoch [10/15], Loss: 0.3203203261788048\n",
            "Epoch [11/15], Loss: 0.27502360695653216\n",
            "Epoch [12/15], Loss: 0.23490454121468782\n",
            "Epoch [13/15], Loss: 0.20308210896760467\n",
            "Epoch [14/15], Loss: 0.16797868764081486\n",
            "Epoch [15/15], Loss: 0.1469889541335717\n"
          ]
        }
      ],
      "source": [
        "epochs=15\n",
        "\n",
        "# Initialize training and test accuracy\n",
        "train_acc = 0\n",
        "test_acc = 0\n",
        "\n",
        "# Loop through each epoch\n",
        "for epoch in range(epochs):\n",
        "    # Update progress bar description with current accuracy\n",
        "\n",
        "    # Set model to training mode\n",
        "    lstm_classifier_12.train()\n",
        "    running_loss=0.0\n",
        "    # Iterate through training data loader\n",
        "    for text,labels in train_loader:\n",
        "        # bs = labels.shape[0]\n",
        "\n",
        "        text=text.to(device)\n",
        "        labels = labels.long().to(device)\n",
        "\n",
        "        # print('text tensor:',text)\n",
        "        # print('text shape:',text.shape)\n",
        "        # print('label tensor:',label)\n",
        "\n",
        "        # Initialize hidden and memory states\n",
        "        # hidden = torch.zeros(num_layers, bs, hidden_size, device=device)\n",
        "        # memory = torch.zeros(num_layers, bs, hidden_size, device=device)\n",
        "\n",
        "        # Forward pass through the model\n",
        "        outputs = lstm_classifier_12(text)\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = loss_criterion(outputs, labels)\n",
        "        running_loss+=loss.item()\n",
        "\n",
        "        # Backpropagation and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    average_loss=running_loss/ len(train_loader)\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {average_loss}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wuspfnonJMRc",
      "metadata": {
        "id": "wuspfnonJMRc"
      },
      "source": [
        "#### evaluating our finetuned model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "CvDzI41jJMRc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvDzI41jJMRc",
        "outputId": "3d087b5e-eb4d-47df-e6c1-6b7648dcd389"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Classification Loss: 0.9499286583935221\n",
            "Classification Accuracy: 75.68202584518993%\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(lstm_classifier_12, test_loader, loss_criterion, device, writer=SummaryWriter('runs/model_11'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "B5zhpoYUK3mt",
      "metadata": {
        "id": "B5zhpoYUK3mt"
      },
      "source": [
        "Based on the results, it seemed that it is not optimal to set the sequence length to 75"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99945115",
      "metadata": {},
      "source": [
        "# RNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6db70168",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b06a00ff",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, embedding_size, hidden_size, num_layers, num_classes):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn = nn.RNN(\n",
        "            input_size=embedding_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True      #(batch_size,sequence length,embedding size)\n",
        "        )\n",
        "\n",
        "        #self.dropout = nn.Dropout(0.5)  #trying dropout but did not make any improvement\n",
        "        self.fc = nn.Linear(hidden_size , num_classes)  #takes the final hidden state and maps it 4 output values\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers , x.size(0), self.hidden_size).to(device)  # we initialize the hidden state to zeros\n",
        "        # (num layers, batch size,hidden size)\n",
        "        # x shape:  (batch size, sequence length, embedding size)\n",
        "        \n",
        "        out, _ = self.rnn(x, h0)\n",
        "        # out has shape: (batch size,sequence length,hidden size)\n",
        "        out = out[:, -1, :]   #(batch size,hidden size)\n",
        "\n",
        "    \n",
        "        #out = self.dropout(out) # did not improve\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "    \n",
        "    # the rnn model performed better without dropout, and biderectional did not enhance it\n",
        "    # adjusting the learning rate to 0.0001, hidden size=128,sequence length=70,  gave the better result for rnn\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a4c6ac5",
      "metadata": {},
      "source": [
        "### Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a9fbd91",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "num_layers=2\n",
        "hidden_size=128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dc4ef85",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = RNN(embedding_size, hidden_size, num_layers, num_classes=4).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7dc93db",
      "metadata": {},
      "outputs": [],
      "source": [
        "def training_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in dataloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        \n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    average = running_loss / len(dataloader)\n",
        "    accuracy = 100 * correct / total\n",
        "    return average, accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ccf8f34",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    average = test_loss / len(dataloader)\n",
        "    accuracy = 100 * correct / total\n",
        "    return average, accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a0072b6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10] Train Loss: 1.0145, Train Acc: 54.93% | Test Loss: 0.8455, Test Acc: 64.72%\n",
            "Epoch [2/10] Train Loss: 0.8239, Train Acc: 64.02% | Test Loss: 0.7835, Test Acc: 67.98%\n",
            "Epoch [3/10] Train Loss: 0.7977, Train Acc: 65.47% | Test Loss: 0.8089, Test Acc: 65.36%\n",
            "Epoch [4/10] Train Loss: 0.7820, Train Acc: 66.12% | Test Loss: 0.8164, Test Acc: 67.68%\n",
            "Epoch [5/10] Train Loss: 0.7751, Train Acc: 66.07% | Test Loss: 0.7678, Test Acc: 67.71%\n",
            "Epoch [6/10] Train Loss: 0.7698, Train Acc: 66.50% | Test Loss: 0.7551, Test Acc: 67.89%\n",
            "Epoch [7/10] Train Loss: 0.7665, Train Acc: 66.99% | Test Loss: 0.7497, Test Acc: 68.71%\n",
            "Epoch [8/10] Train Loss: 0.7555, Train Acc: 67.29% | Test Loss: 0.7472, Test Acc: 68.61%\n",
            "Epoch [9/10] Train Loss: 0.7637, Train Acc: 67.07% | Test Loss: 0.7697, Test Acc: 67.28%\n",
            "Epoch [10/10] Train Loss: 0.7590, Train Acc: 67.32% | Test Loss: 0.7637, Test Acc: 68.06%\n"
          ]
        }
      ],
      "source": [
        "num_epochs =10\n",
        "train_losses, train_accuracies = [], []\n",
        "test_losses, test_accuracies = [], []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = training_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracies.append(test_acc)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
        "          f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n",
        "\n",
        "\n",
        "# For rnn this is an expected result because in our dataset we have 4 classes (normal,depression,suicidal,other)\n",
        "# the 'other' label contains other disorders that might have meanings similar to normal,depression,suicidal\n",
        "# also it is hard to classify which mental disorder because you might find word in classifes normal and depression and rnn is bad with memory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fabbb43",
      "metadata": {},
      "source": [
        "# GRU Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7faef173",
      "metadata": {},
      "source": [
        "## Training the model GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7933d1e",
      "metadata": {},
      "outputs": [],
      "source": [
        "num_classes=4\n",
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self, embedding_size, hidden_size, num_layers, num_classes):\n",
        "        super(RNNClassifier, self).__init__()\n",
        "        \n",
        "        self.bidirectional = True  # enable bidirectionality\n",
        "        self.rnn = nn.GRU(\n",
        "            input_size=embedding_size,   # size of each word vector\n",
        "            hidden_size=hidden_size,     # size of hidden layer\n",
        "            num_layers=num_layers,       # how many GRU layers\n",
        "            batch_first=True,            # input shape = (batch, sequence, features)\n",
        "            bidirectional=self.bidirectional  # read both directions\n",
        "        )\n",
        "\n",
        "        # Output size becomes hidden_size * 2 if bidirectional\n",
        "        self.fc = nn.Linear(hidden_size * 2 if self.bidirectional else hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.rnn(x)  # shape: (batch, seq_len, hidden*2 if bidirectional)\n",
        "        out = out[:, -1, :]   # get the last time step output\n",
        "        out = self.fc(out)    # final prediction layer\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43337258",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training new model\n",
            "Epoch [1/10], Step [10], Loss: 1.3643\n",
            "Epoch [1/10], Step [20], Loss: 1.2867\n",
            "Epoch [1/10], Step [30], Loss: 1.3048\n",
            "Epoch [1/10], Step [40], Loss: 1.1600\n",
            "Epoch [1/10], Step [50], Loss: 1.3497\n",
            "Epoch [1/10], Step [60], Loss: 1.2505\n",
            "Epoch [1/10], Step [70], Loss: 1.2591\n",
            "Epoch [1/10], Step [80], Loss: 1.2565\n",
            "Epoch [1/10], Step [90], Loss: 2.3818\n",
            "Epoch [1/10], Step [100], Loss: 1.1836\n",
            "Epoch [1/10], Step [110], Loss: 1.2924\n",
            "Epoch [1/10], Step [120], Loss: 1.3345\n",
            "Epoch [1/10], Step [130], Loss: 1.3397\n",
            "Epoch [1/10], Step [140], Loss: 1.3982\n",
            "Epoch [1/10], Step [150], Loss: 1.1700\n",
            "Epoch [1/10], Step [160], Loss: 1.3374\n",
            "Epoch [1/10], Step [170], Loss: 1.2750\n",
            "Epoch [1/10], Step [180], Loss: 1.1672\n",
            "Epoch [1/10], Step [190], Loss: 1.0863\n",
            "Epoch [1/10], Step [200], Loss: 1.0322\n",
            "Epoch [1/10], Step [210], Loss: 1.2142\n",
            "Epoch [1/10], Step [220], Loss: 1.0548\n",
            "Epoch [1/10], Step [230], Loss: 0.9242\n",
            "Epoch [1/10], Step [240], Loss: 0.9191\n",
            "Epoch [1/10], Step [250], Loss: 0.6904\n",
            "Epoch [1/10], Step [260], Loss: 0.8462\n",
            "Epoch [1/10], Step [270], Loss: 0.8640\n",
            "Epoch [1/10], Step [280], Loss: 0.7571\n",
            "Epoch [1/10], Step [290], Loss: 0.6222\n",
            "Epoch [1/10], Step [300], Loss: 0.8411\n",
            "Epoch [1/10], Step [310], Loss: 0.7643\n",
            "Epoch [1/10], Step [320], Loss: 0.9236\n",
            "Epoch [1/10], Step [330], Loss: 0.7844\n",
            "Epoch [1/10], Step [340], Loss: 0.5995\n",
            "Epoch [1/10], Step [350], Loss: 0.6468\n",
            "Epoch [1/10], Step [360], Loss: 1.2075\n",
            "Epoch [1/10], Step [370], Loss: 0.4970\n",
            "Epoch [1/10], Step [380], Loss: 0.9706\n",
            "Epoch [1/10], Step [390], Loss: 0.6223\n",
            "Epoch [1/10], Step [400], Loss: 0.6261\n",
            "Epoch [1/10], Step [410], Loss: 0.7427\n",
            "Epoch [1/10], Step [420], Loss: 0.5372\n",
            "Epoch [1/10], Step [430], Loss: 0.5099\n",
            "Epoch [1/10], Step [440], Loss: 0.6013\n",
            "Epoch [1/10], Step [450], Loss: 0.8129\n",
            "Epoch [1/10], Step [460], Loss: 0.6456\n",
            "Epoch [1/10], Step [470], Loss: 0.8470\n",
            "Epoch [1/10], Step [480], Loss: 0.8586\n",
            "Epoch [1/10], Step [490], Loss: 0.7112\n",
            "Epoch [1/10], Step [500], Loss: 0.7152\n",
            "Epoch [1/10], Step [510], Loss: 0.5680\n",
            "Epoch [1/10], Step [520], Loss: 0.4291\n",
            "Epoch [1/10], Step [530], Loss: 0.7258\n",
            "Epoch [1/10], Step [540], Loss: 0.6574\n",
            "Epoch [1/10], Step [550], Loss: 0.7800\n",
            "Epoch [1/10], Step [560], Loss: 0.6294\n",
            "Epoch [1/10], Step [570], Loss: 0.8184\n",
            "Epoch [1/10], Step [580], Loss: 0.8453\n",
            "Epoch [1/10], Step [590], Loss: 0.5897\n",
            "Epoch [1/10], Step [600], Loss: 0.5257\n",
            "Epoch [1/10], Step [610], Loss: 0.6327\n",
            "Epoch [1/10], Step [620], Loss: 0.3939\n",
            "Epoch [1/10], Step [630], Loss: 0.7333\n",
            "Epoch [1/10], Step [640], Loss: 0.6824\n",
            "Epoch [1/10], Step [650], Loss: 0.5487\n",
            "Epoch [1/10], Step [660], Loss: 0.6140\n",
            "Epoch [1/10], Step [670], Loss: 0.5524\n",
            "Epoch [1/10], Step [680], Loss: 0.4070\n",
            "Epoch [1/10], Step [690], Loss: 0.4036\n",
            "Epoch [1/10], Step [700], Loss: 0.3790\n",
            "Epoch [1/10], Step [710], Loss: 0.8086\n",
            "Epoch [1/10], Step [720], Loss: 0.6880\n",
            "Epoch [1/10], Step [730], Loss: 0.6011\n",
            "Epoch [1/10], Step [740], Loss: 0.4458\n",
            "Epoch [1/10], Step [750], Loss: 0.6346\n",
            "Epoch [1/10], Step [760], Loss: 0.2832\n",
            "Epoch [1/10], Step [770], Loss: 0.7268\n",
            "Epoch [1/10], Step [780], Loss: 0.7796\n",
            "Epoch [1/10], Step [790], Loss: 0.6107\n",
            "Epoch [1/10], Step [800], Loss: 0.7162\n",
            "Epoch [1/10], Step [810], Loss: 0.6682\n",
            "Epoch [1/10], Step [820], Loss: 0.5830\n",
            "Epoch [1/10], Step [830], Loss: 0.6402\n",
            "Epoch [1/10], Step [840], Loss: 0.4549\n",
            "Epoch [1/10], Step [850], Loss: 0.5099\n",
            "Epoch [1/10], Step [860], Loss: 0.4644\n",
            "Epoch [1/10], Step [870], Loss: 0.8070\n",
            "Epoch [1/10], Step [880], Loss: 0.6169\n",
            "Epoch [1/10], Step [890], Loss: 0.5687\n",
            "Epoch [1/10], Step [900], Loss: 0.4452\n",
            "Epoch [1/10], Step [910], Loss: 0.4488\n",
            "Epoch [1/10], Step [920], Loss: 0.4838\n",
            "Epoch [1/10], Step [930], Loss: 0.6909\n",
            "Epoch [1/10], Step [940], Loss: 0.5810\n",
            "Epoch [1/10], Step [950], Loss: 0.8367\n",
            "Epoch [1/10], Step [960], Loss: 0.4924\n",
            "Epoch [1/10], Step [970], Loss: 0.8012\n",
            "Epoch [1/10], Step [980], Loss: 0.5209\n",
            "Epoch [1/10], Step [990], Loss: 0.8141\n",
            "Epoch [1/10], Step [1000], Loss: 0.7236\n",
            "Epoch [1/10], Step [1010], Loss: 0.7462\n",
            "Epoch [1/10], Step [1020], Loss: 0.8345\n",
            "Epoch [1/10], Step [1030], Loss: 0.7073\n",
            "Epoch [1/10], Step [1040], Loss: 0.5880\n",
            "Epoch [1/10], Step [1050], Loss: 0.4671\n",
            "Epoch [1/10], Step [1060], Loss: 0.6626\n",
            "Epoch [1/10], Step [1070], Loss: 0.5299\n",
            "Epoch [1/10], Step [1080], Loss: 0.6737\n",
            "Epoch [1/10], Step [1090], Loss: 1.0326\n",
            "Epoch [1/10], Step [1100], Loss: 0.5523\n",
            "Epoch [1/10], Step [1110], Loss: 0.7634\n",
            "Epoch [1/10], Step [1120], Loss: 0.8020\n",
            "Epoch [1/10], Step [1130], Loss: 0.5477\n",
            "Epoch [1/10], Step [1140], Loss: 0.6714\n",
            "Epoch [1/10], Step [1150], Loss: 0.5877\n",
            "Epoch [1/10], Step [1160], Loss: 0.6059\n",
            "Epoch [1/10], Step [1170], Loss: 0.7003\n",
            "Epoch [1/10], Step [1180], Loss: 0.6345\n",
            "Epoch [1/10], Step [1190], Loss: 0.5369\n",
            "Epoch [1/10], Step [1200], Loss: 0.5472\n",
            "Epoch [1/10], Step [1210], Loss: 0.4405\n",
            "Epoch [1/10], Step [1220], Loss: 0.5892\n",
            "Epoch [1/10], Step [1230], Loss: 0.7666\n",
            "Epoch [1/10], Step [1240], Loss: 0.5211\n",
            "Epoch [1/10], Step [1250], Loss: 0.7707\n",
            "Epoch [1/10], Step [1260], Loss: 0.6728\n",
            "Epoch [1/10], Step [1270], Loss: 0.8221\n",
            "Epoch [1/10], Step [1280], Loss: 0.5114\n",
            "Epoch [1/10], Step [1290], Loss: 0.6891\n",
            "Epoch [1/10], Step [1300], Loss: 0.8660\n",
            "Epoch [1/10], Step [1310], Loss: 0.5021\n",
            "Epoch [1/10], Step [1320], Loss: 0.3915\n",
            "Epoch [1/10], Step [1330], Loss: 0.3566\n",
            "Epoch [1/10], Step [1340], Loss: 0.5700\n",
            "Epoch [1/10], Step [1350], Loss: 0.5435\n",
            "Epoch [1/10], Step [1360], Loss: 0.5034\n",
            "Epoch [1/10], Step [1370], Loss: 0.5539\n",
            "Epoch [1/10], Step [1380], Loss: 0.8118\n",
            "Epoch [1/10], Step [1390], Loss: 0.6664\n",
            "Epoch [1/10], Step [1400], Loss: 0.5053\n",
            "Epoch [1/10], Step [1410], Loss: 0.4468\n",
            "Epoch [1/10], Step [1420], Loss: 0.6648\n",
            "Epoch [1/10], Step [1430], Loss: 0.6579\n",
            "Epoch [1/10], Step [1440], Loss: 0.5847\n",
            "Epoch [1/10], Step [1450], Loss: 0.4818\n",
            "Epoch [1/10], Step [1460], Loss: 0.4164\n",
            "Epoch [1/10], Step [1470], Loss: 0.7173\n",
            "Epoch [1/10], Step [1480], Loss: 0.4103\n",
            "Epoch [1/10], Step [1490], Loss: 0.5506\n",
            "Epoch [1/10], Step [1500], Loss: 0.6146\n",
            "Epoch [1/10], Step [1510], Loss: 0.6311\n",
            "Epoch [1/10], Step [1520], Loss: 0.5566\n",
            "Epoch [1/10], Step [1530], Loss: 0.7120\n",
            "Epoch [1/10], Step [1540], Loss: 0.6597\n",
            "Epoch [1/10], Step [1550], Loss: 0.7026\n",
            "Epoch [1/10], Step [1560], Loss: 0.5344\n",
            "Epoch [1/10], Step [1570], Loss: 0.5067\n",
            "Epoch [1/10], Step [1580], Loss: 0.8090\n",
            "Epoch [1/10], Step [1590], Loss: 1.0807\n",
            "Epoch [1/10], Step [1600], Loss: 0.5153\n",
            "Epoch [1/10], Step [1610], Loss: 0.4033\n",
            "Epoch [1/10], Step [1620], Loss: 0.8130\n",
            "Epoch [1/10], Step [1630], Loss: 0.8730\n",
            "Epoch [1/10], Step [1640], Loss: 0.4263\n",
            "Epoch [1/10], Step [1650], Loss: 0.5529\n",
            "Epoch [1/10], Step [1660], Loss: 0.9255\n",
            "Epoch [1/10], Step [1670], Loss: 0.7862\n",
            "Epoch [1/10], Step [1680], Loss: 0.4082\n",
            "Epoch [1/10], Step [1690], Loss: 0.6221\n",
            "Epoch [1/10], Step [1700], Loss: 0.5832\n",
            "Epoch [1/10], Step [1710], Loss: 0.7611\n",
            "Epoch [1/10], Step [1720], Loss: 0.5162\n",
            "Epoch [1/10], Step [1730], Loss: 0.2891\n",
            "Epoch [2/10], Step [10], Loss: 0.5463\n",
            "Epoch [2/10], Step [20], Loss: 0.6903\n",
            "Epoch [2/10], Step [30], Loss: 0.7791\n",
            "Epoch [2/10], Step [40], Loss: 0.4297\n",
            "Epoch [2/10], Step [50], Loss: 0.6000\n",
            "Epoch [2/10], Step [60], Loss: 0.3768\n",
            "Epoch [2/10], Step [70], Loss: 0.6140\n",
            "Epoch [2/10], Step [80], Loss: 0.4010\n",
            "Epoch [2/10], Step [90], Loss: 0.6801\n",
            "Epoch [2/10], Step [100], Loss: 0.5744\n",
            "Epoch [2/10], Step [110], Loss: 0.4510\n",
            "Epoch [2/10], Step [120], Loss: 0.6719\n",
            "Epoch [2/10], Step [130], Loss: 0.6308\n",
            "Epoch [2/10], Step [140], Loss: 0.4834\n",
            "Epoch [2/10], Step [150], Loss: 0.5929\n",
            "Epoch [2/10], Step [160], Loss: 0.3357\n",
            "Epoch [2/10], Step [170], Loss: 0.4145\n",
            "Epoch [2/10], Step [180], Loss: 0.5226\n",
            "Epoch [2/10], Step [190], Loss: 0.9104\n",
            "Epoch [2/10], Step [200], Loss: 0.6739\n",
            "Epoch [2/10], Step [210], Loss: 0.3344\n",
            "Epoch [2/10], Step [220], Loss: 0.5114\n",
            "Epoch [2/10], Step [230], Loss: 0.3439\n",
            "Epoch [2/10], Step [240], Loss: 0.5800\n",
            "Epoch [2/10], Step [250], Loss: 0.2951\n",
            "Epoch [2/10], Step [260], Loss: 0.4857\n",
            "Epoch [2/10], Step [270], Loss: 0.4141\n",
            "Epoch [2/10], Step [280], Loss: 0.5839\n",
            "Epoch [2/10], Step [290], Loss: 0.5303\n",
            "Epoch [2/10], Step [300], Loss: 0.4481\n",
            "Epoch [2/10], Step [310], Loss: 0.6162\n",
            "Epoch [2/10], Step [320], Loss: 0.6567\n",
            "Epoch [2/10], Step [330], Loss: 0.3804\n",
            "Epoch [2/10], Step [340], Loss: 0.4446\n",
            "Epoch [2/10], Step [350], Loss: 0.5399\n",
            "Epoch [2/10], Step [360], Loss: 0.6105\n",
            "Epoch [2/10], Step [370], Loss: 0.6108\n",
            "Epoch [2/10], Step [380], Loss: 0.4864\n",
            "Epoch [2/10], Step [390], Loss: 0.4277\n",
            "Epoch [2/10], Step [400], Loss: 0.3595\n",
            "Epoch [2/10], Step [410], Loss: 0.5203\n",
            "Epoch [2/10], Step [420], Loss: 0.5962\n",
            "Epoch [2/10], Step [430], Loss: 0.6227\n",
            "Epoch [2/10], Step [440], Loss: 0.5324\n",
            "Epoch [2/10], Step [450], Loss: 0.4573\n",
            "Epoch [2/10], Step [460], Loss: 0.4305\n",
            "Epoch [2/10], Step [470], Loss: 0.7085\n",
            "Epoch [2/10], Step [480], Loss: 0.5956\n",
            "Epoch [2/10], Step [490], Loss: 0.6093\n",
            "Epoch [2/10], Step [500], Loss: 0.4093\n",
            "Epoch [2/10], Step [510], Loss: 0.4951\n",
            "Epoch [2/10], Step [520], Loss: 0.7144\n",
            "Epoch [2/10], Step [530], Loss: 0.3995\n",
            "Epoch [2/10], Step [540], Loss: 0.5877\n",
            "Epoch [2/10], Step [550], Loss: 0.4273\n",
            "Epoch [2/10], Step [560], Loss: 0.4484\n",
            "Epoch [2/10], Step [570], Loss: 0.5301\n",
            "Epoch [2/10], Step [580], Loss: 0.6275\n",
            "Epoch [2/10], Step [590], Loss: 0.4523\n",
            "Epoch [2/10], Step [600], Loss: 0.4759\n",
            "Epoch [2/10], Step [610], Loss: 0.2734\n",
            "Epoch [2/10], Step [620], Loss: 0.4083\n",
            "Epoch [2/10], Step [630], Loss: 0.3079\n",
            "Epoch [2/10], Step [640], Loss: 0.6753\n",
            "Epoch [2/10], Step [650], Loss: 0.5336\n",
            "Epoch [2/10], Step [660], Loss: 0.9241\n",
            "Epoch [2/10], Step [670], Loss: 0.5195\n",
            "Epoch [2/10], Step [680], Loss: 0.5814\n",
            "Epoch [2/10], Step [690], Loss: 0.4396\n",
            "Epoch [2/10], Step [700], Loss: 0.4761\n",
            "Epoch [2/10], Step [710], Loss: 0.4889\n",
            "Epoch [2/10], Step [720], Loss: 0.4761\n",
            "Epoch [2/10], Step [730], Loss: 0.4271\n",
            "Epoch [2/10], Step [740], Loss: 0.7031\n",
            "Epoch [2/10], Step [750], Loss: 0.4347\n",
            "Epoch [2/10], Step [760], Loss: 0.6205\n",
            "Epoch [2/10], Step [770], Loss: 0.4745\n",
            "Epoch [2/10], Step [780], Loss: 0.7362\n",
            "Epoch [2/10], Step [790], Loss: 0.5237\n",
            "Epoch [2/10], Step [800], Loss: 0.2423\n",
            "Epoch [2/10], Step [810], Loss: 0.5594\n",
            "Epoch [2/10], Step [820], Loss: 0.9714\n",
            "Epoch [2/10], Step [830], Loss: 0.6587\n",
            "Epoch [2/10], Step [840], Loss: 0.4011\n",
            "Epoch [2/10], Step [850], Loss: 0.4403\n",
            "Epoch [2/10], Step [860], Loss: 0.4035\n",
            "Epoch [2/10], Step [870], Loss: 0.5507\n",
            "Epoch [2/10], Step [880], Loss: 0.5556\n",
            "Epoch [2/10], Step [890], Loss: 0.6986\n",
            "Epoch [2/10], Step [900], Loss: 0.8395\n",
            "Epoch [2/10], Step [910], Loss: 0.5420\n",
            "Epoch [2/10], Step [920], Loss: 0.5443\n",
            "Epoch [2/10], Step [930], Loss: 0.9288\n",
            "Epoch [2/10], Step [940], Loss: 0.2914\n",
            "Epoch [2/10], Step [950], Loss: 0.6284\n",
            "Epoch [2/10], Step [960], Loss: 0.8907\n",
            "Epoch [2/10], Step [970], Loss: 0.5200\n",
            "Epoch [2/10], Step [980], Loss: 0.5824\n",
            "Epoch [2/10], Step [990], Loss: 0.5867\n",
            "Epoch [2/10], Step [1000], Loss: 0.6518\n",
            "Epoch [2/10], Step [1010], Loss: 0.5436\n",
            "Epoch [2/10], Step [1020], Loss: 0.3862\n",
            "Epoch [2/10], Step [1030], Loss: 0.4530\n",
            "Epoch [2/10], Step [1040], Loss: 0.7550\n",
            "Epoch [2/10], Step [1050], Loss: 0.7893\n",
            "Epoch [2/10], Step [1060], Loss: 0.3555\n",
            "Epoch [2/10], Step [1070], Loss: 0.7869\n",
            "Epoch [2/10], Step [1080], Loss: 0.3179\n",
            "Epoch [2/10], Step [1090], Loss: 0.4504\n",
            "Epoch [2/10], Step [1100], Loss: 0.4151\n",
            "Epoch [2/10], Step [1110], Loss: 0.8148\n",
            "Epoch [2/10], Step [1120], Loss: 0.5191\n",
            "Epoch [2/10], Step [1130], Loss: 0.3632\n",
            "Epoch [2/10], Step [1140], Loss: 0.8440\n",
            "Epoch [2/10], Step [1150], Loss: 0.3596\n",
            "Epoch [2/10], Step [1160], Loss: 0.5846\n",
            "Epoch [2/10], Step [1170], Loss: 0.5780\n",
            "Epoch [2/10], Step [1180], Loss: 0.4062\n",
            "Epoch [2/10], Step [1190], Loss: 0.5655\n",
            "Epoch [2/10], Step [1200], Loss: 1.0324\n",
            "Epoch [2/10], Step [1210], Loss: 0.4774\n",
            "Epoch [2/10], Step [1220], Loss: 0.2992\n",
            "Epoch [2/10], Step [1230], Loss: 0.5746\n",
            "Epoch [2/10], Step [1240], Loss: 0.6855\n",
            "Epoch [2/10], Step [1250], Loss: 0.3967\n",
            "Epoch [2/10], Step [1260], Loss: 0.6803\n",
            "Epoch [2/10], Step [1270], Loss: 0.8315\n",
            "Epoch [2/10], Step [1280], Loss: 1.0403\n",
            "Epoch [2/10], Step [1290], Loss: 0.5800\n",
            "Epoch [2/10], Step [1300], Loss: 0.4082\n",
            "Epoch [2/10], Step [1310], Loss: 0.4852\n",
            "Epoch [2/10], Step [1320], Loss: 0.5392\n",
            "Epoch [2/10], Step [1330], Loss: 0.2096\n",
            "Epoch [2/10], Step [1340], Loss: 0.4009\n",
            "Epoch [2/10], Step [1350], Loss: 0.7019\n",
            "Epoch [2/10], Step [1360], Loss: 0.3846\n",
            "Epoch [2/10], Step [1370], Loss: 0.5707\n",
            "Epoch [2/10], Step [1380], Loss: 0.6753\n",
            "Epoch [2/10], Step [1390], Loss: 0.6010\n",
            "Epoch [2/10], Step [1400], Loss: 0.6119\n",
            "Epoch [2/10], Step [1410], Loss: 0.4998\n",
            "Epoch [2/10], Step [1420], Loss: 0.6970\n",
            "Epoch [2/10], Step [1430], Loss: 0.6598\n",
            "Epoch [2/10], Step [1440], Loss: 0.4748\n",
            "Epoch [2/10], Step [1450], Loss: 0.5747\n",
            "Epoch [2/10], Step [1460], Loss: 0.6263\n",
            "Epoch [2/10], Step [1470], Loss: 0.4898\n",
            "Epoch [2/10], Step [1480], Loss: 0.4135\n",
            "Epoch [2/10], Step [1490], Loss: 0.6637\n",
            "Epoch [2/10], Step [1500], Loss: 0.4007\n",
            "Epoch [2/10], Step [1510], Loss: 0.5424\n",
            "Epoch [2/10], Step [1520], Loss: 0.7549\n",
            "Epoch [2/10], Step [1530], Loss: 0.8324\n",
            "Epoch [2/10], Step [1540], Loss: 0.3860\n",
            "Epoch [2/10], Step [1550], Loss: 0.3862\n",
            "Epoch [2/10], Step [1560], Loss: 0.7664\n",
            "Epoch [2/10], Step [1570], Loss: 0.6278\n",
            "Epoch [2/10], Step [1580], Loss: 0.5764\n",
            "Epoch [2/10], Step [1590], Loss: 0.6146\n",
            "Epoch [2/10], Step [1600], Loss: 0.4991\n",
            "Epoch [2/10], Step [1610], Loss: 0.5356\n",
            "Epoch [2/10], Step [1620], Loss: 0.7927\n",
            "Epoch [2/10], Step [1630], Loss: 0.9512\n",
            "Epoch [2/10], Step [1640], Loss: 0.4793\n",
            "Epoch [2/10], Step [1650], Loss: 0.4231\n",
            "Epoch [2/10], Step [1660], Loss: 0.5484\n",
            "Epoch [2/10], Step [1670], Loss: 0.4291\n",
            "Epoch [2/10], Step [1680], Loss: 0.4734\n",
            "Epoch [2/10], Step [1690], Loss: 0.6224\n",
            "Epoch [2/10], Step [1700], Loss: 0.7108\n",
            "Epoch [2/10], Step [1710], Loss: 0.5234\n",
            "Epoch [2/10], Step [1720], Loss: 0.6632\n",
            "Epoch [2/10], Step [1730], Loss: 0.4098\n",
            "Epoch [3/10], Step [10], Loss: 0.4779\n",
            "Epoch [3/10], Step [20], Loss: 0.4492\n",
            "Epoch [3/10], Step [30], Loss: 0.4276\n",
            "Epoch [3/10], Step [40], Loss: 0.6439\n",
            "Epoch [3/10], Step [50], Loss: 0.6926\n",
            "Epoch [3/10], Step [60], Loss: 0.3972\n",
            "Epoch [3/10], Step [70], Loss: 0.5977\n",
            "Epoch [3/10], Step [80], Loss: 0.5943\n",
            "Epoch [3/10], Step [90], Loss: 0.5098\n",
            "Epoch [3/10], Step [100], Loss: 0.3479\n",
            "Epoch [3/10], Step [110], Loss: 0.4199\n",
            "Epoch [3/10], Step [120], Loss: 0.3966\n",
            "Epoch [3/10], Step [130], Loss: 0.4303\n",
            "Epoch [3/10], Step [140], Loss: 0.2430\n",
            "Epoch [3/10], Step [150], Loss: 0.5660\n",
            "Epoch [3/10], Step [160], Loss: 0.2014\n",
            "Epoch [3/10], Step [170], Loss: 0.3473\n",
            "Epoch [3/10], Step [180], Loss: 0.3646\n",
            "Epoch [3/10], Step [190], Loss: 0.3845\n",
            "Epoch [3/10], Step [200], Loss: 0.6277\n",
            "Epoch [3/10], Step [210], Loss: 0.3918\n",
            "Epoch [3/10], Step [220], Loss: 0.5085\n",
            "Epoch [3/10], Step [230], Loss: 0.3939\n",
            "Epoch [3/10], Step [240], Loss: 0.2861\n",
            "Epoch [3/10], Step [250], Loss: 0.2984\n",
            "Epoch [3/10], Step [260], Loss: 0.6752\n",
            "Epoch [3/10], Step [270], Loss: 0.4158\n",
            "Epoch [3/10], Step [280], Loss: 0.3456\n",
            "Epoch [3/10], Step [290], Loss: 0.2848\n",
            "Epoch [3/10], Step [300], Loss: 0.2906\n",
            "Epoch [3/10], Step [310], Loss: 0.3981\n",
            "Epoch [3/10], Step [320], Loss: 0.5213\n",
            "Epoch [3/10], Step [330], Loss: 0.2909\n",
            "Epoch [3/10], Step [340], Loss: 0.4277\n",
            "Epoch [3/10], Step [350], Loss: 0.4762\n",
            "Epoch [3/10], Step [360], Loss: 0.6462\n",
            "Epoch [3/10], Step [370], Loss: 0.4129\n",
            "Epoch [3/10], Step [380], Loss: 0.5883\n",
            "Epoch [3/10], Step [390], Loss: 0.4997\n",
            "Epoch [3/10], Step [400], Loss: 0.6487\n",
            "Epoch [3/10], Step [410], Loss: 0.3608\n",
            "Epoch [3/10], Step [420], Loss: 0.4035\n",
            "Epoch [3/10], Step [430], Loss: 0.3681\n",
            "Epoch [3/10], Step [440], Loss: 0.4330\n",
            "Epoch [3/10], Step [450], Loss: 0.3621\n",
            "Epoch [3/10], Step [460], Loss: 0.2471\n",
            "Epoch [3/10], Step [470], Loss: 0.2827\n",
            "Epoch [3/10], Step [480], Loss: 0.7337\n",
            "Epoch [3/10], Step [490], Loss: 0.5493\n",
            "Epoch [3/10], Step [500], Loss: 0.2233\n",
            "Epoch [3/10], Step [510], Loss: 0.6276\n",
            "Epoch [3/10], Step [520], Loss: 0.3950\n",
            "Epoch [3/10], Step [530], Loss: 0.5381\n",
            "Epoch [3/10], Step [540], Loss: 0.5118\n",
            "Epoch [3/10], Step [550], Loss: 0.3446\n",
            "Epoch [3/10], Step [560], Loss: 0.5515\n",
            "Epoch [3/10], Step [570], Loss: 0.3039\n",
            "Epoch [3/10], Step [580], Loss: 0.6556\n",
            "Epoch [3/10], Step [590], Loss: 0.5869\n",
            "Epoch [3/10], Step [600], Loss: 0.4949\n",
            "Epoch [3/10], Step [610], Loss: 0.4702\n",
            "Epoch [3/10], Step [620], Loss: 0.8333\n",
            "Epoch [3/10], Step [630], Loss: 0.8401\n",
            "Epoch [3/10], Step [640], Loss: 0.5221\n",
            "Epoch [3/10], Step [650], Loss: 0.4527\n",
            "Epoch [3/10], Step [660], Loss: 0.2592\n",
            "Epoch [3/10], Step [670], Loss: 0.5511\n",
            "Epoch [3/10], Step [680], Loss: 0.8037\n",
            "Epoch [3/10], Step [690], Loss: 0.4781\n",
            "Epoch [3/10], Step [700], Loss: 0.3988\n",
            "Epoch [3/10], Step [710], Loss: 0.4463\n",
            "Epoch [3/10], Step [720], Loss: 0.5455\n",
            "Epoch [3/10], Step [730], Loss: 0.5998\n",
            "Epoch [3/10], Step [740], Loss: 0.3397\n",
            "Epoch [3/10], Step [750], Loss: 0.7435\n",
            "Epoch [3/10], Step [760], Loss: 0.8992\n",
            "Epoch [3/10], Step [770], Loss: 0.3440\n",
            "Epoch [3/10], Step [780], Loss: 0.4836\n",
            "Epoch [3/10], Step [790], Loss: 0.5212\n",
            "Epoch [3/10], Step [800], Loss: 0.4323\n",
            "Epoch [3/10], Step [810], Loss: 0.5485\n",
            "Epoch [3/10], Step [820], Loss: 0.2941\n",
            "Epoch [3/10], Step [830], Loss: 0.4259\n",
            "Epoch [3/10], Step [840], Loss: 0.5477\n",
            "Epoch [3/10], Step [850], Loss: 0.7011\n",
            "Epoch [3/10], Step [860], Loss: 0.4476\n",
            "Epoch [3/10], Step [870], Loss: 0.4312\n",
            "Epoch [3/10], Step [880], Loss: 0.4983\n",
            "Epoch [3/10], Step [890], Loss: 0.3767\n",
            "Epoch [3/10], Step [900], Loss: 0.6074\n",
            "Epoch [3/10], Step [910], Loss: 0.9199\n",
            "Epoch [3/10], Step [920], Loss: 0.9142\n",
            "Epoch [3/10], Step [930], Loss: 0.6099\n",
            "Epoch [3/10], Step [940], Loss: 0.8160\n",
            "Epoch [3/10], Step [950], Loss: 0.5979\n",
            "Epoch [3/10], Step [960], Loss: 0.4424\n",
            "Epoch [3/10], Step [970], Loss: 0.3966\n",
            "Epoch [3/10], Step [980], Loss: 0.5451\n",
            "Epoch [3/10], Step [990], Loss: 0.5561\n",
            "Epoch [3/10], Step [1000], Loss: 0.8738\n",
            "Epoch [3/10], Step [1010], Loss: 0.7382\n",
            "Epoch [3/10], Step [1020], Loss: 0.2521\n",
            "Epoch [3/10], Step [1030], Loss: 0.7235\n",
            "Epoch [3/10], Step [1040], Loss: 0.4028\n",
            "Epoch [3/10], Step [1050], Loss: 0.5829\n",
            "Epoch [3/10], Step [1060], Loss: 0.6235\n",
            "Epoch [3/10], Step [1070], Loss: 0.6685\n",
            "Epoch [3/10], Step [1080], Loss: 0.4150\n",
            "Epoch [3/10], Step [1090], Loss: 0.5694\n",
            "Epoch [3/10], Step [1100], Loss: 0.3218\n",
            "Epoch [3/10], Step [1110], Loss: 0.4053\n",
            "Epoch [3/10], Step [1120], Loss: 0.3986\n",
            "Epoch [3/10], Step [1130], Loss: 0.5280\n",
            "Epoch [3/10], Step [1140], Loss: 0.6416\n",
            "Epoch [3/10], Step [1150], Loss: 0.5228\n",
            "Epoch [3/10], Step [1160], Loss: 0.5455\n",
            "Epoch [3/10], Step [1170], Loss: 0.3851\n",
            "Epoch [3/10], Step [1180], Loss: 0.4798\n",
            "Epoch [3/10], Step [1190], Loss: 0.5670\n",
            "Epoch [3/10], Step [1200], Loss: 1.1716\n",
            "Epoch [3/10], Step [1210], Loss: 0.4554\n",
            "Epoch [3/10], Step [1220], Loss: 0.7309\n",
            "Epoch [3/10], Step [1230], Loss: 0.4980\n",
            "Epoch [3/10], Step [1240], Loss: 0.5275\n",
            "Epoch [3/10], Step [1250], Loss: 0.6381\n",
            "Epoch [3/10], Step [1260], Loss: 0.5933\n",
            "Epoch [3/10], Step [1270], Loss: 0.7674\n",
            "Epoch [3/10], Step [1280], Loss: 0.3001\n",
            "Epoch [3/10], Step [1290], Loss: 0.4593\n",
            "Epoch [3/10], Step [1300], Loss: 0.4649\n",
            "Epoch [3/10], Step [1310], Loss: 0.3353\n",
            "Epoch [3/10], Step [1320], Loss: 0.9445\n",
            "Epoch [3/10], Step [1330], Loss: 0.5568\n",
            "Epoch [3/10], Step [1340], Loss: 0.4490\n",
            "Epoch [3/10], Step [1350], Loss: 0.5656\n",
            "Epoch [3/10], Step [1360], Loss: 0.3637\n",
            "Epoch [3/10], Step [1370], Loss: 0.3298\n",
            "Epoch [3/10], Step [1380], Loss: 0.6094\n",
            "Epoch [3/10], Step [1390], Loss: 0.4759\n",
            "Epoch [3/10], Step [1400], Loss: 0.6537\n",
            "Epoch [3/10], Step [1410], Loss: 0.4847\n",
            "Epoch [3/10], Step [1420], Loss: 0.3225\n",
            "Epoch [3/10], Step [1430], Loss: 0.4807\n",
            "Epoch [3/10], Step [1440], Loss: 0.5133\n",
            "Epoch [3/10], Step [1450], Loss: 0.5723\n",
            "Epoch [3/10], Step [1460], Loss: 0.4393\n",
            "Epoch [3/10], Step [1470], Loss: 0.5584\n",
            "Epoch [3/10], Step [1480], Loss: 0.4947\n",
            "Epoch [3/10], Step [1490], Loss: 0.3177\n",
            "Epoch [3/10], Step [1500], Loss: 0.4602\n",
            "Epoch [3/10], Step [1510], Loss: 0.4401\n",
            "Epoch [3/10], Step [1520], Loss: 0.3846\n",
            "Epoch [3/10], Step [1530], Loss: 0.4590\n",
            "Epoch [3/10], Step [1540], Loss: 0.9819\n",
            "Epoch [3/10], Step [1550], Loss: 0.5197\n",
            "Epoch [3/10], Step [1560], Loss: 0.4801\n",
            "Epoch [3/10], Step [1570], Loss: 0.3150\n",
            "Epoch [3/10], Step [1580], Loss: 0.3730\n",
            "Epoch [3/10], Step [1590], Loss: 0.7240\n",
            "Epoch [3/10], Step [1600], Loss: 0.4051\n",
            "Epoch [3/10], Step [1610], Loss: 0.6472\n",
            "Epoch [3/10], Step [1620], Loss: 0.5179\n",
            "Epoch [3/10], Step [1630], Loss: 0.4668\n",
            "Epoch [3/10], Step [1640], Loss: 0.5810\n",
            "Epoch [3/10], Step [1650], Loss: 0.6936\n",
            "Epoch [3/10], Step [1660], Loss: 0.3734\n",
            "Epoch [3/10], Step [1670], Loss: 0.3524\n",
            "Epoch [3/10], Step [1680], Loss: 0.4987\n",
            "Epoch [3/10], Step [1690], Loss: 0.4334\n",
            "Epoch [3/10], Step [1700], Loss: 0.5590\n",
            "Epoch [3/10], Step [1710], Loss: 0.3190\n",
            "Epoch [3/10], Step [1720], Loss: 0.4361\n",
            "Epoch [3/10], Step [1730], Loss: 0.6137\n",
            "Epoch [4/10], Step [10], Loss: 0.4823\n",
            "Epoch [4/10], Step [20], Loss: 0.3451\n",
            "Epoch [4/10], Step [30], Loss: 0.2801\n",
            "Epoch [4/10], Step [40], Loss: 0.5430\n",
            "Epoch [4/10], Step [50], Loss: 0.6949\n",
            "Epoch [4/10], Step [60], Loss: 0.6217\n",
            "Epoch [4/10], Step [70], Loss: 0.7320\n",
            "Epoch [4/10], Step [80], Loss: 0.3101\n",
            "Epoch [4/10], Step [90], Loss: 0.2201\n",
            "Epoch [4/10], Step [100], Loss: 0.4803\n",
            "Epoch [4/10], Step [110], Loss: 0.3547\n",
            "Epoch [4/10], Step [120], Loss: 0.3625\n",
            "Epoch [4/10], Step [130], Loss: 0.3856\n",
            "Epoch [4/10], Step [140], Loss: 0.2926\n",
            "Epoch [4/10], Step [150], Loss: 0.5257\n",
            "Epoch [4/10], Step [160], Loss: 0.4885\n",
            "Epoch [4/10], Step [170], Loss: 0.3944\n",
            "Epoch [4/10], Step [180], Loss: 0.5449\n",
            "Epoch [4/10], Step [190], Loss: 0.2450\n",
            "Epoch [4/10], Step [200], Loss: 0.1908\n",
            "Epoch [4/10], Step [210], Loss: 0.4573\n",
            "Epoch [4/10], Step [220], Loss: 0.4993\n",
            "Epoch [4/10], Step [230], Loss: 0.5078\n",
            "Epoch [4/10], Step [240], Loss: 0.5880\n",
            "Epoch [4/10], Step [250], Loss: 0.4117\n",
            "Epoch [4/10], Step [260], Loss: 0.4900\n",
            "Epoch [4/10], Step [270], Loss: 0.2918\n",
            "Epoch [4/10], Step [280], Loss: 0.3170\n",
            "Epoch [4/10], Step [290], Loss: 0.4328\n",
            "Epoch [4/10], Step [300], Loss: 0.5157\n",
            "Epoch [4/10], Step [310], Loss: 0.4244\n",
            "Epoch [4/10], Step [320], Loss: 0.5791\n",
            "Epoch [4/10], Step [330], Loss: 0.3985\n",
            "Epoch [4/10], Step [340], Loss: 0.1932\n",
            "Epoch [4/10], Step [350], Loss: 0.2816\n",
            "Epoch [4/10], Step [360], Loss: 0.3092\n",
            "Epoch [4/10], Step [370], Loss: 0.3979\n",
            "Epoch [4/10], Step [380], Loss: 0.3537\n",
            "Epoch [4/10], Step [390], Loss: 0.5476\n",
            "Epoch [4/10], Step [400], Loss: 0.2839\n",
            "Epoch [4/10], Step [410], Loss: 0.2154\n",
            "Epoch [4/10], Step [420], Loss: 0.4240\n",
            "Epoch [4/10], Step [430], Loss: 0.2979\n",
            "Epoch [4/10], Step [440], Loss: 0.3714\n",
            "Epoch [4/10], Step [450], Loss: 0.5208\n",
            "Epoch [4/10], Step [460], Loss: 0.4328\n",
            "Epoch [4/10], Step [470], Loss: 0.3188\n",
            "Epoch [4/10], Step [480], Loss: 0.3222\n",
            "Epoch [4/10], Step [490], Loss: 0.5811\n",
            "Epoch [4/10], Step [500], Loss: 0.3133\n",
            "Epoch [4/10], Step [510], Loss: 0.5425\n",
            "Epoch [4/10], Step [520], Loss: 0.3662\n",
            "Epoch [4/10], Step [530], Loss: 0.2167\n",
            "Epoch [4/10], Step [540], Loss: 0.4956\n",
            "Epoch [4/10], Step [550], Loss: 0.5314\n",
            "Epoch [4/10], Step [560], Loss: 0.3204\n",
            "Epoch [4/10], Step [570], Loss: 0.4565\n",
            "Epoch [4/10], Step [580], Loss: 0.4997\n",
            "Epoch [4/10], Step [590], Loss: 0.3810\n",
            "Epoch [4/10], Step [600], Loss: 0.4115\n",
            "Epoch [4/10], Step [610], Loss: 0.6165\n",
            "Epoch [4/10], Step [620], Loss: 0.3999\n",
            "Epoch [4/10], Step [630], Loss: 0.7590\n",
            "Epoch [4/10], Step [640], Loss: 0.5682\n",
            "Epoch [4/10], Step [650], Loss: 0.4480\n",
            "Epoch [4/10], Step [660], Loss: 0.5340\n",
            "Epoch [4/10], Step [670], Loss: 0.3284\n",
            "Epoch [4/10], Step [680], Loss: 0.3659\n",
            "Epoch [4/10], Step [690], Loss: 0.3378\n",
            "Epoch [4/10], Step [700], Loss: 0.4048\n",
            "Epoch [4/10], Step [710], Loss: 0.7282\n",
            "Epoch [4/10], Step [720], Loss: 0.3741\n",
            "Epoch [4/10], Step [730], Loss: 0.4267\n",
            "Epoch [4/10], Step [740], Loss: 0.8201\n",
            "Epoch [4/10], Step [750], Loss: 0.7686\n",
            "Epoch [4/10], Step [760], Loss: 0.3424\n",
            "Epoch [4/10], Step [770], Loss: 0.4801\n",
            "Epoch [4/10], Step [780], Loss: 1.0131\n",
            "Epoch [4/10], Step [790], Loss: 0.4824\n",
            "Epoch [4/10], Step [800], Loss: 0.4566\n",
            "Epoch [4/10], Step [810], Loss: 0.7954\n",
            "Epoch [4/10], Step [820], Loss: 0.5071\n",
            "Epoch [4/10], Step [830], Loss: 0.4113\n",
            "Epoch [4/10], Step [840], Loss: 0.4420\n",
            "Epoch [4/10], Step [850], Loss: 0.3945\n",
            "Epoch [4/10], Step [860], Loss: 0.4626\n",
            "Epoch [4/10], Step [870], Loss: 0.3149\n",
            "Epoch [4/10], Step [880], Loss: 0.4917\n",
            "Epoch [4/10], Step [890], Loss: 0.3466\n",
            "Epoch [4/10], Step [900], Loss: 0.5416\n",
            "Epoch [4/10], Step [910], Loss: 0.2557\n",
            "Epoch [4/10], Step [920], Loss: 0.3369\n",
            "Epoch [4/10], Step [930], Loss: 0.4274\n",
            "Epoch [4/10], Step [940], Loss: 0.3721\n",
            "Epoch [4/10], Step [950], Loss: 0.6650\n",
            "Epoch [4/10], Step [960], Loss: 0.5089\n",
            "Epoch [4/10], Step [970], Loss: 0.2690\n",
            "Epoch [4/10], Step [980], Loss: 0.5947\n",
            "Epoch [4/10], Step [990], Loss: 0.2538\n",
            "Epoch [4/10], Step [1000], Loss: 0.3675\n",
            "Epoch [4/10], Step [1010], Loss: 0.2666\n",
            "Epoch [4/10], Step [1020], Loss: 0.5141\n",
            "Epoch [4/10], Step [1030], Loss: 0.4603\n",
            "Epoch [4/10], Step [1040], Loss: 0.5570\n",
            "Epoch [4/10], Step [1050], Loss: 0.4638\n",
            "Epoch [4/10], Step [1060], Loss: 0.7804\n",
            "Epoch [4/10], Step [1070], Loss: 0.6571\n",
            "Epoch [4/10], Step [1080], Loss: 0.3679\n",
            "Epoch [4/10], Step [1090], Loss: 0.5577\n",
            "Epoch [4/10], Step [1100], Loss: 0.7981\n",
            "Epoch [4/10], Step [1110], Loss: 0.2021\n",
            "Epoch [4/10], Step [1120], Loss: 0.7133\n",
            "Epoch [4/10], Step [1130], Loss: 0.5013\n",
            "Epoch [4/10], Step [1140], Loss: 0.3045\n",
            "Epoch [4/10], Step [1150], Loss: 0.4400\n",
            "Epoch [4/10], Step [1160], Loss: 0.4415\n",
            "Epoch [4/10], Step [1170], Loss: 0.2144\n",
            "Epoch [4/10], Step [1180], Loss: 0.7403\n",
            "Epoch [4/10], Step [1190], Loss: 0.5435\n",
            "Epoch [4/10], Step [1200], Loss: 0.5313\n",
            "Epoch [4/10], Step [1210], Loss: 0.4953\n",
            "Epoch [4/10], Step [1220], Loss: 0.4832\n",
            "Epoch [4/10], Step [1230], Loss: 0.4163\n",
            "Epoch [4/10], Step [1240], Loss: 0.2360\n",
            "Epoch [4/10], Step [1250], Loss: 0.4756\n",
            "Epoch [4/10], Step [1260], Loss: 0.3429\n",
            "Epoch [4/10], Step [1270], Loss: 0.3760\n",
            "Epoch [4/10], Step [1280], Loss: 0.5557\n",
            "Epoch [4/10], Step [1290], Loss: 0.2521\n",
            "Epoch [4/10], Step [1300], Loss: 0.4215\n",
            "Epoch [4/10], Step [1310], Loss: 0.5089\n",
            "Epoch [4/10], Step [1320], Loss: 0.4319\n",
            "Epoch [4/10], Step [1330], Loss: 0.5713\n",
            "Epoch [4/10], Step [1340], Loss: 0.4158\n",
            "Epoch [4/10], Step [1350], Loss: 0.5941\n",
            "Epoch [4/10], Step [1360], Loss: 0.3475\n",
            "Epoch [4/10], Step [1370], Loss: 0.3751\n",
            "Epoch [4/10], Step [1380], Loss: 0.2823\n",
            "Epoch [4/10], Step [1390], Loss: 0.4507\n",
            "Epoch [4/10], Step [1400], Loss: 0.4873\n",
            "Epoch [4/10], Step [1410], Loss: 0.2587\n",
            "Epoch [4/10], Step [1420], Loss: 0.6039\n",
            "Epoch [4/10], Step [1430], Loss: 0.3042\n",
            "Epoch [4/10], Step [1440], Loss: 0.5801\n",
            "Epoch [4/10], Step [1450], Loss: 0.3811\n",
            "Epoch [4/10], Step [1460], Loss: 0.7018\n",
            "Epoch [4/10], Step [1470], Loss: 0.8517\n",
            "Epoch [4/10], Step [1480], Loss: 0.5109\n",
            "Epoch [4/10], Step [1490], Loss: 0.6400\n",
            "Epoch [4/10], Step [1500], Loss: 0.5271\n",
            "Epoch [4/10], Step [1510], Loss: 0.5015\n",
            "Epoch [4/10], Step [1520], Loss: 0.2655\n",
            "Epoch [4/10], Step [1530], Loss: 0.3374\n",
            "Epoch [4/10], Step [1540], Loss: 0.2000\n",
            "Epoch [4/10], Step [1550], Loss: 0.7511\n",
            "Epoch [4/10], Step [1560], Loss: 0.2818\n",
            "Epoch [4/10], Step [1570], Loss: 0.5884\n",
            "Epoch [4/10], Step [1580], Loss: 0.6854\n",
            "Epoch [4/10], Step [1590], Loss: 0.2293\n",
            "Epoch [4/10], Step [1600], Loss: 0.7686\n",
            "Epoch [4/10], Step [1610], Loss: 0.3201\n",
            "Epoch [4/10], Step [1620], Loss: 0.3342\n",
            "Epoch [4/10], Step [1630], Loss: 0.3569\n",
            "Epoch [4/10], Step [1640], Loss: 0.4161\n",
            "Epoch [4/10], Step [1650], Loss: 0.3897\n",
            "Epoch [4/10], Step [1660], Loss: 0.3890\n",
            "Epoch [4/10], Step [1670], Loss: 0.5455\n",
            "Epoch [4/10], Step [1680], Loss: 0.6582\n",
            "Epoch [4/10], Step [1690], Loss: 0.4783\n",
            "Epoch [4/10], Step [1700], Loss: 0.2995\n",
            "Epoch [4/10], Step [1710], Loss: 0.3464\n",
            "Epoch [4/10], Step [1720], Loss: 0.2596\n",
            "Epoch [4/10], Step [1730], Loss: 0.3709\n",
            "Epoch [5/10], Step [10], Loss: 0.4216\n",
            "Epoch [5/10], Step [20], Loss: 0.6226\n",
            "Epoch [5/10], Step [30], Loss: 0.3279\n",
            "Epoch [5/10], Step [40], Loss: 0.4502\n",
            "Epoch [5/10], Step [50], Loss: 0.2148\n",
            "Epoch [5/10], Step [60], Loss: 0.3215\n",
            "Epoch [5/10], Step [70], Loss: 0.7001\n",
            "Epoch [5/10], Step [80], Loss: 0.5633\n",
            "Epoch [5/10], Step [90], Loss: 0.1773\n",
            "Epoch [5/10], Step [100], Loss: 0.3199\n",
            "Epoch [5/10], Step [110], Loss: 0.6319\n",
            "Epoch [5/10], Step [120], Loss: 0.3804\n",
            "Epoch [5/10], Step [130], Loss: 0.4246\n",
            "Epoch [5/10], Step [140], Loss: 0.5052\n",
            "Epoch [5/10], Step [150], Loss: 0.3649\n",
            "Epoch [5/10], Step [160], Loss: 0.2502\n",
            "Epoch [5/10], Step [170], Loss: 0.4625\n",
            "Epoch [5/10], Step [180], Loss: 0.6046\n",
            "Epoch [5/10], Step [190], Loss: 0.5056\n",
            "Epoch [5/10], Step [200], Loss: 0.3506\n",
            "Epoch [5/10], Step [210], Loss: 0.4064\n",
            "Epoch [5/10], Step [220], Loss: 0.4048\n",
            "Epoch [5/10], Step [230], Loss: 0.5161\n",
            "Epoch [5/10], Step [240], Loss: 0.5118\n",
            "Epoch [5/10], Step [250], Loss: 0.4060\n",
            "Epoch [5/10], Step [260], Loss: 0.7993\n",
            "Epoch [5/10], Step [270], Loss: 0.5568\n",
            "Epoch [5/10], Step [280], Loss: 0.6230\n",
            "Epoch [5/10], Step [290], Loss: 0.3931\n",
            "Epoch [5/10], Step [300], Loss: 0.3004\n",
            "Epoch [5/10], Step [310], Loss: 0.4519\n",
            "Epoch [5/10], Step [320], Loss: 0.5050\n",
            "Epoch [5/10], Step [330], Loss: 0.2880\n",
            "Epoch [5/10], Step [340], Loss: 0.3098\n",
            "Epoch [5/10], Step [350], Loss: 0.3476\n",
            "Epoch [5/10], Step [360], Loss: 0.7003\n",
            "Epoch [5/10], Step [370], Loss: 0.2815\n",
            "Epoch [5/10], Step [380], Loss: 0.4952\n",
            "Epoch [5/10], Step [390], Loss: 0.3965\n",
            "Epoch [5/10], Step [400], Loss: 0.1933\n",
            "Epoch [5/10], Step [410], Loss: 0.3478\n",
            "Epoch [5/10], Step [420], Loss: 0.5206\n",
            "Epoch [5/10], Step [430], Loss: 0.4875\n",
            "Epoch [5/10], Step [440], Loss: 0.5853\n",
            "Epoch [5/10], Step [450], Loss: 0.4319\n",
            "Epoch [5/10], Step [460], Loss: 0.2940\n",
            "Epoch [5/10], Step [470], Loss: 0.3840\n",
            "Epoch [5/10], Step [480], Loss: 0.2886\n",
            "Epoch [5/10], Step [490], Loss: 0.4828\n",
            "Epoch [5/10], Step [500], Loss: 0.5287\n",
            "Epoch [5/10], Step [510], Loss: 0.3512\n",
            "Epoch [5/10], Step [520], Loss: 0.5417\n",
            "Epoch [5/10], Step [530], Loss: 0.3462\n",
            "Epoch [5/10], Step [540], Loss: 0.4875\n",
            "Epoch [5/10], Step [550], Loss: 0.3432\n",
            "Epoch [5/10], Step [560], Loss: 0.2182\n",
            "Epoch [5/10], Step [570], Loss: 0.3704\n",
            "Epoch [5/10], Step [580], Loss: 0.7204\n",
            "Epoch [5/10], Step [590], Loss: 0.2085\n",
            "Epoch [5/10], Step [600], Loss: 0.5967\n",
            "Epoch [5/10], Step [610], Loss: 0.3435\n",
            "Epoch [5/10], Step [620], Loss: 0.6392\n",
            "Epoch [5/10], Step [630], Loss: 0.3817\n",
            "Epoch [5/10], Step [640], Loss: 0.6030\n",
            "Epoch [5/10], Step [650], Loss: 0.2648\n",
            "Epoch [5/10], Step [660], Loss: 0.2196\n",
            "Epoch [5/10], Step [670], Loss: 0.2394\n",
            "Epoch [5/10], Step [680], Loss: 0.7897\n",
            "Epoch [5/10], Step [690], Loss: 0.5381\n",
            "Epoch [5/10], Step [700], Loss: 0.4564\n",
            "Epoch [5/10], Step [710], Loss: 0.3788\n",
            "Epoch [5/10], Step [720], Loss: 0.3091\n",
            "Epoch [5/10], Step [730], Loss: 0.5172\n",
            "Epoch [5/10], Step [740], Loss: 0.4119\n",
            "Epoch [5/10], Step [750], Loss: 0.4847\n",
            "Epoch [5/10], Step [760], Loss: 0.4496\n",
            "Epoch [5/10], Step [770], Loss: 0.2491\n",
            "Epoch [5/10], Step [780], Loss: 0.3406\n",
            "Epoch [5/10], Step [790], Loss: 0.5229\n",
            "Epoch [5/10], Step [800], Loss: 0.2252\n",
            "Epoch [5/10], Step [810], Loss: 0.5102\n",
            "Epoch [5/10], Step [820], Loss: 0.2661\n",
            "Epoch [5/10], Step [830], Loss: 0.6231\n",
            "Epoch [5/10], Step [840], Loss: 0.5956\n",
            "Epoch [5/10], Step [850], Loss: 0.6384\n",
            "Epoch [5/10], Step [860], Loss: 0.4444\n",
            "Epoch [5/10], Step [870], Loss: 0.4819\n",
            "Epoch [5/10], Step [880], Loss: 0.5351\n",
            "Epoch [5/10], Step [890], Loss: 0.6986\n",
            "Epoch [5/10], Step [900], Loss: 0.4613\n",
            "Epoch [5/10], Step [910], Loss: 0.2360\n",
            "Epoch [5/10], Step [920], Loss: 0.5428\n",
            "Epoch [5/10], Step [930], Loss: 0.2202\n",
            "Epoch [5/10], Step [940], Loss: 0.6078\n",
            "Epoch [5/10], Step [950], Loss: 0.2704\n",
            "Epoch [5/10], Step [960], Loss: 0.3835\n",
            "Epoch [5/10], Step [970], Loss: 0.4981\n",
            "Epoch [5/10], Step [980], Loss: 0.4700\n",
            "Epoch [5/10], Step [990], Loss: 0.3137\n",
            "Epoch [5/10], Step [1000], Loss: 0.4841\n",
            "Epoch [5/10], Step [1010], Loss: 0.1996\n",
            "Epoch [5/10], Step [1020], Loss: 0.4462\n",
            "Epoch [5/10], Step [1030], Loss: 0.3420\n",
            "Epoch [5/10], Step [1040], Loss: 0.4369\n",
            "Epoch [5/10], Step [1050], Loss: 0.6400\n",
            "Epoch [5/10], Step [1060], Loss: 0.5337\n",
            "Epoch [5/10], Step [1070], Loss: 0.3225\n",
            "Epoch [5/10], Step [1080], Loss: 0.3934\n",
            "Epoch [5/10], Step [1090], Loss: 0.4271\n",
            "Epoch [5/10], Step [1100], Loss: 0.6108\n",
            "Epoch [5/10], Step [1110], Loss: 0.5143\n",
            "Epoch [5/10], Step [1120], Loss: 0.5664\n",
            "Epoch [5/10], Step [1130], Loss: 0.4721\n",
            "Epoch [5/10], Step [1140], Loss: 0.2255\n",
            "Epoch [5/10], Step [1150], Loss: 0.4836\n",
            "Epoch [5/10], Step [1160], Loss: 0.2714\n",
            "Epoch [5/10], Step [1170], Loss: 0.4354\n",
            "Epoch [5/10], Step [1180], Loss: 0.4003\n",
            "Epoch [5/10], Step [1190], Loss: 0.2388\n",
            "Epoch [5/10], Step [1200], Loss: 0.4040\n",
            "Epoch [5/10], Step [1210], Loss: 0.5277\n",
            "Epoch [5/10], Step [1220], Loss: 0.2615\n",
            "Epoch [5/10], Step [1230], Loss: 0.3102\n",
            "Epoch [5/10], Step [1240], Loss: 0.2763\n",
            "Epoch [5/10], Step [1250], Loss: 0.3402\n",
            "Epoch [5/10], Step [1260], Loss: 0.2900\n",
            "Epoch [5/10], Step [1270], Loss: 0.3484\n",
            "Epoch [5/10], Step [1280], Loss: 0.3556\n",
            "Epoch [5/10], Step [1290], Loss: 0.2890\n",
            "Epoch [5/10], Step [1300], Loss: 0.4586\n",
            "Epoch [5/10], Step [1310], Loss: 0.3030\n",
            "Epoch [5/10], Step [1320], Loss: 0.6123\n",
            "Epoch [5/10], Step [1330], Loss: 0.4165\n",
            "Epoch [5/10], Step [1340], Loss: 0.1961\n",
            "Epoch [5/10], Step [1350], Loss: 0.4638\n",
            "Epoch [5/10], Step [1360], Loss: 0.3784\n",
            "Epoch [5/10], Step [1370], Loss: 0.4799\n",
            "Epoch [5/10], Step [1380], Loss: 0.2714\n",
            "Epoch [5/10], Step [1390], Loss: 0.5141\n",
            "Epoch [5/10], Step [1400], Loss: 0.4587\n",
            "Epoch [5/10], Step [1410], Loss: 0.5681\n",
            "Epoch [5/10], Step [1420], Loss: 0.5318\n",
            "Epoch [5/10], Step [1430], Loss: 0.3865\n",
            "Epoch [5/10], Step [1440], Loss: 0.4151\n",
            "Epoch [5/10], Step [1450], Loss: 0.4042\n",
            "Epoch [5/10], Step [1460], Loss: 0.4810\n",
            "Epoch [5/10], Step [1470], Loss: 0.4441\n",
            "Epoch [5/10], Step [1480], Loss: 0.7111\n",
            "Epoch [5/10], Step [1490], Loss: 0.4863\n",
            "Epoch [5/10], Step [1500], Loss: 0.5320\n",
            "Epoch [5/10], Step [1510], Loss: 0.3174\n",
            "Epoch [5/10], Step [1520], Loss: 0.5534\n",
            "Epoch [5/10], Step [1530], Loss: 0.5126\n",
            "Epoch [5/10], Step [1540], Loss: 0.4307\n",
            "Epoch [5/10], Step [1550], Loss: 0.5721\n",
            "Epoch [5/10], Step [1560], Loss: 0.3220\n",
            "Epoch [5/10], Step [1570], Loss: 0.4183\n",
            "Epoch [5/10], Step [1580], Loss: 0.2126\n",
            "Epoch [5/10], Step [1590], Loss: 0.6240\n",
            "Epoch [5/10], Step [1600], Loss: 0.4326\n",
            "Epoch [5/10], Step [1610], Loss: 0.8623\n",
            "Epoch [5/10], Step [1620], Loss: 0.4641\n",
            "Epoch [5/10], Step [1630], Loss: 0.5491\n",
            "Epoch [5/10], Step [1640], Loss: 0.4407\n",
            "Epoch [5/10], Step [1650], Loss: 0.2675\n",
            "Epoch [5/10], Step [1660], Loss: 0.2149\n",
            "Epoch [5/10], Step [1670], Loss: 0.8031\n",
            "Epoch [5/10], Step [1680], Loss: 0.4873\n",
            "Epoch [5/10], Step [1690], Loss: 0.2096\n",
            "Epoch [5/10], Step [1700], Loss: 0.6280\n",
            "Epoch [5/10], Step [1710], Loss: 0.5950\n",
            "Epoch [5/10], Step [1720], Loss: 0.3677\n",
            "Epoch [5/10], Step [1730], Loss: 0.5161\n",
            "Epoch [6/10], Step [10], Loss: 0.4246\n",
            "Epoch [6/10], Step [20], Loss: 0.2613\n",
            "Epoch [6/10], Step [30], Loss: 0.4071\n",
            "Epoch [6/10], Step [40], Loss: 0.4217\n",
            "Epoch [6/10], Step [50], Loss: 0.3018\n",
            "Epoch [6/10], Step [60], Loss: 0.2314\n",
            "Epoch [6/10], Step [70], Loss: 0.3682\n",
            "Epoch [6/10], Step [80], Loss: 0.2419\n",
            "Epoch [6/10], Step [90], Loss: 0.2216\n",
            "Epoch [6/10], Step [100], Loss: 0.2778\n",
            "Epoch [6/10], Step [110], Loss: 0.4439\n",
            "Epoch [6/10], Step [120], Loss: 0.2925\n",
            "Epoch [6/10], Step [130], Loss: 0.4307\n",
            "Epoch [6/10], Step [140], Loss: 0.3216\n",
            "Epoch [6/10], Step [150], Loss: 0.3024\n",
            "Epoch [6/10], Step [160], Loss: 0.3940\n",
            "Epoch [6/10], Step [170], Loss: 0.3454\n",
            "Epoch [6/10], Step [180], Loss: 0.5880\n",
            "Epoch [6/10], Step [190], Loss: 0.3685\n",
            "Epoch [6/10], Step [200], Loss: 0.4448\n",
            "Epoch [6/10], Step [210], Loss: 0.2696\n",
            "Epoch [6/10], Step [220], Loss: 0.3924\n",
            "Epoch [6/10], Step [230], Loss: 0.3335\n",
            "Epoch [6/10], Step [240], Loss: 0.2782\n",
            "Epoch [6/10], Step [250], Loss: 0.4398\n",
            "Epoch [6/10], Step [260], Loss: 0.2841\n",
            "Epoch [6/10], Step [270], Loss: 0.1837\n",
            "Epoch [6/10], Step [280], Loss: 0.2152\n",
            "Epoch [6/10], Step [290], Loss: 0.5224\n",
            "Epoch [6/10], Step [300], Loss: 0.2441\n",
            "Epoch [6/10], Step [310], Loss: 0.2535\n",
            "Epoch [6/10], Step [320], Loss: 0.2101\n",
            "Epoch [6/10], Step [330], Loss: 0.5262\n",
            "Epoch [6/10], Step [340], Loss: 0.3425\n",
            "Epoch [6/10], Step [350], Loss: 0.2523\n",
            "Epoch [6/10], Step [360], Loss: 0.3349\n",
            "Epoch [6/10], Step [370], Loss: 0.3595\n",
            "Epoch [6/10], Step [380], Loss: 0.6950\n",
            "Epoch [6/10], Step [390], Loss: 0.3477\n",
            "Epoch [6/10], Step [400], Loss: 0.5077\n",
            "Epoch [6/10], Step [410], Loss: 0.4345\n",
            "Epoch [6/10], Step [420], Loss: 0.2221\n",
            "Epoch [6/10], Step [430], Loss: 0.2841\n",
            "Epoch [6/10], Step [440], Loss: 0.2079\n",
            "Epoch [6/10], Step [450], Loss: 0.2327\n",
            "Epoch [6/10], Step [460], Loss: 0.6067\n",
            "Epoch [6/10], Step [470], Loss: 0.2523\n",
            "Epoch [6/10], Step [480], Loss: 0.2720\n",
            "Epoch [6/10], Step [490], Loss: 0.4157\n",
            "Epoch [6/10], Step [500], Loss: 0.3035\n",
            "Epoch [6/10], Step [510], Loss: 0.2987\n",
            "Epoch [6/10], Step [520], Loss: 0.2360\n",
            "Epoch [6/10], Step [530], Loss: 0.2472\n",
            "Epoch [6/10], Step [540], Loss: 0.4020\n",
            "Epoch [6/10], Step [550], Loss: 0.3471\n",
            "Epoch [6/10], Step [560], Loss: 0.3045\n",
            "Epoch [6/10], Step [570], Loss: 0.2783\n",
            "Epoch [6/10], Step [580], Loss: 0.1647\n",
            "Epoch [6/10], Step [590], Loss: 0.3847\n",
            "Epoch [6/10], Step [600], Loss: 0.3526\n",
            "Epoch [6/10], Step [610], Loss: 0.3956\n",
            "Epoch [6/10], Step [620], Loss: 0.3868\n",
            "Epoch [6/10], Step [630], Loss: 0.3258\n",
            "Epoch [6/10], Step [640], Loss: 0.5722\n",
            "Epoch [6/10], Step [650], Loss: 0.5366\n",
            "Epoch [6/10], Step [660], Loss: 0.3920\n",
            "Epoch [6/10], Step [670], Loss: 0.2756\n",
            "Epoch [6/10], Step [680], Loss: 0.6087\n",
            "Epoch [6/10], Step [690], Loss: 0.3832\n",
            "Epoch [6/10], Step [700], Loss: 0.2829\n",
            "Epoch [6/10], Step [710], Loss: 0.4192\n",
            "Epoch [6/10], Step [720], Loss: 0.4301\n",
            "Epoch [6/10], Step [730], Loss: 0.5805\n",
            "Epoch [6/10], Step [740], Loss: 0.2689\n",
            "Epoch [6/10], Step [750], Loss: 0.5084\n",
            "Epoch [6/10], Step [760], Loss: 0.3809\n",
            "Epoch [6/10], Step [770], Loss: 0.3204\n",
            "Epoch [6/10], Step [780], Loss: 0.4292\n",
            "Epoch [6/10], Step [790], Loss: 0.3729\n",
            "Epoch [6/10], Step [800], Loss: 0.3430\n",
            "Epoch [6/10], Step [810], Loss: 0.4460\n",
            "Epoch [6/10], Step [820], Loss: 0.4606\n",
            "Epoch [6/10], Step [830], Loss: 0.2586\n",
            "Epoch [6/10], Step [840], Loss: 0.2681\n",
            "Epoch [6/10], Step [850], Loss: 0.3366\n",
            "Epoch [6/10], Step [860], Loss: 0.5900\n",
            "Epoch [6/10], Step [870], Loss: 0.4275\n",
            "Epoch [6/10], Step [880], Loss: 0.2721\n",
            "Epoch [6/10], Step [890], Loss: 0.4368\n",
            "Epoch [6/10], Step [900], Loss: 0.4782\n",
            "Epoch [6/10], Step [910], Loss: 0.5975\n",
            "Epoch [6/10], Step [920], Loss: 0.2928\n",
            "Epoch [6/10], Step [930], Loss: 0.4963\n",
            "Epoch [6/10], Step [940], Loss: 0.3517\n",
            "Epoch [6/10], Step [950], Loss: 0.2287\n",
            "Epoch [6/10], Step [960], Loss: 0.3948\n",
            "Epoch [6/10], Step [970], Loss: 0.7343\n",
            "Epoch [6/10], Step [980], Loss: 0.2807\n",
            "Epoch [6/10], Step [990], Loss: 0.4992\n",
            "Epoch [6/10], Step [1000], Loss: 0.4117\n",
            "Epoch [6/10], Step [1010], Loss: 0.4509\n",
            "Epoch [6/10], Step [1020], Loss: 0.4241\n",
            "Epoch [6/10], Step [1030], Loss: 0.2589\n",
            "Epoch [6/10], Step [1040], Loss: 0.3558\n",
            "Epoch [6/10], Step [1050], Loss: 0.3794\n",
            "Epoch [6/10], Step [1060], Loss: 0.3050\n",
            "Epoch [6/10], Step [1070], Loss: 0.4541\n",
            "Epoch [6/10], Step [1080], Loss: 0.3403\n",
            "Epoch [6/10], Step [1090], Loss: 0.4810\n",
            "Epoch [6/10], Step [1100], Loss: 0.4433\n",
            "Epoch [6/10], Step [1110], Loss: 0.3769\n",
            "Epoch [6/10], Step [1120], Loss: 0.2655\n",
            "Epoch [6/10], Step [1130], Loss: 0.4258\n",
            "Epoch [6/10], Step [1140], Loss: 0.3795\n",
            "Epoch [6/10], Step [1150], Loss: 0.4367\n",
            "Epoch [6/10], Step [1160], Loss: 0.7466\n",
            "Epoch [6/10], Step [1170], Loss: 0.5888\n",
            "Epoch [6/10], Step [1180], Loss: 0.4450\n",
            "Epoch [6/10], Step [1190], Loss: 0.4778\n",
            "Epoch [6/10], Step [1200], Loss: 0.3535\n",
            "Epoch [6/10], Step [1210], Loss: 0.5576\n",
            "Epoch [6/10], Step [1220], Loss: 0.4090\n",
            "Epoch [6/10], Step [1230], Loss: 0.4982\n",
            "Epoch [6/10], Step [1240], Loss: 0.4746\n",
            "Epoch [6/10], Step [1250], Loss: 0.2543\n",
            "Epoch [6/10], Step [1260], Loss: 0.2684\n",
            "Epoch [6/10], Step [1270], Loss: 0.3526\n",
            "Epoch [6/10], Step [1280], Loss: 0.4721\n",
            "Epoch [6/10], Step [1290], Loss: 0.2525\n",
            "Epoch [6/10], Step [1300], Loss: 0.3012\n",
            "Epoch [6/10], Step [1310], Loss: 0.3315\n",
            "Epoch [6/10], Step [1320], Loss: 0.2072\n",
            "Epoch [6/10], Step [1330], Loss: 0.7301\n",
            "Epoch [6/10], Step [1340], Loss: 0.3338\n",
            "Epoch [6/10], Step [1350], Loss: 0.3622\n",
            "Epoch [6/10], Step [1360], Loss: 0.4844\n",
            "Epoch [6/10], Step [1370], Loss: 0.4465\n",
            "Epoch [6/10], Step [1380], Loss: 0.3856\n",
            "Epoch [6/10], Step [1390], Loss: 0.4375\n",
            "Epoch [6/10], Step [1400], Loss: 0.4830\n",
            "Epoch [6/10], Step [1410], Loss: 0.3957\n",
            "Epoch [6/10], Step [1420], Loss: 0.6242\n",
            "Epoch [6/10], Step [1430], Loss: 0.4407\n",
            "Epoch [6/10], Step [1440], Loss: 0.6130\n",
            "Epoch [6/10], Step [1450], Loss: 0.2943\n",
            "Epoch [6/10], Step [1460], Loss: 0.2841\n",
            "Epoch [6/10], Step [1470], Loss: 0.3625\n",
            "Epoch [6/10], Step [1480], Loss: 0.5984\n",
            "Epoch [6/10], Step [1490], Loss: 0.6115\n",
            "Epoch [6/10], Step [1500], Loss: 0.5386\n",
            "Epoch [6/10], Step [1510], Loss: 0.2161\n",
            "Epoch [6/10], Step [1520], Loss: 0.4325\n",
            "Epoch [6/10], Step [1530], Loss: 0.3538\n",
            "Epoch [6/10], Step [1540], Loss: 0.5832\n",
            "Epoch [6/10], Step [1550], Loss: 0.2020\n",
            "Epoch [6/10], Step [1560], Loss: 0.5922\n",
            "Epoch [6/10], Step [1570], Loss: 0.4653\n",
            "Epoch [6/10], Step [1580], Loss: 0.2655\n",
            "Epoch [6/10], Step [1590], Loss: 0.5558\n",
            "Epoch [6/10], Step [1600], Loss: 0.3227\n",
            "Epoch [6/10], Step [1610], Loss: 0.4177\n",
            "Epoch [6/10], Step [1620], Loss: 0.3109\n",
            "Epoch [6/10], Step [1630], Loss: 0.5294\n",
            "Epoch [6/10], Step [1640], Loss: 0.3946\n",
            "Epoch [6/10], Step [1650], Loss: 0.2899\n",
            "Epoch [6/10], Step [1660], Loss: 0.3582\n",
            "Epoch [6/10], Step [1670], Loss: 0.1825\n",
            "Epoch [6/10], Step [1680], Loss: 0.3050\n",
            "Epoch [6/10], Step [1690], Loss: 0.2949\n",
            "Epoch [6/10], Step [1700], Loss: 0.2416\n",
            "Epoch [6/10], Step [1710], Loss: 0.3334\n",
            "Epoch [6/10], Step [1720], Loss: 0.3998\n",
            "Epoch [6/10], Step [1730], Loss: 0.2776\n",
            "Epoch [7/10], Step [10], Loss: 0.6291\n",
            "Epoch [7/10], Step [20], Loss: 0.4040\n",
            "Epoch [7/10], Step [30], Loss: 0.2850\n",
            "Epoch [7/10], Step [40], Loss: 0.3369\n",
            "Epoch [7/10], Step [50], Loss: 0.2161\n",
            "Epoch [7/10], Step [60], Loss: 0.2329\n",
            "Epoch [7/10], Step [70], Loss: 0.3897\n",
            "Epoch [7/10], Step [80], Loss: 0.4876\n",
            "Epoch [7/10], Step [90], Loss: 0.3387\n",
            "Epoch [7/10], Step [100], Loss: 0.2306\n",
            "Epoch [7/10], Step [110], Loss: 0.3060\n",
            "Epoch [7/10], Step [120], Loss: 0.2768\n",
            "Epoch [7/10], Step [130], Loss: 0.2069\n",
            "Epoch [7/10], Step [140], Loss: 0.1636\n",
            "Epoch [7/10], Step [150], Loss: 0.4244\n",
            "Epoch [7/10], Step [160], Loss: 0.7935\n",
            "Epoch [7/10], Step [170], Loss: 0.3389\n",
            "Epoch [7/10], Step [180], Loss: 0.4195\n",
            "Epoch [7/10], Step [190], Loss: 0.5307\n",
            "Epoch [7/10], Step [200], Loss: 0.4591\n",
            "Epoch [7/10], Step [210], Loss: 0.5354\n",
            "Epoch [7/10], Step [220], Loss: 0.2018\n",
            "Epoch [7/10], Step [230], Loss: 0.4561\n",
            "Epoch [7/10], Step [240], Loss: 0.4384\n",
            "Epoch [7/10], Step [250], Loss: 0.5837\n",
            "Epoch [7/10], Step [260], Loss: 0.3785\n",
            "Epoch [7/10], Step [270], Loss: 0.2632\n",
            "Epoch [7/10], Step [280], Loss: 0.2765\n",
            "Epoch [7/10], Step [290], Loss: 0.2857\n",
            "Epoch [7/10], Step [300], Loss: 0.2084\n",
            "Epoch [7/10], Step [310], Loss: 0.3994\n",
            "Epoch [7/10], Step [320], Loss: 0.3688\n",
            "Epoch [7/10], Step [330], Loss: 0.5167\n",
            "Epoch [7/10], Step [340], Loss: 0.2340\n",
            "Epoch [7/10], Step [350], Loss: 0.3788\n",
            "Epoch [7/10], Step [360], Loss: 0.4837\n",
            "Epoch [7/10], Step [370], Loss: 0.3762\n",
            "Epoch [7/10], Step [380], Loss: 0.3892\n",
            "Epoch [7/10], Step [390], Loss: 0.2236\n",
            "Epoch [7/10], Step [400], Loss: 0.5902\n",
            "Epoch [7/10], Step [410], Loss: 0.2064\n",
            "Epoch [7/10], Step [420], Loss: 0.4120\n",
            "Epoch [7/10], Step [430], Loss: 0.4384\n",
            "Epoch [7/10], Step [440], Loss: 0.1055\n",
            "Epoch [7/10], Step [450], Loss: 0.4716\n",
            "Epoch [7/10], Step [460], Loss: 0.4462\n",
            "Epoch [7/10], Step [470], Loss: 0.3712\n",
            "Epoch [7/10], Step [480], Loss: 0.3844\n",
            "Epoch [7/10], Step [490], Loss: 0.3022\n",
            "Epoch [7/10], Step [500], Loss: 0.1845\n",
            "Epoch [7/10], Step [510], Loss: 0.4336\n",
            "Epoch [7/10], Step [520], Loss: 0.3756\n",
            "Epoch [7/10], Step [530], Loss: 0.7880\n",
            "Epoch [7/10], Step [540], Loss: 0.4874\n",
            "Epoch [7/10], Step [550], Loss: 0.3191\n",
            "Epoch [7/10], Step [560], Loss: 0.3825\n",
            "Epoch [7/10], Step [570], Loss: 0.3677\n",
            "Epoch [7/10], Step [580], Loss: 0.1994\n",
            "Epoch [7/10], Step [590], Loss: 0.4906\n",
            "Epoch [7/10], Step [600], Loss: 0.3650\n",
            "Epoch [7/10], Step [610], Loss: 0.2621\n",
            "Epoch [7/10], Step [620], Loss: 0.5307\n",
            "Epoch [7/10], Step [630], Loss: 0.5996\n",
            "Epoch [7/10], Step [640], Loss: 0.2254\n",
            "Epoch [7/10], Step [650], Loss: 0.2368\n",
            "Epoch [7/10], Step [660], Loss: 0.4107\n",
            "Epoch [7/10], Step [670], Loss: 0.4250\n",
            "Epoch [7/10], Step [680], Loss: 0.2122\n",
            "Epoch [7/10], Step [690], Loss: 0.4653\n",
            "Epoch [7/10], Step [700], Loss: 0.3652\n",
            "Epoch [7/10], Step [710], Loss: 0.2690\n",
            "Epoch [7/10], Step [720], Loss: 0.5839\n",
            "Epoch [7/10], Step [730], Loss: 0.2906\n",
            "Epoch [7/10], Step [740], Loss: 0.1615\n",
            "Epoch [7/10], Step [750], Loss: 0.3221\n",
            "Epoch [7/10], Step [760], Loss: 0.5063\n",
            "Epoch [7/10], Step [770], Loss: 0.4712\n",
            "Epoch [7/10], Step [780], Loss: 0.3271\n",
            "Epoch [7/10], Step [790], Loss: 0.2961\n",
            "Epoch [7/10], Step [800], Loss: 0.3897\n",
            "Epoch [7/10], Step [810], Loss: 0.3980\n",
            "Epoch [7/10], Step [820], Loss: 0.3105\n",
            "Epoch [7/10], Step [830], Loss: 0.3592\n",
            "Epoch [7/10], Step [840], Loss: 0.6315\n",
            "Epoch [7/10], Step [850], Loss: 0.3138\n",
            "Epoch [7/10], Step [860], Loss: 0.3130\n",
            "Epoch [7/10], Step [870], Loss: 0.2425\n",
            "Epoch [7/10], Step [880], Loss: 0.7006\n",
            "Epoch [7/10], Step [890], Loss: 0.2968\n",
            "Epoch [7/10], Step [900], Loss: 1.1362\n",
            "Epoch [7/10], Step [910], Loss: 0.1066\n",
            "Epoch [7/10], Step [920], Loss: 0.5150\n",
            "Epoch [7/10], Step [930], Loss: 0.1602\n",
            "Epoch [7/10], Step [940], Loss: 0.5809\n",
            "Epoch [7/10], Step [950], Loss: 0.5316\n",
            "Epoch [7/10], Step [960], Loss: 0.5706\n",
            "Epoch [7/10], Step [970], Loss: 0.5431\n",
            "Epoch [7/10], Step [980], Loss: 0.2418\n",
            "Epoch [7/10], Step [990], Loss: 0.3827\n",
            "Epoch [7/10], Step [1000], Loss: 0.1164\n",
            "Epoch [7/10], Step [1010], Loss: 0.5983\n",
            "Epoch [7/10], Step [1020], Loss: 0.3936\n",
            "Epoch [7/10], Step [1030], Loss: 0.3870\n",
            "Epoch [7/10], Step [1040], Loss: 0.3771\n",
            "Epoch [7/10], Step [1050], Loss: 0.2725\n",
            "Epoch [7/10], Step [1060], Loss: 0.2331\n",
            "Epoch [7/10], Step [1070], Loss: 0.2975\n",
            "Epoch [7/10], Step [1080], Loss: 0.4272\n",
            "Epoch [7/10], Step [1090], Loss: 0.2898\n",
            "Epoch [7/10], Step [1100], Loss: 0.4667\n",
            "Epoch [7/10], Step [1110], Loss: 0.2450\n",
            "Epoch [7/10], Step [1120], Loss: 0.2439\n",
            "Epoch [7/10], Step [1130], Loss: 0.1931\n",
            "Epoch [7/10], Step [1140], Loss: 0.2597\n",
            "Epoch [7/10], Step [1150], Loss: 0.3051\n",
            "Epoch [7/10], Step [1160], Loss: 0.3077\n",
            "Epoch [7/10], Step [1170], Loss: 0.5084\n",
            "Epoch [7/10], Step [1180], Loss: 0.3386\n",
            "Epoch [7/10], Step [1190], Loss: 0.1943\n",
            "Epoch [7/10], Step [1200], Loss: 0.4395\n",
            "Epoch [7/10], Step [1210], Loss: 0.2368\n",
            "Epoch [7/10], Step [1220], Loss: 0.2281\n",
            "Epoch [7/10], Step [1230], Loss: 0.4412\n",
            "Epoch [7/10], Step [1240], Loss: 0.2978\n",
            "Epoch [7/10], Step [1250], Loss: 0.3670\n",
            "Epoch [7/10], Step [1260], Loss: 0.3515\n",
            "Epoch [7/10], Step [1270], Loss: 0.3980\n",
            "Epoch [7/10], Step [1280], Loss: 0.4163\n",
            "Epoch [7/10], Step [1290], Loss: 0.3362\n",
            "Epoch [7/10], Step [1300], Loss: 0.5755\n",
            "Epoch [7/10], Step [1310], Loss: 0.2704\n",
            "Epoch [7/10], Step [1320], Loss: 0.2953\n",
            "Epoch [7/10], Step [1330], Loss: 0.2929\n",
            "Epoch [7/10], Step [1340], Loss: 0.3986\n",
            "Epoch [7/10], Step [1350], Loss: 0.4890\n",
            "Epoch [7/10], Step [1360], Loss: 0.6345\n",
            "Epoch [7/10], Step [1370], Loss: 0.7183\n",
            "Epoch [7/10], Step [1380], Loss: 0.2927\n",
            "Epoch [7/10], Step [1390], Loss: 0.2951\n",
            "Epoch [7/10], Step [1400], Loss: 0.3610\n",
            "Epoch [7/10], Step [1410], Loss: 0.2697\n",
            "Epoch [7/10], Step [1420], Loss: 0.5471\n",
            "Epoch [7/10], Step [1430], Loss: 0.5266\n",
            "Epoch [7/10], Step [1440], Loss: 0.2047\n",
            "Epoch [7/10], Step [1450], Loss: 0.3353\n",
            "Epoch [7/10], Step [1460], Loss: 0.4586\n",
            "Epoch [7/10], Step [1470], Loss: 0.2491\n",
            "Epoch [7/10], Step [1480], Loss: 0.2023\n",
            "Epoch [7/10], Step [1490], Loss: 0.5727\n",
            "Epoch [7/10], Step [1500], Loss: 0.3766\n",
            "Epoch [7/10], Step [1510], Loss: 0.1614\n",
            "Epoch [7/10], Step [1520], Loss: 0.4011\n",
            "Epoch [7/10], Step [1530], Loss: 0.4054\n",
            "Epoch [7/10], Step [1540], Loss: 0.4270\n",
            "Epoch [7/10], Step [1550], Loss: 0.2108\n",
            "Epoch [7/10], Step [1560], Loss: 0.5391\n",
            "Epoch [7/10], Step [1570], Loss: 0.3213\n",
            "Epoch [7/10], Step [1580], Loss: 0.2739\n",
            "Epoch [7/10], Step [1590], Loss: 0.3748\n",
            "Epoch [7/10], Step [1600], Loss: 0.2202\n",
            "Epoch [7/10], Step [1610], Loss: 0.3784\n",
            "Epoch [7/10], Step [1620], Loss: 0.2850\n",
            "Epoch [7/10], Step [1630], Loss: 0.5672\n",
            "Epoch [7/10], Step [1640], Loss: 0.5724\n",
            "Epoch [7/10], Step [1650], Loss: 0.4308\n",
            "Epoch [7/10], Step [1660], Loss: 0.4927\n",
            "Epoch [7/10], Step [1670], Loss: 0.7809\n",
            "Epoch [7/10], Step [1680], Loss: 0.3244\n",
            "Epoch [7/10], Step [1690], Loss: 0.1249\n",
            "Epoch [7/10], Step [1700], Loss: 0.4729\n",
            "Epoch [7/10], Step [1710], Loss: 0.3302\n",
            "Epoch [7/10], Step [1720], Loss: 0.4741\n",
            "Epoch [7/10], Step [1730], Loss: 0.1773\n",
            "Epoch [8/10], Step [10], Loss: 0.1998\n",
            "Epoch [8/10], Step [20], Loss: 0.3566\n",
            "Epoch [8/10], Step [30], Loss: 0.2843\n",
            "Epoch [8/10], Step [40], Loss: 0.4691\n",
            "Epoch [8/10], Step [50], Loss: 0.3241\n",
            "Epoch [8/10], Step [60], Loss: 0.2967\n",
            "Epoch [8/10], Step [70], Loss: 0.4004\n",
            "Epoch [8/10], Step [80], Loss: 0.4601\n",
            "Epoch [8/10], Step [90], Loss: 0.1596\n",
            "Epoch [8/10], Step [100], Loss: 0.2731\n",
            "Epoch [8/10], Step [110], Loss: 0.3918\n",
            "Epoch [8/10], Step [120], Loss: 0.2079\n",
            "Epoch [8/10], Step [130], Loss: 0.5156\n",
            "Epoch [8/10], Step [140], Loss: 0.2405\n",
            "Epoch [8/10], Step [150], Loss: 0.2543\n",
            "Epoch [8/10], Step [160], Loss: 0.4127\n",
            "Epoch [8/10], Step [170], Loss: 0.3541\n",
            "Epoch [8/10], Step [180], Loss: 0.5096\n",
            "Epoch [8/10], Step [190], Loss: 0.4172\n",
            "Epoch [8/10], Step [200], Loss: 0.1193\n",
            "Epoch [8/10], Step [210], Loss: 0.5009\n",
            "Epoch [8/10], Step [220], Loss: 0.2068\n",
            "Epoch [8/10], Step [230], Loss: 0.2111\n",
            "Epoch [8/10], Step [240], Loss: 0.2206\n",
            "Epoch [8/10], Step [250], Loss: 0.5986\n",
            "Epoch [8/10], Step [260], Loss: 0.2281\n",
            "Epoch [8/10], Step [270], Loss: 0.5921\n",
            "Epoch [8/10], Step [280], Loss: 0.2818\n",
            "Epoch [8/10], Step [290], Loss: 0.2647\n",
            "Epoch [8/10], Step [300], Loss: 0.2671\n",
            "Epoch [8/10], Step [310], Loss: 0.4855\n",
            "Epoch [8/10], Step [320], Loss: 0.2330\n",
            "Epoch [8/10], Step [330], Loss: 0.2268\n",
            "Epoch [8/10], Step [340], Loss: 0.3719\n",
            "Epoch [8/10], Step [350], Loss: 0.2550\n",
            "Epoch [8/10], Step [360], Loss: 0.7058\n",
            "Epoch [8/10], Step [370], Loss: 0.2028\n",
            "Epoch [8/10], Step [380], Loss: 0.3598\n",
            "Epoch [8/10], Step [390], Loss: 0.5984\n",
            "Epoch [8/10], Step [400], Loss: 0.2501\n",
            "Epoch [8/10], Step [410], Loss: 0.2207\n",
            "Epoch [8/10], Step [420], Loss: 0.3361\n",
            "Epoch [8/10], Step [430], Loss: 0.4107\n",
            "Epoch [8/10], Step [440], Loss: 0.2218\n",
            "Epoch [8/10], Step [450], Loss: 0.1514\n",
            "Epoch [8/10], Step [460], Loss: 0.5055\n",
            "Epoch [8/10], Step [470], Loss: 0.3864\n",
            "Epoch [8/10], Step [480], Loss: 0.3580\n",
            "Epoch [8/10], Step [490], Loss: 0.3053\n",
            "Epoch [8/10], Step [500], Loss: 0.2929\n",
            "Epoch [8/10], Step [510], Loss: 0.3631\n",
            "Epoch [8/10], Step [520], Loss: 0.2959\n",
            "Epoch [8/10], Step [530], Loss: 0.2088\n",
            "Epoch [8/10], Step [540], Loss: 0.2637\n",
            "Epoch [8/10], Step [550], Loss: 0.6039\n",
            "Epoch [8/10], Step [560], Loss: 0.2636\n",
            "Epoch [8/10], Step [570], Loss: 0.5640\n",
            "Epoch [8/10], Step [580], Loss: 0.3293\n",
            "Epoch [8/10], Step [590], Loss: 0.2384\n",
            "Epoch [8/10], Step [600], Loss: 0.4002\n",
            "Epoch [8/10], Step [610], Loss: 0.2066\n",
            "Epoch [8/10], Step [620], Loss: 0.6877\n",
            "Epoch [8/10], Step [630], Loss: 0.3680\n",
            "Epoch [8/10], Step [640], Loss: 0.2162\n",
            "Epoch [8/10], Step [650], Loss: 0.1388\n",
            "Epoch [8/10], Step [660], Loss: 0.2359\n",
            "Epoch [8/10], Step [670], Loss: 0.2498\n",
            "Epoch [8/10], Step [680], Loss: 0.4571\n",
            "Epoch [8/10], Step [690], Loss: 0.2663\n",
            "Epoch [8/10], Step [700], Loss: 0.2516\n",
            "Epoch [8/10], Step [710], Loss: 0.1925\n",
            "Epoch [8/10], Step [720], Loss: 0.0782\n",
            "Epoch [8/10], Step [730], Loss: 0.4108\n",
            "Epoch [8/10], Step [740], Loss: 0.4457\n",
            "Epoch [8/10], Step [750], Loss: 0.2311\n",
            "Epoch [8/10], Step [760], Loss: 0.4071\n",
            "Epoch [8/10], Step [770], Loss: 0.4044\n",
            "Epoch [8/10], Step [780], Loss: 0.3766\n",
            "Epoch [8/10], Step [790], Loss: 0.3667\n",
            "Epoch [8/10], Step [800], Loss: 0.3199\n",
            "Epoch [8/10], Step [810], Loss: 0.3297\n",
            "Epoch [8/10], Step [820], Loss: 0.2608\n",
            "Epoch [8/10], Step [830], Loss: 0.3452\n",
            "Epoch [8/10], Step [840], Loss: 0.3004\n",
            "Epoch [8/10], Step [850], Loss: 0.3499\n",
            "Epoch [8/10], Step [860], Loss: 0.2411\n",
            "Epoch [8/10], Step [870], Loss: 0.2625\n",
            "Epoch [8/10], Step [880], Loss: 0.1542\n",
            "Epoch [8/10], Step [890], Loss: 0.2904\n",
            "Epoch [8/10], Step [900], Loss: 0.2037\n",
            "Epoch [8/10], Step [910], Loss: 0.3892\n",
            "Epoch [8/10], Step [920], Loss: 0.4360\n",
            "Epoch [8/10], Step [930], Loss: 0.4414\n",
            "Epoch [8/10], Step [940], Loss: 0.1575\n",
            "Epoch [8/10], Step [950], Loss: 0.1731\n",
            "Epoch [8/10], Step [960], Loss: 0.1866\n",
            "Epoch [8/10], Step [970], Loss: 0.2439\n",
            "Epoch [8/10], Step [980], Loss: 0.3961\n",
            "Epoch [8/10], Step [990], Loss: 0.1910\n",
            "Epoch [8/10], Step [1000], Loss: 0.3620\n",
            "Epoch [8/10], Step [1010], Loss: 0.2861\n",
            "Epoch [8/10], Step [1020], Loss: 0.4372\n",
            "Epoch [8/10], Step [1030], Loss: 0.1952\n",
            "Epoch [8/10], Step [1040], Loss: 0.1657\n",
            "Epoch [8/10], Step [1050], Loss: 0.2742\n",
            "Epoch [8/10], Step [1060], Loss: 0.3374\n",
            "Epoch [8/10], Step [1070], Loss: 0.2345\n",
            "Epoch [8/10], Step [1080], Loss: 0.2659\n",
            "Epoch [8/10], Step [1090], Loss: 0.8037\n",
            "Epoch [8/10], Step [1100], Loss: 0.2464\n",
            "Epoch [8/10], Step [1110], Loss: 0.3941\n",
            "Epoch [8/10], Step [1120], Loss: 0.3447\n",
            "Epoch [8/10], Step [1130], Loss: 0.3126\n",
            "Epoch [8/10], Step [1140], Loss: 0.3270\n",
            "Epoch [8/10], Step [1150], Loss: 0.4507\n",
            "Epoch [8/10], Step [1160], Loss: 0.4198\n",
            "Epoch [8/10], Step [1170], Loss: 0.4257\n",
            "Epoch [8/10], Step [1180], Loss: 0.1824\n",
            "Epoch [8/10], Step [1190], Loss: 0.4099\n",
            "Epoch [8/10], Step [1200], Loss: 0.4029\n",
            "Epoch [8/10], Step [1210], Loss: 0.2098\n",
            "Epoch [8/10], Step [1220], Loss: 0.3145\n",
            "Epoch [8/10], Step [1230], Loss: 0.1054\n",
            "Epoch [8/10], Step [1240], Loss: 0.2371\n",
            "Epoch [8/10], Step [1250], Loss: 0.2013\n",
            "Epoch [8/10], Step [1260], Loss: 0.4394\n",
            "Epoch [8/10], Step [1270], Loss: 0.3979\n",
            "Epoch [8/10], Step [1280], Loss: 0.3045\n",
            "Epoch [8/10], Step [1290], Loss: 0.2145\n",
            "Epoch [8/10], Step [1300], Loss: 0.4444\n",
            "Epoch [8/10], Step [1310], Loss: 0.6022\n",
            "Epoch [8/10], Step [1320], Loss: 0.7309\n",
            "Epoch [8/10], Step [1330], Loss: 0.4323\n",
            "Epoch [8/10], Step [1340], Loss: 0.2515\n",
            "Epoch [8/10], Step [1350], Loss: 0.3232\n",
            "Epoch [8/10], Step [1360], Loss: 0.4318\n",
            "Epoch [8/10], Step [1370], Loss: 0.4848\n",
            "Epoch [8/10], Step [1380], Loss: 0.4582\n",
            "Epoch [8/10], Step [1390], Loss: 0.2133\n",
            "Epoch [8/10], Step [1400], Loss: 0.2008\n",
            "Epoch [8/10], Step [1410], Loss: 0.3851\n",
            "Epoch [8/10], Step [1420], Loss: 0.6315\n",
            "Epoch [8/10], Step [1430], Loss: 0.1529\n",
            "Epoch [8/10], Step [1440], Loss: 0.3778\n",
            "Epoch [8/10], Step [1450], Loss: 0.2440\n",
            "Epoch [8/10], Step [1460], Loss: 0.2897\n",
            "Epoch [8/10], Step [1470], Loss: 0.2079\n",
            "Epoch [8/10], Step [1480], Loss: 0.2261\n",
            "Epoch [8/10], Step [1490], Loss: 0.2797\n",
            "Epoch [8/10], Step [1500], Loss: 0.2868\n",
            "Epoch [8/10], Step [1510], Loss: 0.4227\n",
            "Epoch [8/10], Step [1520], Loss: 0.3723\n",
            "Epoch [8/10], Step [1530], Loss: 0.2207\n",
            "Epoch [8/10], Step [1540], Loss: 0.4762\n",
            "Epoch [8/10], Step [1550], Loss: 0.3300\n",
            "Epoch [8/10], Step [1560], Loss: 0.2136\n",
            "Epoch [8/10], Step [1570], Loss: 0.1651\n",
            "Epoch [8/10], Step [1580], Loss: 0.8094\n",
            "Epoch [8/10], Step [1590], Loss: 0.4508\n",
            "Epoch [8/10], Step [1600], Loss: 0.3079\n",
            "Epoch [8/10], Step [1610], Loss: 0.3987\n",
            "Epoch [8/10], Step [1620], Loss: 0.4118\n",
            "Epoch [8/10], Step [1630], Loss: 0.2795\n",
            "Epoch [8/10], Step [1640], Loss: 0.5364\n",
            "Epoch [8/10], Step [1650], Loss: 0.3495\n",
            "Epoch [8/10], Step [1660], Loss: 0.6767\n",
            "Epoch [8/10], Step [1670], Loss: 0.2382\n",
            "Epoch [8/10], Step [1680], Loss: 0.4219\n",
            "Epoch [8/10], Step [1690], Loss: 0.1987\n",
            "Epoch [8/10], Step [1700], Loss: 0.3259\n",
            "Epoch [8/10], Step [1710], Loss: 0.3550\n",
            "Epoch [8/10], Step [1720], Loss: 0.2756\n",
            "Epoch [8/10], Step [1730], Loss: 0.1790\n",
            "Epoch [9/10], Step [10], Loss: 0.2134\n",
            "Epoch [9/10], Step [20], Loss: 0.5131\n",
            "Epoch [9/10], Step [30], Loss: 0.3034\n",
            "Epoch [9/10], Step [40], Loss: 0.1918\n",
            "Epoch [9/10], Step [50], Loss: 0.2491\n",
            "Epoch [9/10], Step [60], Loss: 0.1554\n",
            "Epoch [9/10], Step [70], Loss: 0.5066\n",
            "Epoch [9/10], Step [80], Loss: 0.1654\n",
            "Epoch [9/10], Step [90], Loss: 0.1681\n",
            "Epoch [9/10], Step [100], Loss: 0.1863\n",
            "Epoch [9/10], Step [110], Loss: 0.1901\n",
            "Epoch [9/10], Step [120], Loss: 0.2280\n",
            "Epoch [9/10], Step [130], Loss: 0.1147\n",
            "Epoch [9/10], Step [140], Loss: 0.3566\n",
            "Epoch [9/10], Step [150], Loss: 0.0583\n",
            "Epoch [9/10], Step [160], Loss: 0.3931\n",
            "Epoch [9/10], Step [170], Loss: 0.4704\n",
            "Epoch [9/10], Step [180], Loss: 0.3003\n",
            "Epoch [9/10], Step [190], Loss: 0.2118\n",
            "Epoch [9/10], Step [200], Loss: 0.1565\n",
            "Epoch [9/10], Step [210], Loss: 0.3005\n",
            "Epoch [9/10], Step [220], Loss: 0.1019\n",
            "Epoch [9/10], Step [230], Loss: 0.2156\n",
            "Epoch [9/10], Step [240], Loss: 0.4778\n",
            "Epoch [9/10], Step [250], Loss: 0.4224\n",
            "Epoch [9/10], Step [260], Loss: 0.2484\n",
            "Epoch [9/10], Step [270], Loss: 0.2455\n",
            "Epoch [9/10], Step [280], Loss: 0.2948\n",
            "Epoch [9/10], Step [290], Loss: 0.1870\n",
            "Epoch [9/10], Step [300], Loss: 0.2724\n",
            "Epoch [9/10], Step [310], Loss: 0.4608\n",
            "Epoch [9/10], Step [320], Loss: 0.2232\n",
            "Epoch [9/10], Step [330], Loss: 0.1428\n",
            "Epoch [9/10], Step [340], Loss: 0.2313\n",
            "Epoch [9/10], Step [350], Loss: 0.2609\n",
            "Epoch [9/10], Step [360], Loss: 0.2099\n",
            "Epoch [9/10], Step [370], Loss: 0.3401\n",
            "Epoch [9/10], Step [380], Loss: 0.4664\n",
            "Epoch [9/10], Step [390], Loss: 0.2559\n",
            "Epoch [9/10], Step [400], Loss: 0.2333\n",
            "Epoch [9/10], Step [410], Loss: 0.1393\n",
            "Epoch [9/10], Step [420], Loss: 0.2265\n",
            "Epoch [9/10], Step [430], Loss: 0.1117\n",
            "Epoch [9/10], Step [440], Loss: 0.2933\n",
            "Epoch [9/10], Step [450], Loss: 0.3933\n",
            "Epoch [9/10], Step [460], Loss: 0.2368\n",
            "Epoch [9/10], Step [470], Loss: 0.2985\n",
            "Epoch [9/10], Step [480], Loss: 0.2825\n",
            "Epoch [9/10], Step [490], Loss: 0.2156\n",
            "Epoch [9/10], Step [500], Loss: 0.4301\n",
            "Epoch [9/10], Step [510], Loss: 0.3229\n",
            "Epoch [9/10], Step [520], Loss: 0.2749\n",
            "Epoch [9/10], Step [530], Loss: 0.2494\n",
            "Epoch [9/10], Step [540], Loss: 0.1716\n",
            "Epoch [9/10], Step [550], Loss: 0.4277\n",
            "Epoch [9/10], Step [560], Loss: 0.3606\n",
            "Epoch [9/10], Step [570], Loss: 0.2994\n",
            "Epoch [9/10], Step [580], Loss: 0.3766\n",
            "Epoch [9/10], Step [590], Loss: 0.1678\n",
            "Epoch [9/10], Step [600], Loss: 0.3011\n",
            "Epoch [9/10], Step [610], Loss: 0.2679\n",
            "Epoch [9/10], Step [620], Loss: 0.4024\n",
            "Epoch [9/10], Step [630], Loss: 0.2741\n",
            "Epoch [9/10], Step [640], Loss: 0.5983\n",
            "Epoch [9/10], Step [650], Loss: 0.2565\n",
            "Epoch [9/10], Step [660], Loss: 0.2720\n",
            "Epoch [9/10], Step [670], Loss: 0.3794\n",
            "Epoch [9/10], Step [680], Loss: 0.3753\n",
            "Epoch [9/10], Step [690], Loss: 0.3608\n",
            "Epoch [9/10], Step [700], Loss: 0.4473\n",
            "Epoch [9/10], Step [710], Loss: 0.4838\n",
            "Epoch [9/10], Step [720], Loss: 0.4284\n",
            "Epoch [9/10], Step [730], Loss: 0.2220\n",
            "Epoch [9/10], Step [740], Loss: 0.3486\n",
            "Epoch [9/10], Step [750], Loss: 0.2378\n",
            "Epoch [9/10], Step [760], Loss: 0.3520\n",
            "Epoch [9/10], Step [770], Loss: 0.2349\n",
            "Epoch [9/10], Step [780], Loss: 0.7396\n",
            "Epoch [9/10], Step [790], Loss: 0.2101\n",
            "Epoch [9/10], Step [800], Loss: 0.1557\n",
            "Epoch [9/10], Step [810], Loss: 0.4004\n",
            "Epoch [9/10], Step [820], Loss: 0.4286\n",
            "Epoch [9/10], Step [830], Loss: 0.3733\n",
            "Epoch [9/10], Step [840], Loss: 0.2308\n",
            "Epoch [9/10], Step [850], Loss: 0.2676\n",
            "Epoch [9/10], Step [860], Loss: 0.3723\n",
            "Epoch [9/10], Step [870], Loss: 0.4283\n",
            "Epoch [9/10], Step [880], Loss: 0.4391\n",
            "Epoch [9/10], Step [890], Loss: 0.1154\n",
            "Epoch [9/10], Step [900], Loss: 0.3302\n",
            "Epoch [9/10], Step [910], Loss: 0.2938\n",
            "Epoch [9/10], Step [920], Loss: 0.5185\n",
            "Epoch [9/10], Step [930], Loss: 0.3016\n",
            "Epoch [9/10], Step [940], Loss: 0.4611\n",
            "Epoch [9/10], Step [950], Loss: 0.2926\n",
            "Epoch [9/10], Step [960], Loss: 0.4246\n",
            "Epoch [9/10], Step [970], Loss: 0.5309\n",
            "Epoch [9/10], Step [980], Loss: 0.3130\n",
            "Epoch [9/10], Step [990], Loss: 0.2840\n",
            "Epoch [9/10], Step [1000], Loss: 0.5175\n",
            "Epoch [9/10], Step [1010], Loss: 0.2734\n",
            "Epoch [9/10], Step [1020], Loss: 0.2766\n",
            "Epoch [9/10], Step [1030], Loss: 0.2447\n",
            "Epoch [9/10], Step [1040], Loss: 0.2693\n",
            "Epoch [9/10], Step [1050], Loss: 0.1895\n",
            "Epoch [9/10], Step [1060], Loss: 0.3584\n",
            "Epoch [9/10], Step [1070], Loss: 0.2906\n",
            "Epoch [9/10], Step [1080], Loss: 0.1982\n",
            "Epoch [9/10], Step [1090], Loss: 0.4630\n",
            "Epoch [9/10], Step [1100], Loss: 0.1265\n",
            "Epoch [9/10], Step [1110], Loss: 0.5213\n",
            "Epoch [9/10], Step [1120], Loss: 0.1865\n",
            "Epoch [9/10], Step [1130], Loss: 0.2419\n",
            "Epoch [9/10], Step [1140], Loss: 0.3041\n",
            "Epoch [9/10], Step [1150], Loss: 0.3054\n",
            "Epoch [9/10], Step [1160], Loss: 0.4624\n",
            "Epoch [9/10], Step [1170], Loss: 0.4030\n",
            "Epoch [9/10], Step [1180], Loss: 0.4425\n",
            "Epoch [9/10], Step [1190], Loss: 0.6102\n",
            "Epoch [9/10], Step [1200], Loss: 0.3450\n",
            "Epoch [9/10], Step [1210], Loss: 0.3779\n",
            "Epoch [9/10], Step [1220], Loss: 0.1724\n",
            "Epoch [9/10], Step [1230], Loss: 0.5953\n",
            "Epoch [9/10], Step [1240], Loss: 0.3171\n",
            "Epoch [9/10], Step [1250], Loss: 0.2548\n",
            "Epoch [9/10], Step [1260], Loss: 0.2588\n",
            "Epoch [9/10], Step [1270], Loss: 0.4159\n",
            "Epoch [9/10], Step [1280], Loss: 0.1507\n",
            "Epoch [9/10], Step [1290], Loss: 0.3044\n",
            "Epoch [9/10], Step [1300], Loss: 0.3666\n",
            "Epoch [9/10], Step [1310], Loss: 0.3383\n",
            "Epoch [9/10], Step [1320], Loss: 0.5540\n",
            "Epoch [9/10], Step [1330], Loss: 0.3121\n",
            "Epoch [9/10], Step [1340], Loss: 0.3229\n",
            "Epoch [9/10], Step [1350], Loss: 0.3055\n",
            "Epoch [9/10], Step [1360], Loss: 0.2732\n",
            "Epoch [9/10], Step [1370], Loss: 0.2001\n",
            "Epoch [9/10], Step [1380], Loss: 0.3060\n",
            "Epoch [9/10], Step [1390], Loss: 0.3933\n",
            "Epoch [9/10], Step [1400], Loss: 0.1570\n",
            "Epoch [9/10], Step [1410], Loss: 0.3242\n",
            "Epoch [9/10], Step [1420], Loss: 0.2880\n",
            "Epoch [9/10], Step [1430], Loss: 0.2562\n",
            "Epoch [9/10], Step [1440], Loss: 0.3366\n",
            "Epoch [9/10], Step [1450], Loss: 0.3550\n",
            "Epoch [9/10], Step [1460], Loss: 0.1571\n",
            "Epoch [9/10], Step [1470], Loss: 0.6323\n",
            "Epoch [9/10], Step [1480], Loss: 0.2058\n",
            "Epoch [9/10], Step [1490], Loss: 0.3612\n",
            "Epoch [9/10], Step [1500], Loss: 0.2396\n",
            "Epoch [9/10], Step [1510], Loss: 0.2644\n",
            "Epoch [9/10], Step [1520], Loss: 0.6441\n",
            "Epoch [9/10], Step [1530], Loss: 0.4760\n",
            "Epoch [9/10], Step [1540], Loss: 0.3392\n",
            "Epoch [9/10], Step [1550], Loss: 0.4627\n",
            "Epoch [9/10], Step [1560], Loss: 0.5069\n",
            "Epoch [9/10], Step [1570], Loss: 0.5143\n",
            "Epoch [9/10], Step [1580], Loss: 0.2608\n",
            "Epoch [9/10], Step [1590], Loss: 0.1547\n",
            "Epoch [9/10], Step [1600], Loss: 0.3113\n",
            "Epoch [9/10], Step [1610], Loss: 0.2125\n",
            "Epoch [9/10], Step [1620], Loss: 0.1471\n",
            "Epoch [9/10], Step [1630], Loss: 0.4126\n",
            "Epoch [9/10], Step [1640], Loss: 0.4884\n",
            "Epoch [9/10], Step [1650], Loss: 0.5605\n",
            "Epoch [9/10], Step [1660], Loss: 0.3194\n",
            "Epoch [9/10], Step [1670], Loss: 0.2338\n",
            "Epoch [9/10], Step [1680], Loss: 0.1898\n",
            "Epoch [9/10], Step [1690], Loss: 0.2947\n",
            "Epoch [9/10], Step [1700], Loss: 0.2617\n",
            "Epoch [9/10], Step [1710], Loss: 0.3125\n",
            "Epoch [9/10], Step [1720], Loss: 0.2960\n",
            "Epoch [9/10], Step [1730], Loss: 0.2872\n",
            "Epoch [10/10], Step [10], Loss: 0.2948\n",
            "Epoch [10/10], Step [20], Loss: 0.2803\n",
            "Epoch [10/10], Step [30], Loss: 0.2317\n",
            "Epoch [10/10], Step [40], Loss: 0.2334\n",
            "Epoch [10/10], Step [50], Loss: 0.2557\n",
            "Epoch [10/10], Step [60], Loss: 0.1167\n",
            "Epoch [10/10], Step [70], Loss: 0.1088\n",
            "Epoch [10/10], Step [80], Loss: 0.1482\n",
            "Epoch [10/10], Step [90], Loss: 0.2310\n",
            "Epoch [10/10], Step [100], Loss: 0.2750\n",
            "Epoch [10/10], Step [110], Loss: 0.4135\n",
            "Epoch [10/10], Step [120], Loss: 0.2419\n",
            "Epoch [10/10], Step [130], Loss: 0.3021\n",
            "Epoch [10/10], Step [140], Loss: 0.1468\n",
            "Epoch [10/10], Step [150], Loss: 0.2609\n",
            "Epoch [10/10], Step [160], Loss: 0.4529\n",
            "Epoch [10/10], Step [170], Loss: 0.1596\n",
            "Epoch [10/10], Step [180], Loss: 0.2214\n",
            "Epoch [10/10], Step [190], Loss: 0.2425\n",
            "Epoch [10/10], Step [200], Loss: 0.1648\n",
            "Epoch [10/10], Step [210], Loss: 0.2510\n",
            "Epoch [10/10], Step [220], Loss: 0.1206\n",
            "Epoch [10/10], Step [230], Loss: 0.4245\n",
            "Epoch [10/10], Step [240], Loss: 0.2423\n",
            "Epoch [10/10], Step [250], Loss: 0.2878\n",
            "Epoch [10/10], Step [260], Loss: 0.2640\n",
            "Epoch [10/10], Step [270], Loss: 0.3443\n",
            "Epoch [10/10], Step [280], Loss: 0.3824\n",
            "Epoch [10/10], Step [290], Loss: 0.2062\n",
            "Epoch [10/10], Step [300], Loss: 0.1305\n",
            "Epoch [10/10], Step [310], Loss: 0.2853\n",
            "Epoch [10/10], Step [320], Loss: 0.4658\n",
            "Epoch [10/10], Step [330], Loss: 0.3365\n",
            "Epoch [10/10], Step [340], Loss: 0.3724\n",
            "Epoch [10/10], Step [350], Loss: 0.4236\n",
            "Epoch [10/10], Step [360], Loss: 0.2549\n",
            "Epoch [10/10], Step [370], Loss: 0.3794\n",
            "Epoch [10/10], Step [380], Loss: 0.1217\n",
            "Epoch [10/10], Step [390], Loss: 0.2118\n",
            "Epoch [10/10], Step [400], Loss: 0.4317\n",
            "Epoch [10/10], Step [410], Loss: 0.2580\n",
            "Epoch [10/10], Step [420], Loss: 0.1666\n",
            "Epoch [10/10], Step [430], Loss: 0.2639\n",
            "Epoch [10/10], Step [440], Loss: 0.4624\n",
            "Epoch [10/10], Step [450], Loss: 0.3129\n",
            "Epoch [10/10], Step [460], Loss: 0.2600\n",
            "Epoch [10/10], Step [470], Loss: 0.4032\n",
            "Epoch [10/10], Step [480], Loss: 0.1025\n",
            "Epoch [10/10], Step [490], Loss: 0.1601\n",
            "Epoch [10/10], Step [500], Loss: 0.2439\n",
            "Epoch [10/10], Step [510], Loss: 0.2565\n",
            "Epoch [10/10], Step [520], Loss: 0.1250\n",
            "Epoch [10/10], Step [530], Loss: 0.4150\n",
            "Epoch [10/10], Step [540], Loss: 0.2880\n",
            "Epoch [10/10], Step [550], Loss: 0.2247\n",
            "Epoch [10/10], Step [560], Loss: 0.1916\n",
            "Epoch [10/10], Step [570], Loss: 0.2287\n",
            "Epoch [10/10], Step [580], Loss: 0.2513\n",
            "Epoch [10/10], Step [590], Loss: 0.3836\n",
            "Epoch [10/10], Step [600], Loss: 0.2936\n",
            "Epoch [10/10], Step [610], Loss: 0.3839\n",
            "Epoch [10/10], Step [620], Loss: 0.3079\n",
            "Epoch [10/10], Step [630], Loss: 0.0943\n",
            "Epoch [10/10], Step [640], Loss: 0.3266\n",
            "Epoch [10/10], Step [650], Loss: 0.1442\n",
            "Epoch [10/10], Step [660], Loss: 0.0905\n",
            "Epoch [10/10], Step [670], Loss: 0.2186\n",
            "Epoch [10/10], Step [680], Loss: 0.3961\n",
            "Epoch [10/10], Step [690], Loss: 0.6236\n",
            "Epoch [10/10], Step [700], Loss: 0.4709\n",
            "Epoch [10/10], Step [710], Loss: 0.2373\n",
            "Epoch [10/10], Step [720], Loss: 0.1465\n",
            "Epoch [10/10], Step [730], Loss: 0.2850\n",
            "Epoch [10/10], Step [740], Loss: 0.2793\n",
            "Epoch [10/10], Step [750], Loss: 0.3632\n",
            "Epoch [10/10], Step [760], Loss: 0.2691\n",
            "Epoch [10/10], Step [770], Loss: 0.2389\n",
            "Epoch [10/10], Step [780], Loss: 0.4209\n",
            "Epoch [10/10], Step [790], Loss: 0.1772\n",
            "Epoch [10/10], Step [800], Loss: 0.1600\n",
            "Epoch [10/10], Step [810], Loss: 0.3397\n",
            "Epoch [10/10], Step [820], Loss: 0.2032\n",
            "Epoch [10/10], Step [830], Loss: 0.2693\n",
            "Epoch [10/10], Step [840], Loss: 0.3805\n",
            "Epoch [10/10], Step [850], Loss: 0.1523\n",
            "Epoch [10/10], Step [860], Loss: 0.1460\n",
            "Epoch [10/10], Step [870], Loss: 0.2304\n",
            "Epoch [10/10], Step [880], Loss: 0.2847\n",
            "Epoch [10/10], Step [890], Loss: 0.2232\n",
            "Epoch [10/10], Step [900], Loss: 0.2685\n",
            "Epoch [10/10], Step [910], Loss: 0.2264\n",
            "Epoch [10/10], Step [920], Loss: 0.3719\n",
            "Epoch [10/10], Step [930], Loss: 0.7088\n",
            "Epoch [10/10], Step [940], Loss: 0.1615\n",
            "Epoch [10/10], Step [950], Loss: 0.1901\n",
            "Epoch [10/10], Step [960], Loss: 0.4034\n",
            "Epoch [10/10], Step [970], Loss: 0.4827\n",
            "Epoch [10/10], Step [980], Loss: 0.3041\n",
            "Epoch [10/10], Step [990], Loss: 0.1400\n",
            "Epoch [10/10], Step [1000], Loss: 0.2068\n",
            "Epoch [10/10], Step [1010], Loss: 0.3782\n",
            "Epoch [10/10], Step [1020], Loss: 0.4178\n",
            "Epoch [10/10], Step [1030], Loss: 0.1077\n",
            "Epoch [10/10], Step [1040], Loss: 0.5970\n",
            "Epoch [10/10], Step [1050], Loss: 0.6622\n",
            "Epoch [10/10], Step [1060], Loss: 0.2062\n",
            "Epoch [10/10], Step [1070], Loss: 0.2146\n",
            "Epoch [10/10], Step [1080], Loss: 0.2559\n",
            "Epoch [10/10], Step [1090], Loss: 0.2771\n",
            "Epoch [10/10], Step [1100], Loss: 0.3785\n",
            "Epoch [10/10], Step [1110], Loss: 0.1531\n",
            "Epoch [10/10], Step [1120], Loss: 0.5285\n",
            "Epoch [10/10], Step [1130], Loss: 0.1557\n",
            "Epoch [10/10], Step [1140], Loss: 0.2969\n",
            "Epoch [10/10], Step [1150], Loss: 0.2353\n",
            "Epoch [10/10], Step [1160], Loss: 0.2517\n",
            "Epoch [10/10], Step [1170], Loss: 0.3144\n",
            "Epoch [10/10], Step [1180], Loss: 0.4936\n",
            "Epoch [10/10], Step [1190], Loss: 0.1750\n",
            "Epoch [10/10], Step [1200], Loss: 0.4427\n",
            "Epoch [10/10], Step [1210], Loss: 0.3190\n",
            "Epoch [10/10], Step [1220], Loss: 0.2328\n",
            "Epoch [10/10], Step [1230], Loss: 0.3053\n",
            "Epoch [10/10], Step [1240], Loss: 0.2528\n",
            "Epoch [10/10], Step [1250], Loss: 0.0916\n",
            "Epoch [10/10], Step [1260], Loss: 0.1623\n",
            "Epoch [10/10], Step [1270], Loss: 0.2964\n",
            "Epoch [10/10], Step [1280], Loss: 0.5545\n",
            "Epoch [10/10], Step [1290], Loss: 0.2106\n",
            "Epoch [10/10], Step [1300], Loss: 0.5200\n",
            "Epoch [10/10], Step [1310], Loss: 0.3016\n",
            "Epoch [10/10], Step [1320], Loss: 0.2234\n",
            "Epoch [10/10], Step [1330], Loss: 0.1176\n",
            "Epoch [10/10], Step [1340], Loss: 0.2894\n",
            "Epoch [10/10], Step [1350], Loss: 0.2737\n",
            "Epoch [10/10], Step [1360], Loss: 0.3244\n",
            "Epoch [10/10], Step [1370], Loss: 0.1962\n",
            "Epoch [10/10], Step [1380], Loss: 0.2063\n",
            "Epoch [10/10], Step [1390], Loss: 0.7060\n",
            "Epoch [10/10], Step [1400], Loss: 0.2755\n",
            "Epoch [10/10], Step [1410], Loss: 0.3285\n",
            "Epoch [10/10], Step [1420], Loss: 0.2333\n",
            "Epoch [10/10], Step [1430], Loss: 0.1811\n",
            "Epoch [10/10], Step [1440], Loss: 0.2928\n",
            "Epoch [10/10], Step [1450], Loss: 0.5581\n",
            "Epoch [10/10], Step [1460], Loss: 0.2912\n",
            "Epoch [10/10], Step [1470], Loss: 0.2603\n",
            "Epoch [10/10], Step [1480], Loss: 0.4400\n",
            "Epoch [10/10], Step [1490], Loss: 0.1001\n",
            "Epoch [10/10], Step [1500], Loss: 0.2651\n",
            "Epoch [10/10], Step [1510], Loss: 0.4322\n",
            "Epoch [10/10], Step [1520], Loss: 0.1307\n",
            "Epoch [10/10], Step [1530], Loss: 0.4696\n",
            "Epoch [10/10], Step [1540], Loss: 0.3723\n",
            "Epoch [10/10], Step [1550], Loss: 0.2801\n",
            "Epoch [10/10], Step [1560], Loss: 0.2604\n",
            "Epoch [10/10], Step [1570], Loss: 0.2027\n",
            "Epoch [10/10], Step [1580], Loss: 0.2909\n",
            "Epoch [10/10], Step [1590], Loss: 0.2719\n",
            "Epoch [10/10], Step [1600], Loss: 0.2702\n",
            "Epoch [10/10], Step [1610], Loss: 0.1790\n",
            "Epoch [10/10], Step [1620], Loss: 0.2631\n",
            "Epoch [10/10], Step [1630], Loss: 0.0823\n",
            "Epoch [10/10], Step [1640], Loss: 0.3235\n",
            "Epoch [10/10], Step [1650], Loss: 0.5185\n",
            "Epoch [10/10], Step [1660], Loss: 0.3196\n",
            "Epoch [10/10], Step [1670], Loss: 0.5135\n",
            "Epoch [10/10], Step [1680], Loss: 0.2292\n",
            "Epoch [10/10], Step [1690], Loss: 0.2700\n",
            "Epoch [10/10], Step [1700], Loss: 0.2954\n",
            "Epoch [10/10], Step [1710], Loss: 0.2041\n",
            "Epoch [10/10], Step [1720], Loss: 0.2465\n",
            "Epoch [10/10], Step [1730], Loss: 0.2671\n",
            "Model saved to gru_model.pth\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "# Each word (or token) is represented as a 300-dimensional vector\n",
        "embedding_size = 300\n",
        "# Number of time steps in each sequence (e.g. number of words in a sentence)         \n",
        "sequence_length = train_data_X.shape[1]\n",
        "            \n",
        "model = RNNClassifier(embedding_size, hidden_size=128, num_layers=1, num_classes=4).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "model_path = \"gru_model.pth\"\n",
        "\n",
        "\n",
        "if os.path.exists(model_path):\n",
        "    print(\"Loading saved model\")\n",
        "    model = RNNClassifier(\n",
        "    embedding_size=embedding_size,  # input size per time step\n",
        "    hidden_size=128,                # how big the internal memory is\n",
        "    num_layers=1,                   # how many GRU layers stacked\n",
        "    num_classes=4                   # number of output classes to predict\n",
        "    ).to(device)\n",
        "    \n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "else:\n",
        "    print(\"Training new model\")\n",
        "    model = RNNClassifier(embedding_size, hidden_size=128, num_layers=1, num_classes=num_classes).to(device)\n",
        "    # Loss function: used to compare predicted vs actual class\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # Optimizer: helps the model learn by adjusting weights\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "             # Forward pass: model makes a prediction\n",
        "            outputs = model(inputs)\n",
        "            # Compute how far off the prediction was\n",
        "            loss = criterion(outputs, labels)\n",
        "             # Backpropagation: update model to reduce loss\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if (i + 1) % 10 == 0:\n",
        "                print(f\"Epoch [{epoch+1}/{epochs}], Step [{i+1}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(f\"Model saved to {model_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f635a3c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True label: Normal vs Predicted: Normal\n",
            "True label: Anxiety-like vs Predicted: Depression\n",
            "True label: Depression vs Predicted: Depression\n",
            "True label: Depression vs Predicted: Depression\n",
            "True label: Normal vs Predicted: Normal\n",
            "True label: Suicidal vs Predicted: Suicidal\n",
            "True label: Normal vs Predicted: Normal\n",
            "True label: Depression vs Predicted: Suicidal\n",
            "True label: Depression vs Predicted: Depression\n",
            "True label: Depression vs Predicted: Depression\n",
            "True label: Anxiety-like vs Predicted: Anxiety-like\n",
            "True label: Normal vs Predicted: Normal\n",
            "True label: Normal vs Predicted: Depression\n",
            "True label: Depression vs Predicted: Suicidal\n",
            "True label: Normal vs Predicted: Normal\n",
            "True label: Normal vs Predicted: Normal\n",
            "True label: Depression vs Predicted: Depression\n",
            "True label: Suicidal vs Predicted: Suicidal\n",
            "True label: Depression vs Predicted: Depression\n",
            "True label: Anxiety-like vs Predicted: Anxiety-like\n",
            "\n",
            " Accuracy on test data: 76.20415089413915%\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "label_names = ['Normal', 'Depression', 'Suicidal', 'Anxiety-like']\n",
        "# Set the model to evaluation mode (disables dropout, etc.)\n",
        "model.eval()\n",
        "y_true = []\n",
        "y_pred = []\n",
        "# Disable gradient calculation to speed up inference\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    shown_examples = 0  # limit number of example predictions to print\n",
        "\n",
        "    for batch_idx, (inputs, labels) in enumerate(test_loader):\n",
        "         # Move data to GPU/CPU device\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        # Run model to get predictions\n",
        "        outputs = model(inputs)\n",
        "          # Get the index of the highest score as the predicted class\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "         # Update total and count correct predictions\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "         # Save results for confusion matrix\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "         # Print up to 20 predicted vs actual labels (for inspection)\n",
        "        for i in range(inputs.size(0)):\n",
        "            if shown_examples >= 20:\n",
        "                break\n",
        "            true_label_id = labels[i].item()\n",
        "            pred_label_id = predicted[i].item()\n",
        "            true_label_name = label_names[true_label_id]\n",
        "            pred_label_name = label_names[pred_label_id]\n",
        "            print(f\"True label: {true_label_name} vs Predicted: {pred_label_name}\")\n",
        "            shown_examples += 1\n",
        "    # Calculate overall accuracy\n",
        "    acc = 100.0 * correct / total\n",
        "    print(f'\\n Accuracy on test data: {acc}%')\n",
        "\n",
        "     # Create and display the confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_names)\n",
        "    disp.plot(cmap=\"Blues\", values_format='d')\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c828c50f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       0.91      0.89      0.90      2479\n",
            "  Depression       0.70      0.64      0.67      2227\n",
            "    Suicidal       0.61      0.71      0.65      1511\n",
            "Anxiety-like       0.78      0.78      0.78      1444\n",
            "\n",
            "    accuracy                           0.76      7661\n",
            "   macro avg       0.75      0.76      0.75      7661\n",
            "weighted avg       0.77      0.76      0.76      7661\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true, y_pred, target_names=label_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4103666e",
      "metadata": {},
      "source": [
        "# Final Note on our results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad455cc2",
      "metadata": {},
      "source": [
        "it is noticed that our accuary on our test data has changed positively when we tried out different models. For the RNN model, we could not exceed the 70 % accuracy, and that is logical given the complexity and long sequences found in our data. As for the GRU, we may notice that there is an increase in terms of accuaracy to 76 %, and for our LSTM models the accuracy ranged between 75% and 78%  for our best model. Thus we may conclude that the reason behind not achieving stronger accuracies is related to the nature of our dataset.\n",
        "\n",
        "When we split the dataset into 4 classes, we convey 4 different categories under a class named other. This conveyment might be the reason why we did not get better results. The four mergered classes (anxiety, bipolar, stress, personality disorder) are classified as one target class that is causing label noise, after all it is a hetrogeneous class that is based on different patterns merged together. Moreover, there is a similarity between some of the depression and suicidal classes with the other class (actually with the hetrogenous classes conveying the other class), this could be solved by categorizing symptoms that similar together but may also cause class imbalance, and must be done clinicly by clincal experts. Further more, the class imbalance after the merger (although the merger some balanced the catastrophic previous class imbalance) may have something to do with our results, after all the other class has far fewer samples than normal, depression, and suicidal, thus possibly causing our models to favor the other classes."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
